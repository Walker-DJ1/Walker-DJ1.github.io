---
layout: article
published: True
title: Llama 2: Open Foundation and Fine-Tuned Chat Models
---
# Llama 2: Open Foundation and Fine-Tuned Chat Models

# æ‘˜è¦

æœ¬æ–‡ä»‹ç» LLaMA 2ï¼Œæˆ‘ä»¬å¼€å‘çš„ä¸€ç»„==é¢„è®­ç»ƒå’Œå¾®è°ƒ==å¤§è¯­è¨€æ¨¡å‹é›†ï¼Œ

- LLaMA2 å‚æ•°è§„æ¨¡ **==`7b~70b`==**ï¼›
- å¾®è°ƒæ¨¡å‹ç§°ä¸º **==`LLaMA2-Chat`==**ï¼Œé’ˆå¯¹å¯¹è¯åœºæ™¯è¿›è¡Œäº†ä¼˜åŒ–ã€‚

ä¸\*\*==å…¶ä»–å¼€æºèŠå¤©æ¨¡å‹==\*\*è¿›è¡Œæ¯”è¾ƒï¼Œ

- å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLaMA2 æ€§èƒ½æ›´å¥½ï¼›
- æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æ–¹é¢ï¼Œäººå·¥è¯„ä¼°ï¼ˆhuman evaluationsï¼‰çš„ç»“æœä¹Ÿè¯æ˜ LLaMA2 æ›´ä¼˜ã€‚

å› æ­¤ï¼ŒLLaMA2 å¯ä»¥ä½œä¸ºä¸€ä¸ªä¸é”™çš„\*\*==é—­æºæ¨¡å‹æ›¿ä»£æ–¹æ¡ˆ==\*\*ã€‚ æœ¬æ–‡å°†è¯¦ç»†æè¿°æˆ‘ä»¬æ˜¯å¦‚ä½•å¯¹ LLaMA2-Chat è¿›è¡Œå¾®è°ƒå’Œå®‰å…¨æ€§æ”¹è¿›çš„ã€‚ ç¤¾åŒºå¯ä»¥åœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¼€å‘è¿­ä»£ï¼Œä¸º LLM çš„è´Ÿè´£ä»»å‘å±•åšå‡ºè´¡çŒ®ã€‚

# 1 å¼•è¨€

å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ä½œä¸ºåŠŸèƒ½å¼ºå¤§çš„äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œåœ¨æ¶‰åŠè·¨é¢†åŸŸã€éœ€è¦ä¸“ä¸šçŸ¥è¯† ï¼ˆä¾‹å¦‚\*\*==ç¼–ç¨‹å’Œåˆ›æ„å†™ä½œ==**ï¼‰çš„**==å¤æ‚æ¨ç†ä»»åŠ¡==\*\*ä¸­è¡¨ç°å‡ºäº†å·¨å¤§çš„æ½œåŠ›ã€‚ LLM é€šè¿‡èŠå¤©çª—å£ä¸äººç±»è¿›è¡Œäº¤äº’ï¼Œç®€å•æ–¹ä¾¿ï¼Œå› æ­¤ä¸€ç»æ¨å‡ºå°±è¿…é€Ÿæ‰“å¼€å¤§ä¼—å¸‚åœºã€‚

å¦‚æœè€ƒè™‘åˆ°èƒŒåçš„\*\*==è®­ç»ƒæ–¹æ³•è®º==**æœ¬è´¨ä¸Šéå¸¸**==ç®€å•ç›´è§‚==\*\*ï¼Œ

- é¦–å…ˆï¼Œåœ¨å¤§é‡è‡ªç›‘ç£æ•°æ®ä¸Šå¯¹ **==`auto-regressive transforer`==** è¿›è¡Œ\*\*==é¢„è®­ç»ƒ==\*\*ï¼Œ
- ç„¶åï¼Œé€šè¿‡åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆ**==`RLHF`==**ï¼‰ç­‰æŠ€æœ¯\*\*==ä¸äººç±»åå¥½å¯¹é½==\*\*ã€‚

å°±æ›´ä¼šéœ‡æƒŠäº LLM çš„èƒ½åŠ›æ˜¯å¤šä¹ˆå‡ºä¼—ã€‚

## 1.1 ç°çŠ¶ï¼šæ²¡æœ‰èƒ½ä¸ ChatGPT åŒ¹æ•Œçš„å¼€æºå¤§æ¨¡å‹

å°½ç®¡å¤§æ¨¡å‹çš„è®­ç»ƒæ–¹æ³•å¾ˆç®€å•ï¼Œä½†æ˜¯ï¼Œæé«˜çš„\*\*==ç®—åŠ›è¦æ±‚==\*\*é™åˆ¶äº†å®ƒçš„å‘å±•ï¼Œ ç»“æœæ˜¯åªæœ‰å°‘æ•°å‡ å®¶å…¬å¸æœ‰è´¢åŠ›è¿›è¡Œç ”ç©¶å’Œè®­ç»ƒã€‚ è™½ç„¶ä¹‹å‰å·²ç»å¼€æºäº†ä¸€äº›é¢„è®­ç»ƒçš„å¤§æ¨¡å‹ï¼ŒåŒ…æ‹¬

- **BLOOMï¼ˆScao ç­‰ï¼Œ2022ï¼‰**
- LLaMA-1ï¼ˆTouvron ç­‰ï¼Œ2023ï¼‰
- **Falconï¼ˆPenedo ç­‰ï¼Œ2023ï¼‰**

è¿™äº›æ¨¡å‹çš„\*\*==æ€§èƒ½å·²ç»ä¸ GPT-3==**ï¼ˆBrown ç­‰ï¼Œ2020ï¼‰å’Œ Chinchillaï¼ˆHoffmann ç­‰ï¼Œ2022ï¼‰ ç­‰é—­æºé¢„è®­ç»ƒæ¨¡å‹**==ç›¸å½“==**ï¼Œä½†å®ƒä»¬è¿˜æ— æ³•æˆä¸º ChatGPTã€BARD å’Œ Claude ç­‰**==æ€§èƒ½æ›´å¼ºå¤§çš„é—­æºã€ç”Ÿäº§çº§å¤§æ¨¡å‹==\*\*çš„æ›¿ä»£å“ï¼Œ

- åè€…åšäº†\*\*==å¤§é‡å¾®è°ƒ==\*\*ä»¥ä¸äººç±»åå¥½å¯¹é½ï¼Œ æå¤§å¢å¼ºäº†å®ƒä»¬çš„å¯ç”¨æ€§å’Œå®‰å…¨æ€§ï¼›
- è¿™ä¸€è¿‡ç¨‹éœ€è¦å¤§é‡çš„\*\*==è®¡ç®—å’Œäººå·¥æ ‡æ³¨æˆæœ¬==\*\*ï¼Œå¹¶ä¸”é€šå¸¸ä¸é€æ˜ï¼Œä¹Ÿéš¾ä»¥è½»æ¾ç…§æ¬ï¼Œ å› æ­¤é™åˆ¶äº†ç¤¾åŒºåœ¨ advance AI alignment research æ–¹é¢çš„è¿›å±•ã€‚

## 1.2 å¼€æº LLaMA2/LLaMA2-Chatï¼Œå¡«è¡¥ç©ºç™½

æœ¬æ–‡ä»‹ç»æˆ‘ä»¬å¼€æºçš„ LLaMA2ï¼Œè¿™æ˜¯ä¸€ç»„é¢„è®­ç»ƒå’Œå¾®è°ƒçš„ LLMï¼ŒåŒ…æ‹¬ LLaMA2 å’Œ LLaMA2-Chatã€‚ ä¸å…¶ä»–å¼€æº chat models è¿›è¡Œæ¯”è¾ƒï¼Œ

- å¤§å¤šæ•°åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLaMA2 æ€§èƒ½æ›´å¥½ï¼›
- æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æ–¹é¢ï¼Œäººå·¥è¯„ä¼°ï¼ˆhuman evaluationsï¼‰çš„ç»“æœä¹Ÿè¯æ˜ LLaMA2 æ›´ä¼˜ã€‚

å› æ­¤ï¼ŒLLaMA2 å¯ä»¥ä½œä¸ºä¸€ä¸ªä¸é”™çš„\*\*==é—­æºæ¨¡å‹æ›¿ä»£æ–¹æ¡ˆ==**ã€‚ æœ¬æ–‡å°†è¯¦ç»†æè¿°æˆ‘ä»¬æ˜¯å¦‚ä½•å¯¹ LLaMA2-Chat è¿›è¡Œå¾®è°ƒå’Œå®‰å…¨æ€§æ”¹è¿›çš„ï¼Œ è¿™æ ·ç¤¾åŒºå°±èƒ½å¤Ÿåœ¨æˆ‘ä»¬çš„å·¥ä½œåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¼€å‘è¿­ä»£ï¼Œä¸º LLM çš„è´Ÿè´£ä»»å‘å±•åšå‡ºè´¡çŒ®ã€‚ å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬å‘å…¬ä¼—ï¼ˆthe general publicï¼‰å¼€æºä»¥ä¸‹æ¨¡å‹ï¼Œä¾›**==ç ”ç©¶å’Œå•†ä¸šä½¿ç”¨==\*\*ï¼ˆresearch and commercial useï¼‰ï¼š

1.  LLaMA2ï¼šè¿™æ˜¯ LLaMA 1 çš„å‡çº§ç‰ˆ
    
    - **å°†é¢„è®­ç»ƒè¯­æ–™åº“çš„å¤§å°å¢åŠ äº† 40%**ï¼Œæ•°æ®é›†å¤§å° **==`+40%`==**ï¼ˆ1.4T tokens -> 2T tokensï¼‰ï¼Œ
    - æ¨¡å‹çš„\*\*==ä¸Šä¸‹æ–‡é•¿åº¦ç¿»å€==\*\*ï¼Œï¼ˆ2k -> 4kï¼‰
    - é‡‡ç”¨**åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›**grouped-query attention, **GQA**ï¼ˆAinslie ç­‰ï¼Œ2023ï¼‰  

**æœ¬æ¬¡å‘å¸ƒ 7B/13B/70B å‚æ•°çš„ LLaMA2 æ¨¡å‹**ã€‚æˆ‘ä»¬è¿˜è®­ç»ƒäº† 34B å˜ä½“ï¼Œæˆ‘ä»¬åœ¨æœ¬æ–‡ä¸­å¯¹æ­¤è¿›è¡Œäº†æŠ¥å‘Šï¼Œä½†å¹¶æœªå‘å¸ƒã€‚
2.  LLaMA2-Chatï¼šLLaMA2 çš„å¾®è°ƒç‰ˆæœ¬ï¼Œé’ˆå¯¹\*\*==å¯¹è¯åœºæ™¯==\*\*è¿›è¡Œäº†ä¼˜åŒ–ã€‚
    
    - åŒæ ·å‘å¸ƒ 7B/13B/70B å‚æ•°çš„ç‰ˆæœ¬ã€‚

æˆ‘ä»¬ç›¸ä¿¡ï¼Œåœ¨å®‰å…¨çš„å‰æä¸‹ï¼ŒLLM çš„å¼€æ”¾å°†å¯¹ç¤¾ä¼šäº§ç”Ÿç§¯æå½±å“ã€‚ä½†æ³¨æ„ï¼Œå’Œæ‰€æœ‰ LLM ä¸€æ ·ï¼ŒLLaMA2 æ˜¯ä¸€é¡¹æ–°æŠ€æœ¯ï¼Œ åœ¨ä½¿ç”¨ä¸­å­˜åœ¨æ½œåœ¨é£é™©ï¼ˆBender ç­‰ï¼Œ2021bï¼›Weidinger ç­‰ï¼Œ2021ï¼›Solaiman ç­‰ï¼Œ2023ï¼‰ï¼Œ

- è¿„ä»Šä¸ºæ­¢è¿›è¡Œçš„**æµ‹è¯•éƒ½æ˜¯ç”¨è‹±è¯­**è¿›è¡Œçš„ï¼Œå°šæœªï¼ˆä¹Ÿä¸å¯èƒ½ï¼‰æ¶µç›–æ‰€æœ‰åœºæ™¯ã€‚ åœ¨éƒ¨ç½²ä»»ä½• LLaMA2-Chat åº”ç”¨ä¹‹å‰ï¼Œå¼€å‘è€…åº”é’ˆå¯¹å…¶ç‰¹å®šåœºæ™¯è¿›è¡Œå®‰å…¨æµ‹è¯•å’Œè°ƒä¼˜ï¼›
- æˆ‘ä»¬æä¾›äº†ä¸€ä»½è´Ÿè´£ä»»ä½¿ç”¨æŒ‡å—å’Œä»£ç ç¤ºä¾‹ï¼Œä»¥ä¿ƒè¿› LLaMA2 å’Œ LLaMA2-Chat çš„å®‰å…¨éƒ¨ç½²ã€‚æ›´å¤šä¿¡æ¯è§ 5.3 èŠ‚ã€‚

ä¸€äº›èµ„æ–™é“¾æ¥ï¼š  
ai.meta.com/resources/models-and-libraries/llama/  
ai.meta.com/llama  
github.com/facebookresearch/llama

## 1.3 LLaMA2 æ˜¯å¦‚ä½•ç‚¼æˆçš„ï¼šè®­ç»ƒ+å¾®è°ƒé¸Ÿç°

![å›¾ 4](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/4-FIGURE4-1.png)  
ç‚¼ä¸¹å››æ­¥ï¼š

- ä½¿ç”¨å…¬å¼€æ•°æ®é¢„è®­ç»ƒï¼ˆè‡ªç›‘ç£å­¦ä¹ ï¼‰ï¼Œå¾—åˆ° LLaMA2ï¼›
- å¯¹ LLaMA2 è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ï¼Œå¾—åˆ°ä¸€ä¸ªåˆå§‹ç‰ˆæœ¬çš„ LLaMA2-Chatï¼›
- äººå¯¹ LLaMA2-Chat å›ç­”è¿›è¡Œåé¦ˆå’Œæ ‡æ³¨ï¼Œå¾—åˆ°ä¸¤ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆåˆ†åˆ«é’ˆå¯¹æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼‰ï¼›
- é€šè¿‡åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰/ rejection sampling / PPOï¼Œå¯¹ LLaMA2-Chat è¿›è¡Œï¼ˆå¤šæ¬¡ï¼‰è¿­ä»£ã€‚

## 1.4 æœ¬æ–‡ç»„ç»‡

æœ¬æ–‡å…¶ä½™éƒ¨åˆ†ç»„ç»‡å¦‚ä¸‹ï¼š

ç¬¬ 2 èŠ‚ï¼šé¢„è®­ç»ƒæ–¹æ³•  
ç¬¬ 3 èŠ‚ï¼šå¾®è°ƒæ–¹æ³•  
ç¬¬ 4 èŠ‚ï¼šæ¨¡å‹å®‰å…¨æ–¹æ³•  
ç¬¬ 5 èŠ‚ï¼šæ ¸å¿ƒè§‚å¯Ÿå’Œè§è§£  
ç¬¬ 6 èŠ‚ï¼šç›¸å…³å·¥ä½œ  
ç¬¬ 7 èŠ‚ï¼šæ€»ç»“

# 2 é¢„è®­ç»ƒï¼ˆPretrainingï¼‰

ä¸ºäº†æ‰“é€  LLaMA2 è¿™ä¸ªæ–°ç³»åˆ—æ¨¡å‹ï¼Œæˆ‘ä»¬é‡‡ç”¨äº† **Touvron ç­‰ï¼ˆ2023ï¼‰**çš„é¢„è®­ç»ƒæ–¹æ³•ï¼Œ ä½¿ç”¨äº†ä¸€ä¸ª**ä¼˜åŒ–çš„è‡ªå›å½’ transformer**ï¼Œå¹¶è¿›è¡Œäº†ä¸€äº›æ”¹è¿›ä»¥æé«˜æ€§èƒ½ï¼Œ åŒ…æ‹¬ï¼Œ

- æ›´å¥å£®çš„**æ•°æ®æ¸…æ´—**
- æ›´æ–°çš„**è®­ç»ƒæ•°æ®æ¯”ä¾‹**
- æ›´å¤šçš„**è®­ç»ƒ tokens**
- æ›´é•¿çš„**ä¸Šä¸‹æ–‡**
- ä½¿ç”¨ **grouped-query attentionï¼ˆGQAï¼‰ï¼Œé€šè¿‡ç»„æŸ¥è¯¢æ¥æé«˜æ¨ç†æ€§èƒ½**ã€‚

> GQA ä¼˜åŒ–æ¨ç†çš„åŸºæœ¬åŸç†ï¼šå¤§æ¨¡å‹æ¨ç†çš„æé™ï¼šç†è®ºåˆ†æã€æ•°å­¦å»ºæ¨¡ä¸ CPU/GPU å®æµ‹ï¼ˆ2024ï¼‰ã€‚

è¡¨ 1 æ¯”è¾ƒäº† LLaMA 2 ä¸ LLaMA 1 çš„ä¸€äº›å±æ€§ï¼š  
![è¡¨1](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/5-TABLE1-1.png)

## 2.1 é¢„è®­ç»ƒæ•°æ®ï¼ˆPretraining Dataï¼‰

ç»„åˆäº†ä¸€äº›å…¬å¼€å¯ç”¨çš„æ•°æ®æºï¼Œå…¶ä¸­ä¸åŒ…å«æ¥ Meta äº§å“æˆ–æœåŠ¡çš„æ•°æ®ã€‚æŸäº›ç½‘ç«™åŒ…å«äº†å¾ˆå¤šä¸ªäººä¿¡æ¯ï¼Œæˆ‘ä»¬åŠªåŠ›åˆ æ‰äº†å…¶ä¸­çš„æ­¤ç±»ä¿¡æ¯ã€‚  
è®­ç»ƒäº† **2Tï¼ˆ2 ä¸‡äº¿ï¼‰ä¸ª token**ï¼Œè¿™åœ¨æ€§èƒ½å’Œæˆæœ¬ä¹‹é—´æä¾›äº†ä¸é”™çš„æŠ˜ä¸­ï¼ˆperformanceâ€“cost trade-offï¼‰ï¼Œ  
å¯¹å¤§éƒ¨åˆ†äº‹å®ç±»æ•°æ®æºè¿›è¡Œ up-samplingï¼Œä»¥å¢åŠ çŸ¥è¯†å‡å°‘å¹»è§‰ï¼ˆ increase knowledge and dampen hallucinationsï¼‰ã€‚  
æˆ‘ä»¬è¿›è¡Œäº†å¤§é‡é¢„è®­ç»ƒæ•°æ®ç ”ç©¶ï¼Œè¿™æ ·ç”¨æˆ·å¯ä»¥æ›´å¥½åœ°äº†è§£ LLaMA2 çš„æ½œåœ¨èƒ½åŠ›å’Œé™åˆ¶ï¼›è¯¦ç»†ç»“æœè§ 4.1 èŠ‚ã€‚

## 2.2 è®­ç»ƒç»†èŠ‚ï¼ˆTraining Detailsï¼‰

æˆ‘ä»¬é‡‡ç”¨äº† Llama 1 çš„å¤§éƒ¨åˆ†é¢„è®­ç»ƒè®¾ç½®å’Œæ¨¡å‹æ¶æ„ã€‚

- ä½¿ç”¨**æ ‡å‡†çš„ transformer æ¶æ„**ï¼ˆVaswani ç­‰ï¼Œ2017ï¼‰ï¼Œ
- ä½¿ç”¨ **RMSNorm è¿›è¡Œé¢„å½’ä¸€åŒ–**ï¼ˆZhang å’Œ Sennrichï¼Œ2019ï¼‰ï¼Œ
- ä½¿ç”¨ **SwiGLU æ¿€æ´»å‡½æ•°**ï¼ˆShazeerï¼Œ2020ï¼‰ï¼Œä»¥åŠ**æ—‹è½¬ä½ç½®åµŒå…¥**ï¼ˆrotary positional embeddingsï¼ŒRoPEï¼ŒSu ç­‰ï¼Œ2022ï¼‰ã€‚  

ä¸ Llama 1 ç›¸æ¯”ï¼Œä¸»è¦çš„æ¶æ„å·®å¼‚åŒ…æ‹¬
- ä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆç¿»å€ï¼Œ2k -> 4kï¼‰
- ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQA, grouped-query attentionï¼‰  

 æˆ‘ä»¬åœ¨é™„å½• A.2.1 èŠ‚ä¸­é€šè¿‡æ¶ˆèå®éªŒè¯¦ç»†è¯´æ˜äº†è¿™äº›å·®å¼‚ï¼Œä»¥è¯æ˜å®ƒä»¬çš„é‡è¦æ€§ã€‚

### 2.2.1 è¶…å‚æ•°ï¼ˆHyperparametersï¼‰

- ä½¿ç”¨ AdamW ä¼˜åŒ–å™¨è¿›è¡Œè®­ç»ƒï¼ˆLoshchilov å’Œ Hutterï¼Œ2017ï¼‰ï¼Œå…¶ä¸­ Î²1 = 0.9ï¼ŒÎ²2 = 0.95ï¼Œeps = 10-5ã€‚
- ä½¿ç”¨**ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦**ï¼ˆa cosine learning rate scheduleï¼‰ï¼Œ**çƒ­èº«é˜¶æ®µä¸º 2000 steps**ï¼Œå¹¶å°†**æœ€ç»ˆå­¦ä¹ ç‡è¡°å‡åˆ°å³°å€¼å­¦ä¹ ç‡çš„ 10%**ã€‚
- **ä½¿ç”¨ 0.1 çš„æƒé‡è¡°å‡ï¼ˆweight decayï¼‰å’Œ 1.0 çš„æ¢¯åº¦è¡°å‡ï¼ˆgradient clippingï¼‰**ã€‚  
    å›¾ 5ï¼ˆaï¼‰æ˜¾ç¤ºäº†ä½¿ç”¨è¿™äº›è¶…å‚æ•°è®­ç»ƒçš„ LLaMA2 çš„è®­ç»ƒæŸå¤±ï¼Œ  
    ![img](https://github.com/Walker-DJ1/blog_data//raw/main/llama2/2-FIGURE1-1.png)

### 2.2.2 åˆ†è¯å™¨ï¼ˆTokenizerï¼‰

LLaMA2 ä½¿ç”¨çš„åˆ†è¯å™¨ä¸ Llama 1 ç›¸åŒï¼›é‡‡ç”¨äº†ä¸€ç§**å­—èŠ‚å¯¹ç¼–ç ï¼ˆbytepair encodingï¼ŒBPEï¼‰** ç®—æ³•ï¼ˆSennrich ç­‰ï¼Œ2016ï¼‰ï¼Œ æˆ‘ä»¬ä½¿ç”¨äº† SentencePieceï¼ˆKudo å’Œ Richardsonï¼Œ2018ï¼‰çš„å®ç°ã€‚  
ä¸ Llama 1 ä¸€æ ·ï¼Œ

- å°†æ‰€æœ‰ numbers æ‹†åˆ†ä¸ºå•ä¸ª digitsï¼Œ
- ä½¿ç”¨ bytes æ¥åˆ†è§£æœªçŸ¥çš„ UTF-8 å­—ç¬¦ã€‚
- **vocabulary size ä¸º 32k tokens**ã€‚

### 2.2.3 è®­ç»ƒç¡¬ä»¶å’Œç¢³è¶³è¿¹

#### è®­ç»ƒç¡¬ä»¶ï¼ˆTraining Hardwareï¼‰

æˆ‘ä»¬åœ¨ Meta çš„è¶…çº§é›†ç¾¤ï¼ˆResearch Super Clusterï¼ŒRSCï¼ŒLee å’Œ Senguptaï¼Œ2022ï¼‰ ä»¥åŠå†…éƒ¨ç”Ÿäº§é›†ç¾¤ä¸Šé¢„è®­ç»ƒ LLaMA2ã€‚ ä¸¤ä¸ªé›†ç¾¤ GPU éƒ½æ˜¯ NVIDIA A100ï¼Œç½‘ç»œä¹Ÿéƒ½æ˜¯ 200Gbps äº’è”ï¼Œ ä½†äº’è”æ–¹æ¡ˆå’Œ GPU æœ€å¤§åŠŸè€—ä¸åŒï¼š

- RSC é›†ç¾¤ï¼š200Gbps InfiniBand + 400W GPUï¼›
- ç”Ÿäº§é›†ç¾¤ï¼š200Gbps RoCE + 350W GPUï¼›RoCE æˆæœ¬æ›´ä½ã€‚

ç»“è®ºï¼šRoCE + 350W GPU çš„é›†ç¾¤ï¼Œç»è¿‡ä¼˜åŒ–çš„ä»£ç èƒ½è¾¾åˆ° IB + 400W GPU é›†ç¾¤æ€§èƒ½çš„ 90%ã€‚

#### é¢„è®­ç»ƒç¢³è¶³è¿¹ï¼ˆCarbon Footprint of Pretrainingï¼‰

æ ¹æ®ä¹‹å‰çš„ç ”ç©¶ï¼ˆBender ç­‰ï¼Œ2021aï¼›Patterson ç­‰ï¼Œ2021ï¼›Wu ç­‰ï¼Œ2022ï¼›Dodge ç­‰ï¼Œ2022ï¼‰ï¼Œ ç»“åˆ GPU è®¾å¤‡çš„åŠŸè€—ä¼°è®¡ä»¥åŠç¢³æ•ˆç‡ï¼Œæˆ‘ä»¬æ¥è®¡ç®— LLaMA2 é¢„è®­ç»ƒæ‰€äº§ç”Ÿçš„ç¢³æ’æ”¾é‡ã€‚æ³¨æ„ï¼Œ

- **GPU çš„å®é™…åŠŸè€—å–å†³äºå…¶åˆ©ç”¨ç‡ï¼ˆutilï¼‰**ï¼Œæˆ‘ä»¬ä¼°è®¡ GPU åŠŸè€—ä½¿ç”¨çš„æ˜¯çƒ­è®¾è®¡åŠŸç‡ï¼ˆTDPï¼‰ï¼ŒäºŒè€…å¯èƒ½ä¼šæœ‰æ‰€å·®å¼‚ï¼›
- æˆ‘ä»¬çš„è®¡ç®—ä¸è€ƒè™‘å…¶ä»–ç”µåŠ›éœ€æ±‚ï¼Œä¾‹å¦‚äº’è¿ã€é GPU æœåŠ¡å™¨åŠŸè€—ã€æ•°æ®ä¸­å¿ƒåˆ¶å†·åŠŸè€—ç­‰ï¼›
- ä¸ AI ç¡¬ä»¶ï¼ˆå¦‚ GPUï¼‰ç”Ÿäº§ç›¸å…³çš„ç¢³æ’æ”¾é‡å¯èƒ½ä¼šå¢åŠ æ•´ä½“ç¢³è¶³è¿¹ï¼ˆGupta ç­‰ï¼Œ2022ï¼‰ã€‚  
    è®¡ç®—ç»“æœè§è¡¨ 2ï¼Œ  
    ![è¡¨2](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/6-TABLE2-1.png)  
    è¡¨ 2ï¼šé¢„è®­ç»ƒæœŸé—´çš„äºŒæ°§åŒ–ç¢³æ’æ”¾é‡ã€‚ æ—¶é—´ï¼šè®­ç»ƒæ¯ä¸ªæ¨¡å‹æ‰€éœ€çš„æ€» GPU æ—¶é—´ã€‚ åŠŸè€—ï¼šæ ¹æ®ç”µæºä½¿ç”¨æ•ˆç‡è°ƒæ•´æ‰€ç”¨ GPU çš„æ¯ä¸ª GPU è®¾å¤‡çš„å³°å€¼åŠŸç‡å®¹é‡ã€‚ 100%çš„æ’æ”¾é‡è¢«Metaçš„å¯æŒç»­å‘å±•è®¡åˆ’ç›´æ¥æŠµæ¶ˆï¼Œè€Œä¸”ç”±äºæˆ‘ä»¬å…¬å¼€å‘å¸ƒè¿™äº›æ¨¡å‹ï¼Œå› æ­¤é¢„è®­ç»ƒæˆæœ¬ä¸éœ€è¦ç”±å…¶ä»–äººæ‰¿æ‹…ã€‚
- A100-80GBï¼ˆ400W/350W TDPï¼‰æœºå™¨ï¼Œæ€»å…±è€—è´¹äº† 3.3M GPU-hourï¼›
- ä¼°ç®—çš„æ€»æ’æ”¾é‡ä¸º 539 tCO2eqï¼Œå¯ 100ï¼… ç”± Meta çš„å¯æŒç»­è®¡åˆ’æŠµæ¶ˆï¼›
- LLaMA2 çš„å¼€æºè¿˜æ„å‘³ç€å…¶ä»–å…¬å¸ä¸éœ€è¦æ‰¿æ‹…è¿™äº›é¢„è®­ç»ƒæˆæœ¬ï¼ŒèŠ‚çœäº†æ›´å¤šçš„å…¨çƒèµ„æºã€‚

## 2.3 LLaMA 2 é¢„è®­ç»ƒæ¨¡å‹æ€§èƒ½è¯„ä¼°ï¼ˆPretrained Model Evaluationï¼‰

æœ¬èŠ‚ä»‹ç»åœ¨æ ‡å‡†å­¦æœ¯åŸºå‡†æµ‹è¯•ä¸­ï¼ŒLLaMA 1/2 åŸºç¡€æ¨¡å‹ã€MosaicML é¢„è®­ç»ƒ transforer ï¼ˆMPTï¼‰åŠ Falconï¼ˆAlmazrouei ç­‰ï¼Œ2023ï¼‰çš„ç»“æœã€‚ æ‰€æœ‰è¯„ä¼°éƒ½ä½¿ç”¨æˆ‘ä»¬çš„å†…éƒ¨è¯„ä¼°åº“ã€‚æˆ‘ä»¬åœ¨å†…éƒ¨é‡å¤äº† MPT å’Œ Falcon æ¨¡å‹çš„ç»“æœã€‚ å¯¹äºè¿™äº›æ¨¡å‹ï¼Œå§‹ç»ˆé€‰æ‹©æˆ‘ä»¬è¯„ä¼°æ¡†æ¶å’Œæ‰€æœ‰å…¬å¼€æŠ¥å‘Šçš„ç»“æœä¸­çš„æœ€é«˜åˆ†ï¼ˆthe best score between our evaluation framework and any publicly reported resultsï¼‰ã€‚

åŸºå‡†æµ‹è¯•åˆ†ä¸ºä»¥ä¸‹å‡ ç±»ï¼ˆå•ä¸ªåŸºå‡†æµ‹è¯•çš„ç»“æœè§ A.2.2ï¼‰ï¼š

- ä»£ç ã€‚LLaMA åœ¨ HumanEvalï¼ˆChen ç­‰ï¼Œ2021ï¼‰å’Œ MBPPï¼ˆAustin ç­‰ï¼Œ2021ï¼‰ä¸Šçš„å¹³å‡ pass@1 åˆ†æ•°ã€‚
- å¸¸è¯†æ¨ç†ã€‚PIQAï¼ˆBisk ç­‰ï¼Œ2020ï¼‰ã€SIQAï¼ˆSap ç­‰ï¼Œ2019ï¼‰ã€HellaSwagï¼ˆZellers ç­‰ï¼Œ2019aï¼‰ã€WinoGrandeï¼ˆSakaguchi ç­‰ï¼Œ2021ï¼‰ã€ ARC easy å’Œ challengeï¼ˆClark ç­‰ï¼Œ2018ï¼‰ã€OpenBookQAï¼ˆMihaylov ç­‰ï¼Œ2018ï¼‰å’Œ CommonsenseQAï¼ˆTalmor ç­‰ï¼Œ2018ï¼‰ çš„å¹³å‡åˆ†æ•°ã€‚CommonSenseQA çš„ 7-shot ç»“æœå’Œå…¶ä»–æ‰€æœ‰åŸºå‡†æµ‹è¯•çš„ 0-shot ç»“æœã€‚
- ä¸–ç•ŒçŸ¥è¯†ã€‚è¯„ä¼°äº† NaturalQuestionsï¼ˆKwiatkowski ç­‰ï¼Œ2019ï¼‰å’Œ TriviaQAï¼ˆJoshi ç­‰ï¼Œ2017ï¼‰çš„ 5-shot æ€§èƒ½ï¼Œå¹¶ç»™å‡ºäº†å¹³å‡åˆ†æ•°ã€‚
- é˜…è¯»ç†è§£ã€‚åœ¨ SQuADï¼ˆRajpurkar ç­‰ï¼Œ2018ï¼‰ã€QuACï¼ˆChoi ç­‰ï¼Œ2018ï¼‰å’Œ BoolQï¼ˆClark ç­‰ï¼Œ2019ï¼‰ä¸Šçš„ 0-shot å¹³å‡åˆ†æ•°ã€‚
- æ•°å­¦ã€‚GSM8Kï¼ˆ8 shotï¼‰ï¼ˆCobbe ç­‰ï¼Œ2021ï¼‰å’Œ MATHï¼ˆ4 shotï¼‰ï¼ˆHendrycks ç­‰ï¼Œ2021ï¼‰åŸºå‡†æµ‹è¯•åœ¨ top 1 çš„å¹³å‡åˆ†æ•°ã€‚
- èšåˆåŸºå‡†æµ‹è¯•ã€‚MMLUï¼ˆ5 shotï¼‰ï¼ˆHendrycks ç­‰ï¼Œ2020ï¼‰ã€Big Bench Hardï¼ˆBBHï¼‰ï¼ˆ3 shotï¼‰ï¼ˆSuzgun ç­‰ï¼Œ2022ï¼‰å’Œ AGI Evalï¼ˆ3-5 shotï¼‰ï¼ˆZhong ç­‰ï¼Œ2023ï¼‰çš„æ•´ä½“ç»“æœã€‚ å¯¹äº AGI Evalï¼Œåªè¯„ä¼°è‹±æ–‡ä»»åŠ¡å¹¶ç»™å‡ºå¹³å‡åˆ†æ•°ã€‚

### 2.3.1 ä¸å¼€æºåŸºåº§å¤§æ¨¡å‹å¯¹æ¯”

è¡¨ 3 æ€»ç»“äº†ä¸€ç³»åˆ—å¸¸è§åŸºå‡†æµ‹è¯•çš„æ•´ä½“æ€§èƒ½ã€‚å®‰å…¨åŸºå‡†æµ‹è¯•è§ 4.1 èŠ‚ä¸­ã€‚  
![table2](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/7-TABLE3-1.png)  
è¡¨ 3ï¼šä¸å…¶ä»–å¼€æºçš„åŸºåº§å¤§æ¨¡å‹å¯¹æ¯”æ€§èƒ½ï¼ŒåŸºäºä¸€äº›å­¦æœ¯åŸºå‡†æµ‹è¯•  
å¯ä»¥çœ‹å‡ºï¼Œ

- LLaMA2 ä¼˜äº LLaMA1ï¼›
- ä¸ Llama 1 65B ç›¸æ¯”ï¼ŒLLaMA2 70B åœ¨ MMLU å’Œ BBH ä¸Šçš„ç»“æœåˆ†åˆ«æé«˜äº†çº¦ 5 å’Œ 8 ä¸ªç™¾åˆ†ç‚¹ï¼›
- é™¤äº† Code åŸºå‡†æµ‹è¯•ï¼ŒLLaMA2 7B å’Œ 30B æ¨¡å‹åœ¨å…¶ä»–ç±»åˆ«ä¸Šéƒ½ä¼˜äºç›¸åº”å¤§å°çš„ MPT æ¨¡å‹ï¼›
- LLaMA2 7B å’Œ 34B åœ¨æ‰€æœ‰åŸºå‡†æµ‹è¯•ç±»åˆ«ä¸Šä¼˜äº Falcon 7B å’Œ 40B æ¨¡å‹ã€‚
- LLaMA2 70B æ¨¡å‹ä¼˜äºæ‰€æœ‰å¼€æºæ¨¡å‹ã€‚

### 2.3.2 ä¸é—­æºå¤§æ¨¡å‹å¯¹æ¯”

é™¤äº†å¼€æºæ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜å°† LLaMA2 70B çš„ç»“æœä¸é—­æºæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚å¦‚è¡¨ 4 æ‰€ç¤ºï¼Œ  
![è¡¨4](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/7-TABLE4-1.png)  
è¡¨ 4ï¼šåŸºäºå­¦æœ¯åŸºå‡†æµ‹è¯•ï¼Œå¯¹æ¯” LLaMA2 å’Œé—­æºæ¨¡å‹ã€‚ GPT-3.5/GPT-4 çš„ç»“æœæ¥è‡ª OpenAI (2023)ï¼›PaLM çš„ç»“æœæ¥è‡ª Chowdhery et al. (2022)ï¼› PaLM-2-L çš„ç»“æœæ¥è‡ª Anil et al. (2023).

- LLaMA2 70B åœ¨ MMLU å’Œ GSM8K ä¸Šä¸ GPT-3.5ï¼ˆOpenAIï¼Œ2023ï¼‰æ¥è¿‘ï¼Œä½†åœ¨ç¼–ç åŸºå‡†æµ‹è¯•ä¸Šå­˜åœ¨æ˜¾è‘—å·®è·ï¼›
- LLaMA2 70B ä¸ PaLMï¼ˆ540Bï¼‰ï¼ˆChowdhery ç­‰ï¼Œ2022ï¼‰ç›¸å½“ï¼Œç”šè‡³æ¯”åè€…æ›´å¥½ï¼›
- LLaMA2 70B ä¸ GPT-4/PaLM-2-L ä»å­˜åœ¨è¾ƒå¤§å·®è·ã€‚
- æˆ‘ä»¬è¿˜åˆ†æäº†æ½œåœ¨çš„æ•°æ®æ±¡æŸ“ï¼Œè¯¦ç»†ä¿¡æ¯è§ A.6 èŠ‚ã€‚

# 3 å¾®è°ƒï¼ˆFine-tuningï¼‰

LLaMA2-Chat ç»è¿‡äº†å‡ ä¸ªæœˆçš„å¯¹é½ï¼ˆalignmentï¼‰è¿­ä»£ï¼Œ åŒ…æ‹¬æŒ‡ä»¤å¾®è°ƒï¼ˆinstruction tuningï¼‰å’Œ RLHFï¼Œè¿™äº›éƒ½éœ€è¦å¤§é‡çš„è®¡ç®—å’Œæ ‡æ³¨èµ„æºã€‚ åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬æŠ¥å‘Šä½¿ç”¨ç›‘ç£å¾®è°ƒï¼ˆç¬¬ 3.1 èŠ‚ï¼‰ä»¥åŠåˆå§‹å’Œè¿­ä»£å¥–åŠ±æ¨¡å‹ï¼ˆç¬¬ 3.2.2 èŠ‚ï¼‰å’Œ RLHFï¼ˆç¬¬ 3.2.3 èŠ‚ï¼‰çš„å®éªŒå’Œç»“æœã€‚ æˆ‘ä»¬è¿˜åˆ†äº«äº†ä¸€ç§æ–°æŠ€æœ¯ï¼Œ**Ghost Attentionï¼ˆGAttï¼‰**ï¼Œ**æˆ‘ä»¬å‘ç°å®ƒæœ‰åŠ©äºæ§åˆ¶å¤šä¸ªå›åˆçš„å¯¹è¯æµ**ï¼ˆç¬¬ 3.3 èŠ‚ï¼‰ã€‚ æœ‰å…³å¾®è°ƒæ¨¡å‹çš„å®‰å…¨è¯„ä¼°ï¼Œè¯·å‚é˜…ç¬¬ 4.2 èŠ‚ã€‚

## 3.1 ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰

### 3.1.1 ä½¿ç”¨å…¬å¼€çš„æŒ‡ä»¤å¾®è°ƒæ•°æ®

ä¸ Touvron ç­‰äººï¼ˆ2023ï¼‰ç±»ä¼¼ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å…¬å¼€å¯ç”¨ **instruction tuning æ•°æ®**ï¼ˆChung ç­‰ï¼Œ2022ï¼‰å¼€å§‹ SFT é˜¶æ®µã€‚

### 3.1.2 æ ‡æ³¨è´¨é‡ä¸ºç‹ï¼ˆQuality Is All You Needï¼‰

è¿˜æœ‰ä¸€äº›ä¸åŒæ¥æºçš„ç¬¬ä¸‰æ–¹ SFT æ•°æ®ï¼Œä½†æˆ‘ä»¬å‘ç°å…¶ä¸­ä¸€äº›çš„==å¤šæ ·æ€§å’Œè´¨é‡æ¬ ä½³== â€”â€” å°¤å…¶æ˜¯å¯¹äºå°† LLM å¯¹é½åˆ°å¯¹è¯å¼ï¼ˆdialogue-styleï¼‰æŒ‡ä»¤æ—¶ã€‚ å› æ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆæ”¶é›†äº†**æ•°åƒä¸ªé«˜è´¨é‡çš„ SFT æ•°æ®ç¤ºä¾‹**ï¼Œå¦‚è¡¨ 5 æ‰€ç¤ºï¼Œ  
![è¡¨5](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/8-TABLE5-1.png)  
è¡¨ 5ï¼šSFT annotation ç¤ºä¾‹ã€‚åˆ†åˆ«å±•ç¤ºäº†ä¸€ä¸ª helpfulness å’Œä¸€ä¸ª safety annotationï¼Œå…¶ä¸­çš„ prompt å’Œ answer éƒ½æ˜¯äººï¼ˆæ ‡æ³¨å‘˜ï¼‰å†™çš„ã€‚

- è¿™äº›æ ‡æ³¨æ•°æ®æ˜¯ä»æˆ‘ä»¬çš„ä¾›åº”å•†è·å–çš„ï¼Œæˆ‘ä»¬å‘ç°**åªéœ€å°‘é‡é«˜è´¨é‡ SFT æ ‡æ³¨æ•°æ®å°±èƒ½æ˜¾è‘—æå‡ç»“æœè´¨é‡**ï¼Œè¿™ä¸ Zhou ç­‰äººï¼ˆ2023ï¼‰çš„å‘ç°ç±»ä¼¼ï¼Œåè€…ä¹Ÿå‘ç°åªéœ€è¦ä¸€å°ç»„å¹²å‡€çš„ instruction-tuning data å°±è¶³ä»¥è·å¾—é«˜è´¨é‡ï¼›
- **æ ¹æ®æˆ‘ä»¬çš„å®é™…ç»éªŒï¼Œå‡ ä¸‡ä¸ª SFT æ ‡æ³¨å°±è¶³ä»¥å®ç°é«˜è´¨é‡çš„ç»“æœ**ï¼› å› æ­¤ï¼Œæˆ‘ä»¬æ€»å…±æ”¶é›†äº† **27,540 ä¸ª SFT** annotationï¼Œæ²¡æœ‰å†æ”¶é›†æ›´å¤šï¼›è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ SFT annotations æ²¡ä½¿ç”¨ä»»ä½• Meta ç”¨æˆ·æ•°æ®ï¼›
- æˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°ï¼Œä¸åŒæ ‡æ³¨å¹³å°å’Œä¾›åº”å•†ï¼ˆannotation platforms and vendorsï¼‰ å¯èƒ½å¯¼è‡´æ˜æ˜¾ä¸åŒçš„ä¸‹æ¸¸æ¨¡å‹æ€§èƒ½ï¼Œè¿™**å‡¸æ˜¾äº†åœ¨ä½¿ç”¨ä¾›åº”å•†è·å–æ ‡æ³¨æ—¶æ•°æ®æ£€æŸ¥çš„é‡è¦æ€§**

ä¸ºäº†éªŒè¯æ•°æ®è´¨é‡ï¼Œæˆ‘ä»¬ä»”ç»†æ£€æŸ¥äº†ä¸€ç»„ 180 ä¸ªç¤ºä¾‹ï¼Œå°†**äººå·¥æä¾›çš„æ ‡æ³¨ä¸æ¨¡å‹ç”Ÿæˆçš„è¿›è¡Œäº†äººå·¥å¯¹æ¯”**ã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬å‘ç° SFT ä¹‹åæ¨¡å‹çš„æŠ½æ ·è¾“å‡ºï¼ˆ outputs sampled from the resulting SFT modelï¼‰ä¸äººå·¥æ ‡æ³¨å‘˜æä¾›çš„ SFT æ•°æ® è´¨é‡ç›¸å½“ï¼Œè¿™è¡¨æ˜æˆ‘ä»¬å¯ä»¥è°ƒæ•´ä¼˜å…ˆçº§ï¼Œå°†æ›´å¤šçš„æ ‡å‡†ç²¾åŠ›æŠ•å…¥åˆ° preference-based annotation for RLHFã€‚

### 3.1.3 ä¸€äº›å¾®è°ƒç»†èŠ‚ï¼ˆFine-Tuning Detailsï¼‰

å¯¹äºç›‘ç£å¾®è°ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ª cosine learning rate scheduleï¼Œ

- **åˆå§‹å­¦ä¹ ç‡ä¸º 2Ã—10-5**ï¼Œ
- **æƒé‡è¡°å‡ä¸º 0.1**ï¼Œ
- **batch size 64**ï¼Œ
- **åºåˆ—é•¿åº¦ä¸º 4096 token**ã€‚

==å¯¹äºå¾®è°ƒè¿‡ç¨‹ï¼Œ**æ¯ä¸ªæ ·æœ¬ç”±ä¸€ä¸ªæç¤ºï¼ˆpromptï¼‰å’Œä¸€ä¸ªå›ç­”ï¼ˆanswerï¼‰ç»„æˆã€‚ä¸ºäº†ç¡®ä¿æ¨¡å‹åºåˆ—é•¿åº¦æ­£ç¡®å¡«å……ï¼ˆproperly filledï¼‰ï¼Œæˆ‘ä»¬å°†è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æç¤ºå’Œå›ç­”è¿æ¥èµ·æ¥ï¼Œ ç„¶åä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„ token æ¥åˆ†éš”æç¤ºå’Œå›ç­”æ®µè½ã€‚**  
ä½¿ç”¨**è‡ªå›å½’ç›®æ ‡ï¼Œå°†ç”¨æˆ·æç¤ºä¸­çš„æ ‡è®°æŸå¤±å½’é›¶(autoregressive objective and zero-out the loss on tokens from the user prompt)ï¼Œ å› æ­¤åªåœ¨ answer token ä¸Šè¿›è¡Œåå‘ä¼ æ’­ã€‚** æœ€åï¼Œæˆ‘ä»¬å¯¹æ¨¡å‹è¿›è¡Œ **2 ä¸ª epoch çš„å¾®è°ƒ**==

> tips:å¹¶ä¸æ˜¯æ ¹æ®promptè¾“å‡ºanswer tokenè®¡ç®—æŸå¤±ï¼Œè€Œæ˜¯prompt+answer tokenå¼è‡ªå›å½’è®­ç»ƒã€‚

## 3.2 åŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ Reinforcement Learning with Human Feedbackï¼ˆRLHFï¼‰

RLHF æ˜¯ä¸€ç§æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ï¼ˆmodel training procedureï¼‰ï¼Œ**åº”ç”¨åœ¨å¾®è°ƒæ¨¡å‹ä¹‹ä¸Šï¼Œ ä½¿æ¨¡å‹è¡Œä¸ºä¸äººç±»åå¥½å’ŒæŒ‡ä»¤è¿›ä¸€æ­¥å¯¹é½**ã€‚==ç»™å®šä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼Œäººç±»æ ‡æ³¨å‘˜é€‰å‡ºä»–ä»¬æ›´å–œæ¬¢çš„é‚£ä¸€ä¸ªï¼ˆæ‰“æ ‡ï¼‰ï¼Œ æˆ‘ä»¬è®¤ä¸ºè¿™æ ·å¾—åˆ°çš„ç»“æœä»£è¡¨äº†æ™®éçš„äººç±»åå¥½ã€‚ ç„¶åï¼Œæ‹¿è¿™äº›äººç±»åé¦ˆæ¥è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼Œ è¿™ä¸ªæ¨¡å‹åœ¨å­¦ä¹ å®Œäººç±»æ ‡æ³¨å‘˜çš„åå¥½æ¨¡å¼ä¹‹åï¼Œå°±å¯ä»¥è‡ªåŠ¨åšåå¥½å†³ç­–äº†ã€‚==
>  å¥–åŠ±æ¨¡å‹å»ºæ¨¡è¿‡ç¨‹ï¼šç»™å®šä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºï¼Œäººç±»æ ‡æ³¨å‘˜é€‰å‡ºä»–ä»¬æ›´å–œæ¬¢çš„é‚£ä¸€ä¸ªï¼ˆæ‰“æ ‡ï¼‰ï¼Œ æˆ‘ä»¬è®¤ä¸ºè¿™æ ·å¾—åˆ°çš„ç»“æœä»£è¡¨äº†æ™®éçš„äººç±»åå¥½ã€‚ ç„¶åï¼Œæ‹¿è¿™äº›äººç±»åé¦ˆæ¥è®­ç»ƒä¸€ä¸ªå¥–åŠ±æ¨¡å‹ï¼ˆReward modelï¼‰

3.2.1 äººç±»åå¥½æ•°æ®æ”¶é›†  
å¥–åŠ±å»ºæ¨¡éœ€è¦æ”¶é›†äººç±»åå¥½æ•°æ®ã€‚ æˆ‘ä»¬é€‰æ‹©äº†ä¸€ç§â€œäºŒé€‰ä¸€æ¯”è¾ƒåè®®â€ï¼ˆbinary comparison protocolï¼‰ï¼Œä¸»è¦æ˜¯å› ä¸ºå®ƒèƒ½å¤Ÿæœ€å¤§åŒ–æˆ‘ä»¬æ”¶é›†çš„ prompts çš„å¤šæ ·æ€§ã€‚ å…¶ä»–ç­–ç•¥ä¹Ÿå€¼å¾—è€ƒè™‘ï¼Œæˆ‘ä»¬å°†ç•™å¾…æœªæ¥çš„å·¥ä½œã€‚

æˆ‘ä»¬çš„æ ‡æ³¨è¿‡ç¨‹å¦‚ä¸‹ï¼š

- æ ‡æ³¨å‘˜é¦–å…ˆå†™ä¸€ä¸ªæç¤ºï¼Œç„¶ååŸºäºæä¾›çš„åˆ¤æ–­æ ‡å‡†ï¼Œåœ¨ä¸¤ä¸ª sampled model response ä¸­é€‰æ‹©ä¸€ä¸ªå¥½çš„ï¼›
- ä¸ºäº†æœ€å¤§åŒ–å¤šæ ·æ€§ï¼Œè¿™ä¸¤ä¸ªå›ç­”æ˜¯ä»ä¸¤ä¸ªä¸åŒçš„ model variants ä¸­æŠ½æ ·å¾—åˆ°çš„ï¼Œå¹¶ä¸”ä¼šæ”¹å˜æ¸©åº¦è¶…å‚æ•°ï¼›
- **é™¤äº†äºŒé€‰ä¸€ï¼Œæˆ‘ä»¬è¿˜è¦æ±‚æ ‡æ³¨å‘˜æ ‡è®°ä»–ä»¬çš„å–œæ¬¢ç¨‹åº¦ï¼šæ˜æ˜¾æ›´å¥½/æ›´å¥½/ç•¥å¾®å¥½/å‡ ä¹æ— å·®åˆ«/ä¸ç¡®å®š**ã€‚

å¯¹äºåå¥½æ ‡æ³¨ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯**æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§**ï¼ˆhelpfulness and safetyï¼‰ï¼Œ

- æœ‰ç”¨æ€§æŒ‡çš„æ˜¯ LLaMA2-Chat çš„å›ç­”æ»¡è¶³ç”¨æˆ·è¯·æ±‚å’Œæä¾›æ‰€éœ€ä¿¡æ¯çš„ç¨‹åº¦ï¼›
- å®‰å…¨æ€§æŒ‡çš„æ˜¯ LLaMA2-Chat çš„å›ç­”æ˜¯å¦ä¸å®‰å…¨ï¼Œä¾‹å¦‚ï¼Œâ€œåˆ—å‡ºåˆ¶åšç‚¸å¼¹çš„è¯¦ç»†æ­¥éª¤â€ å¯èƒ½ç¬¦åˆâ€œæœ‰ç”¨æ€§â€æ ‡å‡†ï¼Œä½†æ ¹æ®æˆ‘ä»¬çš„å‡†åˆ™å®ƒä¸æ»¡è¶³â€œå®‰å…¨æ€§â€ã€‚

å°†è¿™ä¸¤è€…åŒºåˆ†å¼€ï¼Œä½¿æˆ‘ä»¬èƒ½å¯¹äºŒè€…åˆ†åˆ«åº”ç”¨å…·ä½“çš„å‡†åˆ™å¹¶æ›´å¥½åœ°æŒ‡å¯¼æ ‡æ³¨å‘˜ã€‚ä¾‹å¦‚ï¼Œ å¸¸è§„æŒ‡å¯¼åŸåˆ™ä¹‹å¤–ï¼Œæˆ‘ä»¬çš„å®‰å…¨æ ‡æ³¨ï¼ˆsafety annotationsï¼‰è¿˜æä¾›äº†å¯¹ adversarial prompts çš„æŒ‡å¯¼ã€‚

é™¤äº†æ ‡æ³¨æŒ‡å¯¼åŸåˆ™çš„å·®å¼‚ï¼Œæˆ‘ä»¬åœ¨å®‰å…¨é˜¶æ®µè¿˜é¢å¤–æ”¶é›†äº†ä¸€ä¸ªå®‰å…¨æ ‡ç­¾ã€‚ è¿™ä¸ªé¢å¤–çš„ä¿¡æ¯å°†æ¨¡å‹çš„å›ç­”åˆ†ä¸ºä¸‰ä¸ªç±»åˆ«ï¼š

- é€‰ä¸­çš„å›ç­”æ˜¯å®‰å…¨çš„ï¼Œå¦ä¸€ä¸ªå›ç­”ä¸å®‰å…¨ï¼›
- ä¸¤ä¸ªå›ç­”éƒ½æ˜¯å®‰å…¨çš„ï¼›
- ä¸¤ä¸ªå›ç­”éƒ½ä¸å®‰å…¨ã€‚

å®‰å…¨æ•°æ®é›†ä¸­åˆ†åˆ«æœ‰ 18%ã€47% å’Œ 35% çš„æ ·æœ¬åˆ†å¸ƒåœ¨è¿™ä¸‰ä¸ªç±»åˆ«ä¸­ã€‚ æˆ‘ä»¬è®¤ä¸ºä¸å­˜åœ¨â€œé€‰ä¸­çš„å›ç­”ä¸å®‰å…¨ï¼Œè€Œå¦ä¸€ä¸ªå›ç­”å®‰å…¨â€çš„åœºæ™¯ï¼Œ å› ä¸ºæˆ‘ä»¬ç›¸ä¿¡æ›´å®‰å…¨çš„å›ç­”ä¹Ÿä¼šè¢«äººç±»è®¤ä¸ºæ›´å¥½/æ›´å—æ¬¢è¿ã€‚ å…³å®‰å…¨å‡†åˆ™å’Œæ›´è¯¦ç»†çš„å®‰å…¨æ ‡æ³¨ä¿¡æ¯ï¼Œè§ 4.2.1ã€‚

äººç±»æ ‡æ³¨æ˜¯æŒ‰æ¯å‘¨çº§åˆ«ï¼ˆweeklyï¼‰æ‰¹æ¬¡æ”¶é›†çš„ã€‚éšç€åå¥½æ•°æ®çš„å¢å¤šï¼Œå¥–åŠ±æ¨¡å‹å¾—åˆ°äº†æ”¹è¿›ï¼Œ èƒ½å¤Ÿè®­ç»ƒå‡ºè´¨é‡è¶Šæ¥è¶Šå¥½çš„ LLaMA2-Chat ç‰ˆæœ¬ï¼ˆè§ç¬¬ 5 èŠ‚ï¼Œå›¾ 20 ä¸­çš„ç»“æœï¼‰ã€‚ **LLaMA2-Chat çš„æ”¹è¿›ä¹Ÿä½¿æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒäº§ç”Ÿäº†æ¼‚ç§»ï¼ˆshiftï¼‰ã€‚** å¦‚æœä¸å°†è¿™ä¸ªæ–°çš„æ•°æ®åˆ†å¸ƒè¾“å…¥å¥–åŠ±æ¨¡å‹ï¼Œå®ƒçš„å‡†ç¡®æ€§ä¼šè¿…é€Ÿä¸‹é™ â€”â€” ä¾‹å¦‚ï¼Œhyper-specialization (Scialom et al., 2020b) ï¼Œâ€”â€” å› æ­¤åœ¨æ–°ä¸€è½® LLaMA2-Chat è°ƒä¼˜è¿­ä»£ä¹‹å‰ï¼Œ ä½¿ç”¨æœ€æ–°çš„åå¥½æ•°æ®è¿­ä»£ä¸€æ¬¡éå¸¸é‡è¦ã€‚ è¿™ä¸€æ­¥æœ‰åŠ©äºä¿æŒå¥–åŠ±æ¨¡å‹çš„æ•°æ®åˆ†å¸ƒå‡†ç¡®æ€§ï¼Œä¸ºæœ€æ–°æ¨¡å‹æä¾›å‡†ç¡®çš„å¥–åŠ±  
![è¡¨6](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/10-TABLE6-1.png)  
è¡¨ 6ï¼šç”¨äºå¥–åŠ±æ¨¡å‹çš„äººç±»åå¥½æ•°æ®ç»Ÿè®¡. æˆ‘ä»¬åˆ—å‡ºäº†ç”¨äºå¥–åŠ±å»ºæ¨¡çš„å¼€æºå’Œå†…éƒ¨æ”¶é›†çš„äººç±»åå¥½æ•°æ®ã€‚ è¯·æ³¨æ„ï¼ŒäºŒå…ƒäººç±»åå¥½æ¯”è¾ƒåŒ…å«å…±äº«ç›¸åŒæç¤ºï¼ˆå’Œä¹‹å‰çš„å¯¹è¯ï¼‰çš„ 2 ä¸ªå“åº”ï¼ˆé€‰æ‹©å’Œæ‹’ç»ï¼‰ã€‚ æ¯ä¸ªç¤ºä¾‹éƒ½åŒ…å«ä¸€ä¸ªæç¤ºï¼ˆåŒ…æ‹¬ä¹‹å‰çš„å¯¹è¯ï¼Œå¦‚æœæœ‰çš„è¯ï¼‰å’Œä¸€ä¸ªå“åº”ï¼Œè¿™æ˜¯å¥–åŠ±æ¨¡å‹çš„è¾“å…¥ã€‚ æˆ‘ä»¬æŠ¥å‘Šæ¯”è¾ƒæ¬¡æ•°ã€æ¯ä¸ªå¯¹è¯çš„å¹³å‡è½®æ•°ã€æ¯ä¸ªç¤ºä¾‹ã€æ¯ä¸ªæç¤ºå’Œæ¯ä¸ªå“åº”çš„å¹³å‡æ ‡è®°æ•°ã€‚ æœ‰å…³æ¯æ‰¹æ¬¡å…ƒæœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æ•°æ®çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜…é™„å½• A.3.1ã€‚

è¡¨ 6 æ€»ç»“äº†æˆ‘ä»¬çš„å¥–åŠ±æ¨¡å‹æ•°æ®ä¿¡æ¯ï¼Œå¹¶ä¸å¤šä¸ªå¼€æºåå¥½æ•°æ®é›†è¿›è¡Œäº†å¯¹æ¯”ï¼Œ åŒ…æ‹¬ Anthropic Helpful and Harmlessï¼ˆBai ç­‰ï¼Œ2022aï¼‰ï¼ŒOpenAI Summarizeï¼ˆStiennon ç­‰ï¼Œ2020ï¼‰ï¼Œ OpenAI WebGPTï¼ˆNakano ç­‰ï¼Œ2021ï¼‰ï¼ŒStackExchangeï¼ˆLambert ç­‰ï¼Œ2023ï¼‰ï¼Œ Stanford Human Preferencesï¼ˆEthayarajh ç­‰ï¼Œ2022ï¼‰å’Œ Synthetic GPT-Jï¼ˆHavrillaï¼‰ã€‚  
åŸºäºå‰é¢ä»‹ç»çš„æŒ‡å¯¼åŸåˆ™ï¼Œæˆ‘ä»¬æ”¶é›†çš„è¶…è¿‡ 100 ä¸‡ä¸ª binary comparisonï¼Œ å¾—åˆ°ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸º Meta reward modeling dataã€‚ è¯·æ³¨æ„ï¼Œæ ¹æ® text domain çš„ä¸åŒï¼Œæç¤ºå’Œå›ç­”ä¸­çš„ token æ•°é‡ä¼šä¸ä¸€æ ·ã€‚

- æ€»ç»“æ€§æ–‡æ¡£ï¼ˆsummarizationï¼‰å’Œåœ¨çº¿è®ºå›æ•°æ®é€šå¸¸ prompt æ›´é•¿ï¼Œ
- å¯¹è¯å¼ prompt é€šå¸¸è¾ƒçŸ­ã€‚
- ä¸ç°æœ‰çš„å¼€æºæ•°æ®é›†ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„åå¥½æ•°æ®å…·æœ‰æ›´å¤šçš„å¯¹è¯è½®æ¬¡ï¼Œå¹¶ä¸”é•¿åº¦æ›´é•¿ã€‚

### 3.2.2 å¥–åŠ±å»ºæ¨¡ï¼ˆReward Modelingï¼‰

å¥–åŠ±æ¨¡å‹çš„å·¥ä½œæœºåˆ¶ï¼š

- **è¾“å…¥ï¼šæ¨¡å‹çš„ response åŠå…¶ç›¸åº”çš„ promptï¼ˆåŒ…æ‹¬å‰å‡ è½®çš„ä¸Šä¸‹æ–‡ï¼‰**ï¼›
- **è¾“å‡ºï¼šä¸€ä¸ªæ ‡é‡åˆ†æ•°ï¼Œè¡¨ç¤ºæ¨¡å‹çš„ç”Ÿæˆè´¨é‡ï¼ˆä¾‹å¦‚æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ï¼‰**ã€‚

åˆ©ç”¨è¿™äº›åˆ†æ•°ä½œä¸ºå¥–åŠ±ï¼Œå¯ä»¥åœ¨ RLHF æœŸé—´ä¼˜åŒ– LLaMA2-Chatï¼Œå®ç°æ›´å¥½çš„äººç±»åå¥½å¯¹é½ï¼Œæ”¹è¿›æœ‰ç”¨æ€§å’Œå®‰å…¨æ€§ã€‚

æœ‰äººå·²ç»å‘ç°ï¼Œæœ‰ç”¨æ€§å’Œå®‰å…¨æ€§æœ‰æ—¶éœ€è¦æŠ˜ä¸­ï¼ˆBai ç­‰ï¼Œ2022aï¼‰ï¼Œè¿™å¯èƒ½ä¼šä½¿å•ä¸ªå¥–åŠ±æ¨¡å‹åœ¨ä¼˜åŒ–è¿™ä¸¤ä¸ªæ–¹é¢æ—¶å…·æœ‰æŒ‘æˆ˜æ€§ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸¤ä¸ªå•ç‹¬çš„å¥–åŠ±æ¨¡å‹ï¼Œ

- ä¸€ä¸ªé’ˆå¯¹æœ‰ç”¨æ€§è¿›è¡Œä¼˜åŒ–ï¼ˆç§°ä¸º Helpfulness RMï¼‰ï¼Œ
- ä¸€ä¸ªé’ˆå¯¹å®‰å…¨æ€§è¿›è¡Œä¼˜åŒ–ï¼ˆSafety RMï¼‰ã€‚

æˆ‘ä»¬ç”¨é¢„è®­ç»ƒçš„ LLaMA2-Chat checkpoint åˆå§‹åŒ–å¥–åŠ±æ¨¡å‹ï¼Œ

- è¿™ä½¿å¾—ä¸¤ä¸ªæ¨¡å‹éƒ½å—ç›Šäºé¢„è®­ç»ƒæ¨¡å‹å·²å­¦åˆ°çš„çŸ¥è¯†ã€‚ç®€è€Œè¨€ä¹‹ï¼Œå¥–åŠ±æ¨¡å‹â€œçŸ¥é“â€èŠå¤©æ¨¡å‹çŸ¥é“çš„æ‰€æœ‰å†…å®¹ã€‚
- è¿™å¯ä»¥é˜²æ­¢ä¸¤ä¸ªæ¨¡å‹ä¿¡æ¯ä¸åŒ¹é…ï¼Œä¾‹å¦‚ï¼Œè¿™å¯èƒ½å¯¼è‡´äº§ç”Ÿå¹»è§‰ï¼ˆhallucinationsï¼‰ã€‚
- æ¨¡å‹æ¶æ„å’Œè¶…å‚æ•°ä¸é¢„è®­ç»ƒæ¨¡å‹ç›¸åŒï¼Œåªæ˜¯ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ª token çš„ classification head æ›¿æ¢ä¸ºä¸€ä¸ª regression headï¼Œç”¨äºè¾“å‡ºæ ‡é‡å¥–åŠ±ã€‚

è®­ç»ƒç›®æ ‡  
ä¸ºäº†è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œæˆ‘ä»¬å°†æ”¶é›†çš„äººç±»åå¥½æ•°æ®è½¬æ¢ä¸º binary ranking label æ ¼å¼ï¼ˆå³ chosen & rejectedï¼‰ï¼Œ å¹¶å¼ºåˆ¶é€‰ä¸­çš„å“åº”æœ‰æ›´é«˜çš„åˆ†æ•°ã€‚æˆ‘ä»¬ä½¿ç”¨äº†ä¸ Ouyang ç­‰äººï¼ˆ2022ï¼‰ä¸€è‡´çš„ binary ranking lossï¼š

Lranking=âˆ’log(Ïƒ(rÎ¸(x,yc)âˆ’rÎ¸(x,yr)))  
where rÎ¸(x,y) is the scalar score output for prompt x and completion y with model weights Î¸. yc is the preferred response that annotators choose and yr is the rejected counterpart. Built on top of this binary ranking loss, we further modify it separately for better helpfulness and safety reward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g., significantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly teach the reward model to assign more discrepant scores to the generations that have more differences. To do so, we further add a margin component in the loss:

Lranking=âˆ’log(Ïƒ(rÎ¸(x,yc)âˆ’rÎ¸(x,yr)âˆ’m(r)))  
where the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin for pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27). We found this margin component can improve Helpfulness reward model accuracy especially on samples where two responses are more separable. More detailed ablation and analysis can be found in Table 28 in Appendix A.3.3.

Data Composition  
We combine our newly collected data with existing open-source preference datasets to form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models while we were in the process of collecting preference annotation data. We note that in the context of RLHF in this study, the role of reward signals is to learn human preference for LLaMA2-Chat outputs rather than any model outputs. However, in our experiments, we do not observe negative transfer from the open-source preference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better generalization for the reward model and prevent reward hacking, i.e. LLaMA2-Chat taking advantage of some weaknesses of our reward, and so artificially inflating the score despite performing less well. With training data available from different sources, we experimented with different mixing recipes for both Helpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the Helpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal parts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The Meta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta Helpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10% helpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected responses were deemed safe.

è®­ç»ƒç»†èŠ‚ï¼ˆTraining Detailsï¼‰  
We train for one epoch over the training data. In earlier experiments, we found that training longer can lead to over-fitting. We use the same optimizer parameters as for the base model. The maximum learning rate is 5 Ã— 10âˆ’6 for the 70B parameter LLaMA2-Chat and 1 Ã— 10âˆ’5 for the rest. The learning rate is decreased on a cosine learning rate schedule, down to 10% of the maximum learning rate. We use a warm-up of 3% of the total number of steps, with a minimum of 5. The effective batch size is kept fixed at 512 pairs, or 1024 rows per batch.

å¥–åŠ±æ¨¡å‹çš„ç»“æœï¼ˆReward Model Resultsï¼‰  
On each batch of human preference annotation for reward modeling, we held out 1000 examples as a test set to evaluate our models. We refer to the union of all prompts for the corresponding test sets as â€œMeta Helpfulnessâ€ and â€œMeta Safety,â€ respectively.

As reference points, we also evaluated other publicly available alternatives as baselines: SteamSHP-XL (Ethayarajh et al., 2022) based on FLAN-T5-xl, the Open Assistant reward model based on DeBERTa V3 Large (He et al., 2020), and GPT4 accessible through the OpenAIâ€™s API. Note that at inference time, as opposed to training, all the reward models can predict a scalar for a single output, without requiring to access its paired output. For GPT-4, we prompt with a zero-shot question â€œChoose the best answer between A and B,â€ where A and B are the two responses for comparison.

We report the results in terms of accuracy in Table 7. As expected, our own reward models perform the best on our internal test sets collected based on LLaMA2-Chat, with the Helpfulness reward model performing best on the Meta Helpfulness test set, and similarly the Safety reward model performing best on the Meta Safety test set. Overall, our reward models outperform all of the baselines, including GPT-4. Interestingly, GPT-4 performs better than other non-Meta reward models, despite not being trained directly nor targeting specifically this reward modeling task.

The fact that helpfulness and safety performed the best on their own domain is potentially due to the tension between the two objectives (i.e., being as helpful as possible versus refusing unsafe prompts when necessary), which may confuse the reward model during training. In order for a single model to perform well on both dimensions, it needs to not only learn to select the better response given a prompt but also to distinguish adversarial prompts from safe ones. As a result, optimizing two separate models eases the reward modeling task. More detailed analysis on this tension between safety and helpfulness can be found in Appendix A.4.1. When we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the â€œsignificantly betterâ€ test set and degrades gradually as comparison pairs become more similar (e.g., â€œslightly betterâ€). It is expected that learning to model human preferences becomes challenging when deciding between two similar model responses, due to annotator subjectivity and their reliance on nuanced details that may differentiate responses. We emphasize that the accuracy on more distinct responses matters the most to improve LLaMA2-Chat performance. The human preference annotation agreement rate is also higher on more distinct responses than similar pairs.

Scaling Trends  
We study the scaling trends in terms of data and model size for the reward model, finetuning different model sizes on an increasing amount of the reward model data collected each week (see the details on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger models obtain higher performance for a similar volume of data. More importantly, the scaling performance has not yet plateaued given the existing volume of data annotation used for training, a signal that there is room for more improvement with more annotations. We note that reward model accuracy is one of the most important proxies for the final performance of LLaMA2-Chat. While best practices for comprehensively evaluating a generative model is an open research question, the ranking task of the reward has no ambiguity. Therefore, everything else being equal, an improvement of the reward model can be directly translated into an improvement for LLaMA2-Chat.

### 3.2.3 Iterative Fine-Tuning

As we received more batches of human preference data annotation, we were able to train better reward models and collect more prompts. We therefore trained successive versions for RLHF models, referred to here as RLHF-V1, . . . , RLHF-V5. We explored RLHF fine-tuning with two main algorithms:

Proximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature.  
Rejection Sampling fine-tuning. We sample K outputs from the model and select the best candidate with our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also proposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step further, and use the selected outputs for a gradient update. For each prompt, the sample obtaining the highest reward score is considered the new gold standard. Similar to Scialom et al. (2020a), we then fine-tune our model on the new set of ranked samples, reinforcing the reward.  
The two RL algorithms mainly differ in:

Breadth â€” in Rejection Sampling, the model explores K samples for a given prompt, while only one generation is done for PPO.  
Depth â€” in PPO, during training at step t the sample is a function of the updated model policy from t âˆ’ 1 after the gradient update of the previous step. In Rejection Sampling fine-tuning, we sample all the outputs given the initial policy of our model to collect a new dataset, before applying the fine-tuning similar to SFT. However, since we applied iterative model updates, the fundamental differences between the two RL algorithms are less pronounced.  
Until RLHF (V4), we used only Rejection Sampling fine-tuning, and after that, we combined the two sequentially, applying PPO on top of the resulted Rejection Sampling checkpoint before sampling again.

Rejection Sampling  
We perform rejection sampling only with our largest 70B LLaMA2-Chat. All smaller models are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model capabilities into the smaller ones. We leave further analysis of the effect of this distillation for future work.

At each iterative stage, we sample K answers for each prompt from the most recent model. We score each sample given the best reward model accessible at the time of the experiment, and then select the best answer for a given prompt. In earlier versions of our model, up to RLHF V3, our approach was to confine answer selection solely to the â€œbagâ€ of samples gathered from the preceding iteration. For example, RLHF V3 was trained using only samples from RLHF V2. However, despite continuous improvement, this method led to a regression in some capabilities. For example, RLHF V3 struggled more than previous versions to compose rhyming lines in poems, as discerned through qualitative analysis, suggesting that further investigation into the causes of and mitigations for forgetting (Kirkpatrick et al., 2017; Nguyen et al., 2019; Ramasesh et al., 2021) could be a fruitful area for additional future research.

In response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from all prior iterations, such as those used in RLHF-V1 and RLHF-V2. Although we do not present specific figures, this adjustment demonstrated considerable enhancements in performance and effectively addressed the previously noted issues. This mitigation can be seen as analogous to Synnaeve et al. (2019) and Vinyals et al. (2019) in the RL literature.

We illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median curves can be interpreted as the potential gain of fine-tuning on the best output. As expected, this delta increases with more samples, since the maximum increases (i.e., more samples, more opportunities to generate a good trajectory), while the median remains stationary. There is a direct connection between the exploration and the maximum reward we can obtain among the samples. The temperature parameter also plays an important role for exploration, as a higher temperature enables us to sample more diverse outputs. In Figure 8, we report for a LLaMA2-Chat-SFT (left) and a LLaMA2-Chat-RLHF (right), the maximum reward curves among N samples (with N âˆˆ \[1, . . . , 100\]), for different temperatures. We can observe that the optimal temperature is not constant during the iterative model updates: RLHF has a direct impact on rescaling the temperature. For LLaMA2-Chat-RLHF, the optimal temperature when sampling between 10 and 100 outputs is T âˆˆ \[1.2, 1.3\]. Given a finite compute budget, it is therefore necessary to re-adjust the temperature progressively. Note that this temperature rescaling happens for a constant number of steps for each model, and always starting from the base model on each new RLHF version.

PPO  
We further train our language model following the RL scheme of Stiennon et al. (2020), which uses the reward model as an estimate for the true reward function (human preference) and the pretrained language model as the policy to optimize. During this phase, we seek to optimize the following objective: arg max

argmaxÏ€ğ”¼pâˆ¼D,gâˆ¼Ï€\[R(gâˆ£p)\]  
We iteratively improve the policy by sampling prompts p from our dataset D and generations g from the policy Ï€ and use the PPO algorithm and loss function to achieve this objective. The final reward function we use during optimization,

R(gâˆ£p)=RÌƒ c(gâˆ£p)âˆ’Î²DKL(Ï€Î¸(gâˆ£p)âˆ¥Ï€0(gâˆ£p))  
contains a penalty term for diverging from the original policy Ï€0. As was observed in other works (Stiennon et al., 2020; Ouyang et al., 2022), we find this constraint is useful for training stability, and to reduce reward hacking wherebywewould achieve high scores from the reward model but lowscores from human evaluation. We define Rc to be a piecewise combination of the safety (Rs) and helpfulness (Rh) reward models. We have tagged prompts in our dataset that might elicit potentially unsafe responses and prioritize the scores from the safety model. The threshold of 0.15 is chosen for filtering unsafe responses, corresponding to a precision of 0.89 and a recall of 0.55 evaluated on the Meta Safety test set. We also find it important to whiten the final linear scores (shown here by reversing the sigmoid with the logit function) in order to increase stability and balance properly with the KL penalty term (Î²) above.

For all models, we use the AdamW optimizer (Loshchilov and Hutter, 2017), with Î²1 = 0.9, Î²2 = 0.95, eps = 10âˆ’5. We use a weight decay of 0.1, gradient clipping of 1.0, and a constant learning rate of 10âˆ’6. For each PPO iteration we use a batch size of 512, a PPO clip threshold of 0.2, a mini-batch size of 64, and take one gradient step per mini-batch. For the 7B and 13B models, we set Î² = 0.01 (KL penalty), and for the 34B and 70B models, we set Î² = 0.005.

We train for between 200 and 400 iterations for all our models, and use evaluations on held-out prompts for early stopping. Each iteration of PPO on the 70B model takes on average â‰ˆ 330 seconds. To train quickly with large batch sizes, we use FSDP (Zhao et al., 2023). This was effective when using O(1) forward or backward passes, but caused a large slow down (â‰ˆ 20Ã—) during generation, even when using a large batch size and KV cache. We were able to mitigate this by consolidating the model weights to each node once before generation and then freeing the memory after generation, resuming the rest of the training loop.

3.3 System Message for Multi-Turn Consistency  
In a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly, or to â€œact asâ€ some public figure. When we provided such instructions to LLaMA2-Chat, the subsequent response should always respect the constraint. However, our initial RLHF models tended to forget the initial instruction after a few turns of dialogue, as illustrated in Figure 9 (left).

To address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context Distillation (Bai et al., 2022b) that hacks the fine-tuning data to help the attention focus in a multi-stage process. GAtt enables dialogue control over multiple turns, as illustrated in Figure 9 (right).

GAtt Method. Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user and an assistant), with a list of messages \[u1, a1, . . . , un, an\], where un and an correspond to the user and assistant messages for turn n, respectively. Then, we define an instruction, inst, that should be respected throughout the dialogue. For example, inst could be â€œact as.â€ We can then synthetically concatenate this instruction to all the user messages of the conversation.

Next, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue and the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of augmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this would lead to a mismatch at training time between the system message, i.e., all the intermediate assistant messages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we simply set the loss to 0 for all the tokens from the previous turns, including assistant messages.

For the training instructions, we created a few synthetic constraints to sample from: Hobbies (â€œYou enjoy e.g. Tennisâ€), Language (â€œSpeak in e.g. Frenchâ€), or Public Figure (â€œAct as e.g. Napoleonâ€). To obtain the lists of hobbies and public figures, we asked LLaMA2-Chat to generate it, avoiding a mismatch between the instruction and model knowledge (e.g., asking the model to act as someone it had not encountered during training). To make the instructions more complex and diverse, we construct the final instruction by randomly combining the above constraints. When constructing the final system message for the training data, we also modify the original instruction half of the time to be less verbose, e.g., â€œAlways act as Napoleon from nowâ€-> â€Figure: Napoleon.â€ These steps produce an SFT dataset, on which we can fine-tune LLaMA2-Chat.

GAtt Evaluation. We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is consistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5). We tried to set constraints not present in the training of GAtt at inference time, for instance â€œAlways answer with Haiku,â€ for which the model remained consistent as illustrated in Appendix Figure 28.

To illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention activations of the model in Figure 10. The left-hand side of each figure corresponds to the system message (â€œAct as OscarWildeâ€). We can see that the GAtt-equipped model (right) maintains large attention activations with respect to the system message for a larger portion of the dialogue, as compared to the model without GAtt (left). Despite its utility, the current implementation of GAtt is vanilla, and more development and iteration on this technique could likely further benefit the model. For instance, we could teach the model to change the system message during the conversation by integrating such data during fine-tuning.

## 3.4 RLHF Results

3.4.1 Model-Based Evaluation  
Evaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can be complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always scalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1 to V5, we first observed the improvement of the rewards from the latest reward models, to save costs and increase iteration speed. We later validated major model versions with human evaluations.

How Far Can Model-Based Evaluation Go? To measure the robustness of our reward model, we collected a test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the answers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall are well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This confirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise Ranking Loss.

Still, as Goodhartâ€™s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure our measure wonâ€™t diverge from the human preferences, we additionally used a more general reward, trained on diverse open-source Reward Modeling datasets. We have not yet observed any such divergence, and hypothesize that iterative model updates may be helping to prevent this. As a last verification step to ensure no regression between our new model and the previous one, we use both to sample during the next annotation iteration. This enables a model comparison â€œfor freeâ€ on new prompts and can help to increase diversity when sampling.

Progression of Models. Figure 11 reports the progress of our different SFT and then RLHF versions for both Safety and Helpfulness axes, measured by our in-house Safety and Helpfulness reward models. On this set of evaluations, we outperform ChatGPT on both axes after RLHF-V3 (harmlessness and helpfulness

50%). Despite the aforementioned relevance of using our reward as a point-wise metric, it can arguably be biased in favor of LLaMA2-Chat. Therefore, for a fair comparison, we additionally compute the final results using GPT-4 to assess which generation is preferred. The order in which ChatGPT and LLaMA2-Chat outputs appeared in GPT-4 prompt are randomly swapped to avoid any bias. As expected, the win-rate in favor of LLaMA2-Chat is less pronounced, although obtaining more than a 60% win-rate for our latest LLaMA2-Chat. The prompts correspond to a validation set of 1, 586 and 584 prompts for safety and helpfulness, respectively.

### 3.4.2 Human Evaluation

Human evaluation is often considered the gold standard for judging models for natural language generation, including dialogue models. To evaluate the quality of major model versions, we asked human evaluators to rate them on helpfulness and safety. We compare the LLaMA2-Chat models to open-source models (Falcon, MPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat- GPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT, we use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all generations. The final prompt count for human evaluations for each model is shown in Table 32. See more methodology details in Appendix, Section A.3.7. The following section shows helpfulness results; safety results are presented in Section 4.4.

Results. As shown in Figure 12, LLaMA2-Chat models outperform open-source models by a significant margin on both single turn and multi-turn prompts. Particularly, LLaMA2-Chat 7B model outperforms MPT-7B-chat on 60% of the prompts. LLaMA2-Chat 34B has an overall win rate of more than 75% against equivalently sized Vicuna-33B and Falcon 40B models.

The largest LLaMA2-Chat model is competitive with ChatGPT. LLaMA2-Chat 70B model has a win rate of 36% and a tie rate of 31.5% relative to ChatGPT. LLaMA2-Chat 70B model outperforms PaLM-bison chat model by a large percentage on our prompt set. More results and analysis is available in Section A.3.7. Inter-Rater Reliability (IRR). In our human evaluations, three different annotators provided independent assessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as better from a data quality perspective, however, context is important. Highly subjective tasks like evaluating the overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling tasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will benefit the research community.

We used Gwetâ€™s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to be the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness task that is used in our analysis, Gwetâ€™s AC2 score varies between 0.37 and 0.55 depending on the specific model comparison. We see scores on the lower end of that range for ratings from model comparisons with similar win rates to each other (like the LLaMA2-Chat-70B-chat vs. ChatGPT comparison). We see scores on the higher end of that range for ratings from model comparisons with a more clear winner (like the Llama 2-Chat-34b-chat vs. Falcon-40b-instruct).

Limitations of human evaluations. While our results indicate that LLaMA2-Chat is on par with ChatGPT on human evaluations, it is important to note that human evaluations have several limitations.

By academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover real-world usage of these models, which will likely cover a significantly larger number of use cases.  
Diversity of the prompts could be another factor in our results. For example, our prompt set does not include any coding- or reasoning-related prompts.  
We only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be to ask the models to complete a task and rate the overall experience with the model over multiple turns.  
Human evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a different set of prompts or with different instructions could result in different results.

# 4 Safetyï¼ˆç•¥ï¼‰

.....

# 5 è®¨è®º

åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è®¨è®ºæˆ‘ä»¬åœ¨ RLHF ä¸­è§‚å¯Ÿåˆ°çš„æœ‰è¶£ç‰¹æ€§ï¼ˆç¬¬ 5.1 èŠ‚ï¼‰ã€‚ ç„¶åæˆ‘ä»¬è®¨è®º Llama 2-Chat çš„å±€é™æ€§ï¼ˆç¬¬ 5.2 èŠ‚ï¼‰ã€‚ æœ€åï¼Œæˆ‘ä»¬æå‡ºäº†è´Ÿè´£ä»»åœ°å‘å¸ƒè¿™äº›æ¨¡å‹çš„ç­–ç•¥ï¼ˆç¬¬ 5.3 èŠ‚ï¼‰

## 5.1 æ–°å‘ç°ä¸è¯„è®ºï¼ˆLearnings and Observationsï¼‰

æˆ‘ä»¬çš„è°ƒä¼˜è¿‡ç¨‹æ­ç¤ºäº†ä¸€äº›æœ‰è¶£çš„ç»“æœï¼Œæ¯”å¦‚ LLaMA2-Chat åœ¨æ—¶é—´ç»´åº¦ä¸Š**ç»„ç»‡çŸ¥è¯†çš„èƒ½åŠ›ï¼Œä»¥åŠè°ƒç”¨å¤–éƒ¨å·¥å…· API çš„èƒ½åŠ›**ã€‚

**è¶…è¶Šäººç±»ç›‘ç£ï¼šä» SFT åˆ° RLHF**  
åœ¨é¡¹ç›®å¼€å§‹æ—¶ï¼Œæˆ‘ä»¬ä¸­çš„è®¸å¤šäººéƒ½å€¾å‘äºä½¿ç”¨æœ‰ç›‘ç£æ ‡æ³¨ï¼ˆsupervised annotationï¼‰ï¼Œ è¢«å…¶æ›´å¯†é›†çš„ä¿¡å·æ‰€å¸å¼•ã€‚ åŒæ—¶ï¼Œ**å¼ºåŒ–å­¦ä¹ ï¼ˆreinforcement learningï¼‰çš„ä¸ç¨³å®šæ€§å·²ç»ä¼—æ‰€å‘¨çŸ¥**ï¼Œ å› æ­¤è‡ªç„¶è¯­è¨€å¤„ç†é¢†åŸŸå¯¹å…¶è¿˜æ˜¯æŠ±æœ‰ä¸€ç§æ€€ç–‘æ€åº¦ã€‚ ä½†äº‹å®==è¯æ˜å¼ºåŒ–å­¦ä¹ éå¸¸æœ‰æ•ˆï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°å…¶æˆæœ¬å’Œæ—¶é—´æ•ˆç‡==ã€‚ æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè®¤ä¸ºï¼š

- **RLHF æˆåŠŸçš„å…³é”®æ˜¯å®ƒåœ¨ æ ‡æ³¨è¿‡ç¨‹ä¸­ä¿ƒè¿›äº†äººå’Œ LLM ä¹‹é—´çš„ååŒï¼ˆthe synergy it fosters between humans and LLMsï¼‰**ã€‚
- å³ä½¿æ˜¯ç†Ÿç»ƒçš„æ ‡æ³¨å‘˜ï¼Œæ¯ä¸ªäººçš„æ ‡æ³¨ï¼ˆä¹¦å†™ï¼‰é£æ ¼ä¹Ÿå­˜åœ¨æ˜¾è‘—å·®å¼‚ã€‚**ç»è¿‡ SFT æ ‡æ³¨å¾®è°ƒå‡ºæ¥çš„æ¨¡å‹å­¦ä¹ åˆ°äº†è¿™ç§å¤šæ ·æ€§â€”â€” ä¸å¹¸çš„æ˜¯ï¼Œè¿™åŒ…æ‹¬é‚£äº›æ ‡æ³¨è´¨é‡å¾ˆå·®çš„é•¿å°¾éƒ¨åˆ†**ã€‚
- **æ¨¡å‹æ€§èƒ½å—é™äºæœ€ç†Ÿç»ƒçš„æ ‡æ³¨å‘˜çš„èƒ½åŠ›**

==å½“æ¯”è¾ƒä¸¤ä¸ªå›ç­”å“ªä¸ªæ›´å¥½æ—¶ï¼Œäººç±»æ ‡æ³¨å‘˜çš„åˆ¤æ–­åŸºæœ¬ä¸Šéƒ½æ˜¯ä¸€è‡´çš„ã€‚ å› æ­¤ï¼Œå¥–åŠ±æœºåˆ¶è¿…é€Ÿä¼šå°†ä½åˆ†åˆ†é…ç»™è´¨é‡å·®çš„å°¾éƒ¨ï¼Œå¹¶æœç€äººç±»åå¥½å¯¹é½==ã€‚ è¿™ä¸€ç°è±¡åœ¨å›¾ 20 ä¸­æœ‰æ‰€ä½“ç°ï¼Œå¯ä»¥çœ‹åˆ°ç»è¿‡å‡ è½®è¿­ä»£ï¼Œ**æœ€å·®çš„ç­”æ¡ˆé€æ¸è¢«ç§»é™¤ï¼Œä½¿åˆ†å¸ƒå‘å³ç§»åŠ¨ã€‚**

![å›¾20](https://github.com/Walker-DJ1/blog_data/raw/main/llama2/31-FIGURE20-1.png)  
å›¾20ï¼š 2-Chat çš„æ¸è¿›ç‰ˆæœ¬çš„åˆ†å¸ƒè½¬å˜ï¼Œä» SFT æ¨¡å‹è½¬å‘ RLHF

æ­¤å¤–ï¼Œåœ¨æ ‡æ³¨è¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹ç”šè‡³æœ‰æ½œåŠ›æ¢ç´¢è¶…è¿‡æœ€ä¼˜ç§€çš„æ ‡æ³¨å‘˜çš„å†™ä½œè½¨è¿¹ï¼ˆventure into writing trajectoriesï¼‰ã€‚ ä½†å½“æ¯”è¾ƒä¸¤ä¸ªå›ç­”æ—¶ï¼Œäººç±»ä»ç„¶å¯ä»¥æä¾›æœ‰ä»·å€¼çš„åé¦ˆï¼Œè¶…è¶Šä»–ä»¬è‡ªå·±çš„å†™ä½œèƒ½åŠ›ã€‚ ç±»æ¯”ä¸€ä¸‹å°±æ˜¯ï¼š==è™½ç„¶æˆ‘ä»¬å¹¶ä¸éƒ½æ˜¯å‡ºè‰²çš„è‰ºæœ¯å®¶ï¼Œä½†æ¬£èµå’Œæ‰¹è¯„è‰ºæœ¯çš„èƒ½åŠ›ä»ç„¶æ˜¯æœ‰çš„==ã€‚ æˆ‘ä»¬è®¤ä¸ºï¼ŒLLM åœ¨æŸäº›ä»»åŠ¡ä¸­è¶…è¶Šäººç±»æ ‡æ³¨å‘˜çš„å“è¶Šå†™ä½œèƒ½åŠ›ï¼Œ åŸºæœ¬ä¸Šæ˜¯ç”± RLHF é©±åŠ¨çš„ï¼ŒGilardi ç­‰äººï¼ˆ2023ï¼‰å’Œ Huang ç­‰äººï¼ˆ2023ï¼‰ä¹‹å‰ä¹Ÿå·²ç»æåˆ°è¿™ä¸€ç‚¹ã€‚  
\*\*Supervised data å¯èƒ½ä¸å†æ˜¯é»„é‡‘æ ‡å‡†ï¼Œè¿™ç§æ¼”å˜è¿«ä½¿æˆ‘ä»¬é‡æ–°è¯„ä¼°â€œç›‘ç£â€ï¼ˆsupervisionï¼‰\*\*è¿™ä¸€æ¦‚å¿µã€‚

**In-Context Temperature Rescaling**  
æˆ‘ä»¬è§‚å¯Ÿåˆ°ä¸€ä¸ª RLHF ç›¸å…³çš„æœ‰è¶£ç°è±¡ï¼Œå°±ç›®å‰æ‰€çŸ¥ï¼Œä¹‹å‰è¿˜æ²¡æœ‰æ–‡ç« æåˆ°è¿™ä¸€ç‚¹ï¼š dynamic re-scaling of temperature contingent upon the contextã€‚ å¦‚å›¾ 8 æ‰€æš—ç¤ºï¼Œæ¸©åº¦ä¼¼ä¹å—åˆ° RLHF çš„å½±å“ï¼Œ

æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¿˜æ­ç¤ºäº†è¿™äº›åˆ†å¸ƒæ¼‚ç§»ï¼ˆshiftsï¼‰å¯¹ prompts çš„åˆ†å¸ƒå¹¶éå‡åŒ€çš„ï¼Œ å¦‚å›¾ 21 æ‰€ç¤ºï¼Œ

å›¾ 21ï¼šRLHF å­¦ä¼šäº†æ ¹æ® prompt ç±»å‹é€‚åº”æ¸©åº¦ã€‚ Lower Self-BLEU corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more diversity when generating responses to creative prompts. We prompt each model with a diverse set of 10 creative and 10 factual instructions and sample 25 responses. This is repeated for the temperatures T âˆˆ {k/10 | k âˆˆ N : 1 â‰¤ k â‰¤ 15}. For each of the 25 responses we compute the Self-BLEU metric and report the mean and standard deviation against the temperatue

ä¾‹å¦‚ï¼Œ  
å·¦è¾¹æ˜¯åŸºäºäº‹å®ä¿¡æ¯çš„æç¤ºï¼Œå¦‚ â€œWhat is the capital of ?â€ï¼Œ  
Self-BLEU æ–œç‡éšæ—¶é—´å‡å°ã€‚è¿™è¡¨æ˜è™½ç„¶æ¸©åº¦åœ¨ä¸Šå‡ï¼Œä½†æ¨¡å‹å­¦ä¼šäº†å¯¹äº‹å®ç±» prompts æä¾›ä¸€è‡´å’Œç›¸åŒçš„å›ç­”ã€‚  
å³è¾¹æ˜¯åˆ›é€ åŠ›ç›¸å…³çš„æç¤ºï¼Œå¦‚ â€œWrite a poem,â€ã€‚  
å¤šæ¬¡ RLHF è¿­ä»£åï¼Œæ¸©åº¦çš„å¢åŠ ä»ç„¶ä¼šä½¿äº§ç”Ÿçš„å›ç­”å‘ç”Ÿå˜åŒ–ã€‚ è¿™å¯ä»¥åœ¨ Self-BLEU æ–œç‡ä¸­è§‚å¯Ÿåˆ°ï¼Œå®ƒå‘ˆç°å‡ºä¸ SFT æ¨¡å‹ç›¸ä¼¼çš„æ¨¡å¼ã€‚

LLaMA2-Chat æ—¶é—´æ„ŸçŸ¥èƒ½åŠ›ï¼ˆTemporal Perceptionï¼‰  
LLaMA2-Chat å±•ç¤ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„æ³›åŒ–æˆ–æ¨å¹¿èƒ½åŠ›ï¼ˆgeneralization abilityï¼‰ï¼Œ å¦‚ä¸‹å›¾æ‰€ç¤ºï¼ˆæ³¨æ„æ¯ä¸ªå¯¹è¯çš„çŸ¥è¯†æˆªæ­¢æ—¶é—´ï¼‰ï¼š

å›¾ 22ï¼šæ—¶é—´æ„ŸçŸ¥èƒ½åŠ› â€”â€” ä½¿ç”¨äº† 1,000 SFT time-focused dataï¼ŒLLaMA2-Chat å­¦åˆ°äº†â€œæ—¶é—´â€æ¦‚å¿µã€‚

ä¸ºäº†åœ¨ LLaMA2-Chat ä¸­çŒè¾“æ—¶é—´çš„æ¦‚å¿µï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸ç‰¹å®šæ—¥æœŸç›¸å…³çš„ 1,000 ä¸ª SFT ç¤ºä¾‹ã€‚ è¿™äº›ç¤ºä¾‹åŒ…æ‹¬è¯¸å¦‚ â€œHow long ago did Barack Obama become president?â€ çš„é—®é¢˜ã€‚ æ¯ä¸ªé—®é¢˜éƒ½ä¸ä¸¤ä¸ªå…³é”®çš„å…ƒæ•°æ®ç›¸å…³è”ï¼š

- æé—®æ—¶çš„æ—¥æœŸï¼šè¿™ä¼šå½±å“å›ç­”ï¼›
- äº‹ä»¶æ—¥æœŸï¼šåœ¨æ­¤æ—¥æœŸä¹‹å‰ï¼Œè¯¥é—®é¢˜å°†æ¯«æ— æ„ä¹‰ã€‚

æˆ‘ä»¬æ‰‹åŠ¨æµ‹è¯•äº†å‡ åä¸ªç¤ºä¾‹ï¼Œè§‚å¯Ÿåˆ°å³ä½¿åªæä¾›ä¸€ä»½æœ€å°æ•°æ®ï¼Œ æˆ‘ä»¬çš„æ¨¡å‹ä¹Ÿè¡¨ç°å‡ºäº†åœ¨æ—¶é—´ä¸Šç»„ç»‡çŸ¥è¯†ï¼ˆorganize its knowledge in a temporal mannerï¼‰çš„å¼ºå¤§èƒ½åŠ›ã€‚ ä»¥ä¸Šè§‚å¯Ÿç»“æœè¡¨æ˜ï¼Œå°½ç®¡ LLM çš„è®­ç»ƒä»…æ¶‰åŠä¸¤æ–¹é¢ï¼š

- è®­ç»ƒæ–¹å¼ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª token
- è®­ç»ƒæ•°æ®ï¼šæ—¶é—´ä¸Šéšæœºå’Œæ— åºçš„æ•°æ®  
    ä½†å®ƒä»¬å¯¹æ—¶é—´çš„æ¦‚å¿µå†…åŒ–ç¨‹åº¦ï¼ˆinternalized the concept of timeï¼‰ æ¯”å…ˆå‰é¢„è®¡çš„æ›´é«˜å¾ˆå¤šã€‚

**å·¥å…·çš„ä½¿ç”¨**  
LLM ä¸å·¥å…·çš„æ•´åˆæ˜¯ä¸€ä¸ªæ­£åœ¨å‘å±•å£®å¤§çš„ç ”ç©¶é¢†åŸŸï¼ˆMialon ç­‰ï¼Œ2023ï¼‰ã€‚ Toolformerï¼ˆSchick ç­‰ï¼Œ2023ï¼‰è®¾è®¡çš„æ–¹æ³•åŒ…æ‹¬å¯¹æ•°ç™¾ä¸‡æ¡è½¨è¿¹è¿›è¡Œé‡‡æ ·ï¼Œ åŒæ—¶ä¸ºæ¯ä¸ªå·¥å…·åˆ¶å®šä¸€äº› few-shot examplesã€‚ ç„¶è€Œï¼Œè¯¥æŠ€æœ¯ä»…é€‚ç”¨äºæ¯ä¸ªç¤ºä¾‹ä½¿ç”¨å•ä¸ªå·¥å…·ï¼ˆusing a single tool per exampleï¼‰çš„æƒ…å†µï¼Œ æ— æ³•æ‰©å±•åˆ°è¿ç»­ä½¿ç”¨å¤šä¸ªå·¥å…·çš„åœºæ™¯ã€‚  
OpenAI å‘å¸ƒçš„æ’ä»¶å¼•å‘äº†å­¦æœ¯ç•Œçš„å¹¿æ³›è®¨è®ºï¼Œå¼•å‡ºäº†ä¸€äº›é—®é¢˜ï¼Œä¾‹å¦‚ï¼š

- å¦‚ä½•æœ‰æ•ˆåœ°æ•™æ¨¡å‹ä½¿ç”¨å·¥å…·ï¼Ÿ
- è¿™ä¸ªè¿‡ç¨‹æ˜¯å¦éœ€è¦å¤§é‡çš„æ•°æ®é›†ï¼Ÿ

æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼Œåœ¨å¯¹é½è¿‡ç¨‹ä¸­ï¼Œå¤§æ¨¡å‹ä¼šè‡ªå‘åœ°å‡ºç°é›¶æ ·æœ¬æ–¹å¼ã€‚ å›¾ 23 å±•ç¤ºäº†ä¸€ä¸ªä¾‹å­ï¼Œå°½ç®¡æˆ‘ä»¬ä»æœªæ˜ç¡®æ ‡æ³¨è¿‡å·¥å…·ä½¿ç”¨ï¼Œä½†æ¨¡å‹å±•ç¤ºäº†åœ¨é›¶æ ·æœ¬ç¯å¢ƒä¸­åˆ©ç”¨ä¸€ç³»åˆ—å·¥å…·çš„èƒ½åŠ›ï¼Œ

å›¾ 23ï¼šLLaMA2-Chat æ¶Œç°å‡ºçš„å·¥å…·ä½¿ç”¨èƒ½åŠ›ã€‚ æ— éœ€ä¸“é—¨è®­ç»ƒï¼Œä»…é€šè¿‡è¯­ä¹‰ï¼ˆsenmanticsï¼‰ï¼Œå®ƒå°±èƒ½ç†è§£å·¥å…·çš„ç”¨é€”å’Œç”¨æ³•ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬çš„ç ”ç©¶è¿˜æ‰©å±•åˆ°äº† LLaMA2-Chat èƒ½ä½¿ç”¨è®¡ç®—å™¨ä¹‹åçš„æ€§èƒ½ã€‚ç»“æœè§è¡¨ 15ï¼Œ  
LLM çš„å·¥å…·ä½¿ç”¨è™½ç„¶ä»¤äººå…´å¥‹ï¼Œä½†ä¹Ÿå¯èƒ½å¼•å‘ä¸€äº›å®‰å…¨é—®é¢˜ã€‚ æˆ‘ä»¬é¼“åŠ±åœ¨è¿™ä¸ªé¢†åŸŸè¿›è¡Œæ›´å¤šçš„ç¤¾åŒºç ”ç©¶å’Œæµ‹è¯•ã€‚

## 5.2 é™åˆ¶å’Œä¼¦ç†è€ƒè™‘

LLaMA2-Chat ä¸å…¶ä»– LLM ç±»ä¼¼ï¼Œéƒ½æœ‰ä¸€äº› well-recognized çš„é™åˆ¶ï¼Œ åŒ…æ‹¬

- é¢„è®­ç»ƒåçŸ¥è¯†åœæ­¢æ›´æ–°ï¼›
- å¯èƒ½ç”Ÿæˆéäº‹å®å†…å®¹ï¼ˆnon-factual generationï¼‰ï¼Œå¦‚ä¸åˆæ ¼çš„å»ºè®®ï¼›
- æ˜“äºäº§ç”Ÿå¹»è§‰çš„å€¾å‘ï¼ˆpropensity towards hallucinationsï¼‰ã€‚  
    æ­¤å¤–ï¼Œæœ€åˆçš„ LLaMA2-Chat ç‰ˆæœ¬ä¸»è¦é›†ä¸­åœ¨è‹±è¯­æ•°æ®ä¸Šã€‚ è™½ç„¶æˆ‘ä»¬çš„å®éªŒè¡¨æ˜åœ¨å…¶ä»–è¯­è¨€ä¸Šä¹Ÿå·²ç»è·å¾—äº†ä¸€å®šçš„ç†Ÿç»ƒåº¦ï¼Œä½†è¿™ç§ç†Ÿç»ƒåº¦ä»ç„¶å—é™ï¼Œ ä¸»è¦æ˜¯ç”±äºéè‹±è¯­çš„é¢„è®­ç»ƒæ•°æ®æœ‰é™ï¼ˆå¦‚è¡¨ 10 æ‰€ç¤ºï¼‰ã€‚ å› æ­¤ï¼Œè¯¥æ¨¡å‹åœ¨è‹±è¯­ä»¥å¤–çš„è¯­è¨€ä¸­æ€§èƒ½ä»ç„¶è¾ƒå¼±ï¼Œåº”è°¨æ…ä½¿ç”¨ã€‚

ä¸å…¶ä»– LLM ä¸€æ ·ï¼ŒLLaMA2 å¯èƒ½ä¼šç”Ÿæˆæœ‰å®³ã€å†’çŠ¯æˆ–å¸¦æœ‰åè§çš„å†…å®¹ï¼Œ å› ä¸ºå®ƒæ˜¯åŸºäºå…¬å¼€å¯ç”¨çš„åœ¨çº¿æ•°æ®é›†è®­ç»ƒçš„ã€‚ æˆ‘ä»¬å°è¯•é€šè¿‡å¾®è°ƒæ¥å‡è½»è¿™ä¸ªé—®é¢˜ï¼Œä½†ä¸€äº›é—®é¢˜å¯èƒ½ä»ç„¶å­˜åœ¨ï¼Œç‰¹åˆ«æ˜¯åœ¨è‹±è¯­ä»¥å¤–çš„è¯­è¨€ä¸­ï¼Œ å› ä¸ºè¿™äº›è¯­è¨€çš„å…¬å¼€å¯ç”¨æ•°æ®è¾ƒå°‘ã€‚ éšç€åœ¨è§£å†³è¿™äº›é—®é¢˜ä¸Šçš„è¿›å±•ï¼Œæˆ‘ä»¬å°†ç»§ç»­è¿›è¡Œå¾®è°ƒå¹¶å‘å¸ƒæ›´æ–°ç‰ˆæœ¬ã€‚

åäººä¹Ÿå¯èƒ½ä½¿ç”¨ AI æ¨¡å‹ï¼Œå› æ­¤èŠå¤©å¼ AI agent å¯èƒ½è¢«ç”¨äºæ¶æ„ç›®çš„ï¼Œ å¦‚ç”Ÿæˆé”™è¯¯ä¿¡æ¯æˆ–è·å–å…³äºç”Ÿç‰©ææ€–ä¸»ä¹‰æˆ–ç½‘ç»œçŠ¯ç½ªç­‰ä¸»é¢˜çš„ä¿¡æ¯ã€‚ æˆ‘ä»¬å·²ç»åŠªåŠ›è°ƒæ•´æ¨¡å‹ï¼Œé¿å…æ¶‰åŠè¿™äº›ä¸»é¢˜ï¼Œå¹¶å‡å°‘å…¶åœ¨è¿™äº›ç”¨ä¾‹ä¸­å¯èƒ½æä¾›çš„èƒ½åŠ›ã€‚

è™½ç„¶æˆ‘ä»¬è¯•å›¾åœ¨å®‰å…¨æ€§å’Œæœ‰ç”¨æ€§ä¹‹é—´å–å¾—åˆç†çš„å¹³è¡¡ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„å®‰å…¨è°ƒæ•´å¯èƒ½è¿‡äºè°¨æ…ã€‚ LLaMA2-Chat çš„ç”¨æˆ·å¯èƒ½ä¼šè§‚å¯Ÿåˆ°è¿‡äºè°¨æ…çš„å¤„ç†æ–¹å¼ï¼Œ æ¨¡å‹å¯èƒ½ä¼šæ‹’ç»æŸäº›è¯·æ±‚ï¼Œæˆ–æ‹’ç»å›ç­”æŸäº›å®‰å…¨ç»†èŠ‚é—®é¢˜ã€‚ é¢„è®­ç»ƒæ¨¡å‹çš„ç”¨æˆ·è¦æ ¼å¤–è°¨æ…ï¼Œè¯·æŒ‰ç…§æˆ‘ä»¬çš„â€œè´Ÿè´£ä»»ä½¿ç”¨æŒ‡å—â€ä¸­æ‰€è¿°ï¼Œé‡‡å–é¢å¤–çš„è°ƒä¼˜å’Œéƒ¨ç½²ã€‚

## 5.3 è´Ÿè´£ä»»çš„å‘å¸ƒç­–ç•¥ï¼ˆResponsible Release Strategyï¼‰

#### 5.3.1 å‘å¸ƒç»†èŠ‚

LLaMA2 å…è®¸ç”¨äºç ”ç©¶å’Œå•†ä¸šç”¨é€”ã€‚ ä½¿ç”¨ LLaMA2 çš„äººå¿…é¡»éµå®ˆå…¶è®¸å¯è¯å’Œæˆ‘ä»¬çš„ Acceptable Use Policyï¼Œç¦æ­¢ä»»ä½•è¿åæ”¿ç­–ã€æ³•å¾‹ã€è§„åˆ™å’Œæ³•è§„çš„ç”¨é€”ã€‚

æˆ‘ä»¬è¿˜æä¾›äº†ä»£ç ç¤ºä¾‹ï¼Œä»¥å¸®åŠ©å¼€å‘è€…é‡å¤æˆ‘ä»¬åœ¨ LLaMA2-Chat ä¸­çš„ safe generationsï¼Œ ä»¥åŠåœ¨ç”¨æˆ·è¾“å…¥å’Œæ¨¡å‹è¾“å‡ºå±‚åº”ç”¨ï¼ˆapplyï¼‰ä¸€äº›åŸºç¡€å®‰å…¨æŠ€æœ¯ã€‚

æœ€åï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä»½â€œè´Ÿè´£ä»»ä½¿ç”¨æŒ‡å—â€ï¼ˆResponsible Use Guideï¼‰ï¼Œ é‡Œé¢æœ‰å…³äºå®‰å…¨å¼€å‘å’Œéƒ¨ç½²ï¼ˆsafe development and deploymentï¼‰çš„æŒ‡å¯¼åŸåˆ™ã€‚

#### 5.3.2 è´Ÿè´£ä»»çš„å‘å¸ƒ

è®¸å¤šå…¬å¸é€‰æ‹©å…³èµ·é—¨æ¥è‡ªå·±é€  AIï¼Œä½†æˆ‘ä»¬å†³å®šå…¬å¼€å‘å¸ƒ LLaMA2ï¼Œä»¥é¼“åŠ±è´Ÿè´£ä»»çš„äººå·¥æ™ºèƒ½åˆ›æ–°ã€‚ æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œå¼€æ”¾çš„æ–¹å¼èƒ½å€ŸåŠ© AI ç¤¾åŒºçš„é›†ä½“æ™ºæ…§ã€å¤šæ ·æ€§å’Œåˆ›é€ åŠ›ï¼Œå¯¹è¿™é¡¹æŠ€æœ¯çš„æ™®æ›´æœ‰æ„ä¹‰ã€‚

åˆä½œå°†ä½¿è¿™äº›æ¨¡å‹å˜å¾—æ›´å¥½ã€æ›´å®‰å…¨ã€‚æ•´ä¸ªäººå·¥æ™ºèƒ½ç¤¾åŒº â€”â€” å­¦æœ¯ç ”ç©¶äººå‘˜ã€å…¬æ°‘ç¤¾ä¼šã€å†³ç­–è€…å’Œäº§ä¸šç•Œ â€”â€” å¿…é¡»å…±åŒåŠªåŠ›ï¼Œå¯¹å½“å‰äººå·¥æ™ºèƒ½ç³»ç»Ÿçš„é£é™©è¿›è¡Œä¸¥æ ¼åˆ†æå’Œæ›å…‰ï¼Œå¹¶æ„å»ºè§£å†³æ½œåœ¨æ»¥ç”¨é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚ è¿™ç§æ–¹æ³•ä¸ä»…ä¿ƒè¿›äº†ä¸å¤§å‹ç§‘æŠ€å…¬å¸ä¹‹å¤–çš„å„æ–¹è¿›è¡ŒçœŸæ­£çš„åˆä½œï¼Œä¹Ÿæ˜¯åŸºç¡€æ¨¡å‹çš„è·å–æ›´åŠ æ°‘ä¸»åŒ–çš„åŸºçŸ³ã€‚ æ­£å¦‚ Zellers ç­‰äººï¼ˆ2019bï¼‰æå‡ºçš„ï¼Œå¼€æ”¾å‘å¸ƒä¿ƒè¿›äº†é€æ˜åº¦ï¼Œä½¿æ›´å¤šäººèƒ½å¤Ÿä½¿ç”¨äººå·¥æ™ºèƒ½å·¥å…·ï¼Œ å®ç°äº†æŠ€æœ¯çš„æ°‘ä¸»åŒ–å’Œäººå·¥æ™ºèƒ½ä¸“ä¸šçŸ¥è¯†çš„å»ä¸­å¿ƒåŒ–ï¼ˆdemocratizing the technology and decentralizing AI expertiseï¼‰ã€‚ æˆ‘ä»¬ç›¸ä¿¡ï¼Œäººå·¥æ™ºèƒ½ä¸“ä¸šçŸ¥è¯†çš„å»ä¸­å¿ƒåŒ–ä¸ä»…ä»…æ˜¯çŸ¥è¯†çš„åˆ†å‘ï¼Œå®ƒè¿˜èƒ½åˆºæ¿€åˆ›æ–°ï¼ŒåŠ é€Ÿè¡Œä¸šè¿›æ­¥ã€‚

æœ€åï¼Œå…¬å¼€å‘å¸ƒè¿™äº›æ¨¡å‹å¯ä»¥æ•´åˆæˆæœ¬ï¼Œæ¶ˆé™¤å‡†å…¥å£å’ï¼Œä½¿å°å‹ä¼ä¸šèƒ½å¤Ÿåˆ©ç”¨ LLM çš„åˆ›æ–°æ¥æ¢ç´¢å’Œæ„å»ºæ–‡æœ¬ç”Ÿæˆä½¿ç”¨åœºæ™¯ã€‚

æœ€ç»ˆï¼Œæˆ‘ä»¬ç›¸ä¿¡è¿™å°†ä¸ºå…¨çƒå„ç§è§„æ¨¡çš„ç»„ç»‡åˆ›é€ ä¸€ä¸ªæ›´åŠ å…¬å¹³çš„ç«äº‰ç¯å¢ƒï¼Œä½¿å¤§å®¶éƒ½èƒ½ä»äººå·¥æ™ºèƒ½çš„è¿›æ­¥å¸¦æ¥çš„ç»æµå¢é•¿ä¸­å—ç›Šã€‚

æˆ‘ä»¬çŸ¥é“ï¼Œä¸æ˜¯æ¯ä¸ªä½¿ç”¨äººå·¥æ™ºèƒ½æ¨¡å‹çš„äººéƒ½æœ‰è‰¯å¥½çš„æ„å›¾ï¼›å…³äºäººå·¥æ™ºèƒ½å°†å¦‚ä½•å½±å“æˆ‘ä»¬çš„ä¸–ç•Œï¼Œäººä»¬ä¹Ÿå­˜åœ¨åˆç†çš„æ‹…å¿§ã€‚ æœ‰å®³å†…å®¹ç”Ÿæˆï¼ˆToxic content generationï¼‰å’Œæœ‰é—®é¢˜çš„å…³è”ï¼ˆproblematic associationsï¼‰æ˜¯äººå·¥æ™ºèƒ½ç¤¾åŒºå°šæœªå®Œå…¨è§£å†³çš„é‡è¦é£é™©ã€‚ æ­£å¦‚æœ¬æ–‡æ‰€æŒ‡å‡ºï¼Œæˆ‘ä»¬åœ¨é™åˆ¶è¿™ç±»å“åº”çš„æ™®éæ€§æ–¹é¢å·²ç»å–å¾—äº†è¿›å±•ã€‚ è™½ç„¶æˆ‘ä»¬è®¤è¯†åˆ°è¿˜æœ‰æ›´å¤šçš„å·¥ä½œè¦åšï¼Œä½†è¿™ç§è®¤è¯†åªä¼šåŠ æ·±æˆ‘ä»¬å¯¹å¼€æ”¾ç§‘å­¦å’Œäººå·¥æ™ºèƒ½ç¤¾åŒºåˆä½œçš„æ‰¿è¯ºã€‚

# 6 ç›¸å…³å·¥ä½œ

## 6.1 Large Language Models

The recent years have witnessed a substantial evolution in the field of LLMs. Following the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B parameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized models, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al., 2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in this progression is the rise of Llama, recognized for its focus on computational efficiency during inference (Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closedsource models. Open-source releases like BLOOM (Scao et al., 2022) and Falcon (Penedo et al., 2023) have risen to challenge their closed-source counterparts like GPT-3 and Chinchilla. Yet, when it comes to the

https://ai.meta.com/llama

â€œproduction-readyâ€ LLMs such as ChatGPT, Bard, and Claude, thereâ€™s a marked distinction in performance and usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande et al., 2023), a process that is still being explored and refined within the open-source community. Attempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023) and Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich et al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set by their closed-source counterparts.

## 6.2 Instruction Tuning

Wei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs on numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction tuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tuning can be created by humans or by LLMs themselves (Zhou et al., 2022), and follow-up instructions can be used to refine initial generations to make them more useful, engaging, and unbiased (Ganguli et al., 2023; Madaan et al., 2023). An approach related to instruction tuning is chain-of-thought prompting (Wei et al., 2022b), in which models are prompted to explain their reasoning when given a complex problem, in order to increase the likelihood that their final answer is correct.

RLHF has emerged as a powerful strategy for fine-tuning Large Language Models, enabling significant improvements in their performance (Christiano et al., 2017). The method, first showcased by Stiennon et al. (2020) in the context of text-summarization tasks, has since been extended to a range of other applications. In this paradigm, models are fine-tuned based on feedback from human users, thus iteratively aligning the modelsâ€™ responses more closely with human expectations and preferences.

Ouyang et al. (2022) demonstrates that a combination of instruction fine-tuning and RLHF can help fix issues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai et al. (2022b) partially automates this fine-tuning-plus-RLHF approach by replacing the human-labeled fine-tuning data with the modelâ€™s own self-critiques and revisions, and by replacing human raters with a model when ranking model outputs in RLHF, a process known as â€œRL from AI Feedbackâ€ (RLAIF).

## 6.3 Known LLM Safety Challenges

Recent literature has extensively explored the risks and challenges linked with Large Language Models. Bender et al. (2021b) and Weidinger et al. (2021) underscore various hazards like bias, toxicity, private data leakage, and the potential for malicious uses. Solaiman et al. (2023) categorizes these impacts into two groupsâ€”those that can be assessed within the base system and those requiring a societal context evaluation, while Kumar et al. (2022) offers potential mitigation strategies to curb harm. Work from Roller et al. (2020) and Dinan et al. (2021) also illuminates the difficulties tied to chatbot-oriented LLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes a taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between potential positive and negative impacts from releasing dialogue models.

Investigations into red teaming reveal specific challenges in tuned LLMs, with studies by Ganguli et al. (2022) and Zhuo et al. (2023) showcasing a variety of successful attack types and their effects on the generation of harmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also raised red flags around advanced emergent model behaviors, cyber threats, and potential misuse in areas like biological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an over-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu and Restrepo, 2018; Autor and Salomons, 2018;Webb, 2019; Shumailov et al., 2023). We are committed to continuing our work engaging with the broader policy, academic, and industry community on these issues.

# 7 æ€»ç»“

æœ¬æ–‡ä»‹ç»äº† LLaMA2 â€”â€” ä¸€ç»„æ–°çš„é¢„è®­ç»ƒå’Œå¾®è°ƒæ¨¡å‹ï¼Œå‚æ•°è§„æ¨¡ä» 7b åˆ° 70bã€‚ å®éªŒç»“æœè¡¨æ˜å°½ç®¡ LLaMA2 ä»ç„¶è½åäº GPT-4 ç­‰æœ€å…ˆè¿›çš„æ¨¡å‹ï¼Œ è¿™äº›ä¸ç°æœ‰çš„å¼€æºèŠå¤©æ¨¡å‹ç›¸æ¯”å·²ç»å…·æœ‰åŒç­‰ç«äº‰åŠ›ï¼Œåœ¨ä¸€äº›æ•°æ®é›†ä¸Šä¸æŸäº›ä¸“æœ‰æ¨¡å‹çš„èƒ½åŠ›ç›¸å½“ã€‚

æœ¬æ–‡è¯¦ç»†é˜è¿°äº†å®ç° LLaMA2 æ‰€ç”¨çš„æ–¹æ³•å’ŒæŠ€æœ¯ï¼Œå¹¶é‡ç‚¹å¦‚ä½•ä¸ helpfulness and safety åŸåˆ™å¯¹é½ã€‚ ä¸ºæ›´æœ‰æ„ä¹‰åœ°ä¸ºç¤¾ä¼šåšå‡ºè´¡çŒ®å¹¶ä¿ƒè¿›ç ”ç©¶æ­¥ä¼ï¼Œæˆ‘ä»¬è´Ÿè´£ä»»åœ°å¼€æ”¾äº† LLaMA2 å’Œ LLaMA2-Chatã€‚ å¯¹é€æ˜åº¦å’Œå®‰å…¨æ€§çš„æŒç»­æ‰¿è¯ºï¼Œå°†ä½¿æˆ‘ä»¬åœ¨æœªæ¥å·¥ä½œä¸­è¿›ä¸€æ­¥æ”¹è¿› LLaMA2-Chatã€‚

18/06/2024 17:36
