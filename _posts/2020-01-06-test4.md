---
layout: prediction_post
published: True
title:   Transformer 是如何工作的：TRANSFORMERS FROM SCRATCH
---
#  Transformer 是如何工作的：TRANSFORMERS FROM SCRATCH
* * *
整理和翻译自 2019 年（最后更新 2023 年）的一篇文章： Transformers From Scratch， 由浅入深地解释了 transformer/self-attention 背后的工作原理。
[原文链接](https://peterbloem.nl/blog/transformers)
07/07/2024 23:34
* * *
## 摘要
Transformer 是一类非常令人着迷的机器学习架构（a family of machine learning architectures）。 之前已经有一些不错的介绍文章（例如 [1, 2]），但过去几年 transformer 变得简单了很多， 因此要解释清楚现代架构（modern architectures）是如何工作的，比以前容易多了。本文试图丢掉历史包袱，开门见山地解释现代 transformer 的工作原理。

神经网络和反向传播（neural networks and backpropagation）的基本知识有助于更好地理解本文，
- [这个讲座](https://mlvu.github.io/beyondlinear/) 介绍了神经网络的基础知识；
- [这个讲座](https://mlvu.github.io/lecture07/) 介绍了神经网络如何应用于现代深度学习系统。
另外，理解本文程序需要一点 Pytorch 基础， 但没有基础关系也不大

## 1 self-attention（自注意力）模型
self-attention 运算是==所有transformer 架构的基本运算==
### 1.0 Attention（注意力）：名字由来

从最简形式上来说，神经网络是一系列**对输入进行加权计算，得到一个输出的过程**。
具体来说，比如给定一个**向量 [1,2,3,4,5] 作为输入**，**权重矩阵可能是 [0, 0, 0, 0.5, 0.5]**， 也就是说最终的 output 实际上只与 input 中的最后两个元素有关系 —— 换句话说， **这一层神经网络只关注最后两个元素（注意力在最后两个元素上）**， 其他元素是什么值对结果没有影响 —— 这就是==attention==这一名字的由来。
> 注意力模型大大降低了神经网络的计算量：经典神经网络是全连接的，而上面的例子中， 这一层神经网络不需要全连接了，每个输出连接到最后两个输入就行了，也就是从 1x5 维降低到了 1x2 维。
> 图像处理中的卷积神经网络（CNN）也是类似原理：只用一小块图像计算下一层的输出，而不是用整帧图像。

### 1.1 输入输出：vector-to-vector 运算
**Self-attention 是一个 sequence-to-sequence 运算**： 输入一个向量序列（a sequence of vectors），输出另一个向量序列。
我们用 𝐱1,𝐱2,…,𝐱t 表示输入向量，用 𝐲1,𝐲2,…,𝐲t 表示相应的输出向量，这些向量都是 k 维的。 要计算输出向量 𝐲i ，self-attention 只需对所有输入向量做加权平均（weighted average），
$\displaystyle\mathbf{y}_{\mathrm{i}}=\sum_{j}w_{ij}x_{j}$
**在传统神经网络中，权重都是（常量）参数， 但这里的权重并不是：==wij 是根据 𝐱i 和 𝐱j 计算出来的==。 计算它有很多种方式（算法），接下来看一种最简单的。**

### 1.2 权重矩阵计算和归一化
计算权重矩阵的最简单函数就是点积（dot product）：
$\mathbf{w}_{\mathrm{ij}}^{\prime}=x_{\mathrm{i}}^{\textrm{T}}x_{\mathrm{j}}$
>注意到权重矩阵的计算跟它所在的位置 (i,j) 直接相关，也就是说，每个位置 (i,j) 对应的权重矩阵都不一样。

