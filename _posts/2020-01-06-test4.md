---
layout: prediction_post
published: True
title:   Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šTRANSFORMERS FROM SCRATCH
---
#  Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šTRANSFORMERS FROM SCRATCH
* * *
#  Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šTRANSFORMERS FROM SCRATCH
* * *
æ•´ç†å’Œç¿»è¯‘è‡ª 2019 å¹´ï¼ˆæœ€åæ›´æ–° 2023 å¹´ï¼‰çš„ä¸€ç¯‡æ–‡ç« ï¼š Transformers From Scratchï¼Œ ç”±æµ…å…¥æ·±åœ°è§£é‡Šäº† transformer/self-attention èƒŒåçš„å·¥ä½œåŸç†ã€‚
[åŸæ–‡é“¾æ¥](https://peterbloem.nl/blog/transformers)
07/07/2024 23:34
* * *
# æ‘˜è¦
Transformer æ˜¯ä¸€ç±»éå¸¸ä»¤äººç€è¿·çš„æœºå™¨å­¦ä¹ æ¶æ„ï¼ˆa family of machine learning architecturesï¼‰ã€‚ ä¹‹å‰å·²ç»æœ‰ä¸€äº›ä¸é”™çš„ä»‹ç»æ–‡ç« ï¼ˆä¾‹å¦‚ [1, 2]ï¼‰ï¼Œä½†è¿‡å»å‡ å¹´ transformer å˜å¾—ç®€å•äº†å¾ˆå¤šï¼Œ å› æ­¤è¦è§£é‡Šæ¸…æ¥šç°ä»£æ¶æ„ï¼ˆmodern architecturesï¼‰æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæ¯”ä»¥å‰å®¹æ˜“å¤šäº†ã€‚æœ¬æ–‡è¯•å›¾ä¸¢æ‰å†å²åŒ…è¢±ï¼Œå¼€é—¨è§å±±åœ°è§£é‡Šç°ä»£ transformer çš„å·¥ä½œåŸç†ã€‚

ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ï¼ˆneural networks and backpropagationï¼‰çš„åŸºæœ¬çŸ¥è¯†æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æœ¬æ–‡ï¼Œ
- [è¿™ä¸ªè®²åº§](https://mlvu.github.io/beyondlinear/) ä»‹ç»äº†ç¥ç»ç½‘ç»œçš„åŸºç¡€çŸ¥è¯†ï¼›
- [è¿™ä¸ªè®²åº§](https://mlvu.github.io/lecture07/) ä»‹ç»äº†ç¥ç»ç½‘ç»œå¦‚ä½•åº”ç”¨äºç°ä»£æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚
å¦å¤–ï¼Œç†è§£æœ¬æ–‡ç¨‹åºéœ€è¦ä¸€ç‚¹ Pytorch åŸºç¡€ï¼Œ ä½†æ²¡æœ‰åŸºç¡€å…³ç³»ä¹Ÿä¸å¤§

# 1 self-attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰æ¨¡å‹
self-attention è¿ç®—æ˜¯==æ‰€æœ‰transformer æ¶æ„çš„åŸºæœ¬è¿ç®—==
### 1.0 Attentionï¼ˆæ³¨æ„åŠ›ï¼‰ï¼šåå­—ç”±æ¥

ä»æœ€ç®€å½¢å¼ä¸Šæ¥è¯´ï¼Œç¥ç»ç½‘ç»œæ˜¯ä¸€ç³»åˆ—**å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒè®¡ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºçš„è¿‡ç¨‹**ã€‚
å…·ä½“æ¥è¯´ï¼Œæ¯”å¦‚ç»™å®šä¸€ä¸ª**å‘é‡ [1,2,3,4,5] ä½œä¸ºè¾“å…¥**ï¼Œ**æƒé‡çŸ©é˜µå¯èƒ½æ˜¯ [0, 0, 0, 0.5, 0.5]**ï¼Œ ä¹Ÿå°±æ˜¯è¯´æœ€ç»ˆçš„ output å®é™…ä¸Šåªä¸ input ä¸­çš„æœ€åä¸¤ä¸ªå…ƒç´ æœ‰å…³ç³» â€”â€” æ¢å¥è¯è¯´ï¼Œ **è¿™ä¸€å±‚ç¥ç»ç½‘ç»œåªå…³æ³¨æœ€åä¸¤ä¸ªå…ƒç´ ï¼ˆæ³¨æ„åŠ›åœ¨æœ€åä¸¤ä¸ªå…ƒç´ ä¸Šï¼‰**ï¼Œ å…¶ä»–å…ƒç´ æ˜¯ä»€ä¹ˆå€¼å¯¹ç»“æœæ²¡æœ‰å½±å“ â€”â€” è¿™å°±æ˜¯==attention==è¿™ä¸€åå­—çš„ç”±æ¥ã€‚
> æ³¨æ„åŠ›æ¨¡å‹å¤§å¤§é™ä½äº†ç¥ç»ç½‘ç»œçš„è®¡ç®—é‡ï¼šç»å…¸ç¥ç»ç½‘ç»œæ˜¯å…¨è¿æ¥çš„ï¼Œè€Œä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ è¿™ä¸€å±‚ç¥ç»ç½‘ç»œä¸éœ€è¦å…¨è¿æ¥äº†ï¼Œæ¯ä¸ªè¾“å‡ºè¿æ¥åˆ°æœ€åä¸¤ä¸ªè¾“å…¥å°±è¡Œäº†ï¼Œä¹Ÿå°±æ˜¯ä» 1x5 ç»´é™ä½åˆ°äº† 1x2 ç»´ã€‚
> å›¾åƒå¤„ç†ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹Ÿæ˜¯ç±»ä¼¼åŸç†ï¼šåªç”¨ä¸€å°å—å›¾åƒè®¡ç®—ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œè€Œä¸æ˜¯ç”¨æ•´å¸§å›¾åƒã€‚

### 1.1 è¾“å…¥è¾“å‡ºï¼švector-to-vector è¿ç®—
**Self-attention æ˜¯ä¸€ä¸ª sequence-to-sequence è¿ç®—**ï¼š è¾“å…¥ä¸€ä¸ªå‘é‡åºåˆ—ï¼ˆa sequence of vectorsï¼‰ï¼Œè¾“å‡ºå¦ä¸€ä¸ªå‘é‡åºåˆ—ã€‚
æˆ‘ä»¬ç”¨ ğ±1,ğ±2,â€¦,ğ±t è¡¨ç¤ºè¾“å…¥å‘é‡ï¼Œç”¨ ğ²1,ğ²2,â€¦,ğ²t è¡¨ç¤ºç›¸åº”çš„è¾“å‡ºå‘é‡ï¼Œè¿™äº›å‘é‡éƒ½æ˜¯ k ç»´çš„ã€‚ è¦è®¡ç®—è¾“å‡ºå‘é‡ ğ²i ï¼Œself-attention åªéœ€å¯¹æ‰€æœ‰è¾“å…¥å‘é‡åšåŠ æƒå¹³å‡ï¼ˆweighted averageï¼‰ï¼Œ
$\displaystyle\mathbf{y}_{\mathrm{i}}=\sum_{j}w_{ij}x_{j}$
**åœ¨ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸­ï¼Œæƒé‡éƒ½æ˜¯ï¼ˆå¸¸é‡ï¼‰å‚æ•°ï¼Œ ä½†è¿™é‡Œçš„æƒé‡å¹¶ä¸æ˜¯ï¼š==wij æ˜¯æ ¹æ® ğ±i å’Œ ğ±j è®¡ç®—å‡ºæ¥çš„==ã€‚ è®¡ç®—å®ƒæœ‰å¾ˆå¤šç§æ–¹å¼ï¼ˆç®—æ³•ï¼‰ï¼Œæ¥ä¸‹æ¥çœ‹ä¸€ç§æœ€ç®€å•çš„ã€‚**

#### 1.2 æƒé‡çŸ©é˜µè®¡ç®—å’Œå½’ä¸€åŒ–
è®¡ç®—æƒé‡çŸ©é˜µçš„æœ€ç®€å•å‡½æ•°å°±æ˜¯ç‚¹ç§¯ï¼ˆdot productï¼‰ï¼š
>ç‚¹ç§¯(dot product)åˆå«æ ‡é‡ç§¯ã€æ•°é‡ç§¯(scalar product)ã€‚å®ƒæ˜¯ä¸¤ä¸ªæ•°å­—åºåˆ—çš„ç›¸åº”æ¡ç›®çš„ä¹˜ç§¯ä¹‹å’Œã€‚

$\mathbf{w}_{\mathrm{ij}}^{\prime}=x_{\mathrm{i}}^{\textrm{T}}x_{\mathrm{j}}$
>æ³¨æ„åˆ°æƒé‡çŸ©é˜µçš„è®¡ç®—è·Ÿå®ƒæ‰€åœ¨çš„ä½ç½® (i,j) ç›´æ¥ç›¸å…³ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªä½ç½® (i,j) å¯¹åº”çš„æƒé‡çŸ©é˜µéƒ½ä¸ä¸€æ ·ã€‚

ç‚¹ç§¯å¾—åˆ°çš„ç»“æœå–å€¼èŒƒå›´æ˜¯æ­£è´Ÿæ— ç©·ï¼Œä¸ºäº†ä½¿ç´¯åŠ å’Œï¼ˆè¡¨ç¤ºæ¦‚ç‡ï¼‰ç­‰äº 100%ï¼Œ éœ€è¦å¯¹å®ƒä»¬åšå½’ä¸€åŒ–ï¼šç”¨ pytorch æœ¯è¯­æ¥è¯´å°±æ˜¯ softmaxï¼Œ
$w_{\mathrm{ij}}={\frac{\exp\mathbf{w}_{\mathrm{ij}}^{\prime}}{\sum_{\mathrm{j}}\exp\displaystyle w_{\mathrm{ij}}^{\prime}}}$
è¿™ä¼šå°†æ¯ä¸ªæƒé‡çŸ©é˜µå½’ä¸€åŒ–åˆ° [0,1]ï¼Œå¹¶ä¸”ç´¯åŠ å’Œç­‰äº 1ã€‚

#### 1.3 ç›´è§‚å±•ç¤ºä¸å°ç»“
ä»¥ä¸Šå°±æ˜¯å…³äº self-attention çš„åŸºæœ¬è¿ç®—ã€‚æ€»ç»“èµ·æ¥å°±æ˜¯ä¸¤ç‚¹ï¼š
1. **vector-to-vector è¿ç®—**ï¼šself-attention æ˜¯å¯¹ input vector åšçŸ©é˜µè¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªåŠ æƒç»“æœä½œä¸º output vectorï¼›
2. **åŠ æƒçŸ©é˜µè®¡ç®—**ï¼šæƒé‡çŸ©é˜µä¸æ˜¯å¸¸é‡ï¼Œè€Œæ˜¯è·Ÿå®ƒæ‰€åœ¨çš„ä½ç½® (i,j) ç›´æ¥ç›¸å…³ï¼Œæ ¹æ®å¯¹åº”ä½ç½®çš„ input vector è®¡ç®—ã€‚
ç”¨å›¾æ¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
![self-attention åŸºæœ¬è¿ç®—](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/self-attention.png)å›¾: self-attention åŸºæœ¬è¿ç®—
- **output vector ä¸­çš„æ¯ä¸ªå…ƒç´  $\mathbb{y}_{\mathrm{j}}$éƒ½æ˜¯å¯¹ input vector ä¸­æ‰€æœ‰å…ƒç´ çš„åŠ æƒå’Œï¼›**
- **å¯¹äº $\mathbb{y}_{\mathrm{j}}$ï¼ŒåŠ æƒçŸ©é˜µç”± input å…ƒç´ $\mathbb{x}_{\mathrm{j}}$ ä¸æ¯ä¸ª input å…ƒç´ è®¡ç®—å¾—åˆ°ï¼›**

è¦æ„å»ºä¸€ä¸ªå®Œæ•´çš„ transformer è¿˜éœ€è¦ä¸€ç‚¹å…¶ä»–ä¸œè¥¿ï¼Œä½†æœ€æ ¸å¿ƒçš„è¿ç®—å°±æ˜¯ä»¥ä¸Šè¿™ä¸¤ä¸ªäº†ã€‚ æ›´é‡è¦çš„æ˜¯ï¼Œ
- è¿™æ˜¯æ•´ä¸ªæ¶æ„ä¸­ï¼Œå”¯ä¸€åœ¨ input & output vector ä¹‹é—´ æ‰€åšçš„è¿ç®—ï¼›
- Transformer æ¶æ„ä¸­çš„å…¶ä»–è¿ç®—éƒ½æ˜¯å•çº¯å¯¹ input vector åšè¿ç®—ã€‚

# 2 self-attention ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿä»¥ç”µå½±æ¨èä¸ºä¾‹
æ­¥éª¤å¾ˆç®€å•ï¼š
1. äººå·¥è®¾è®¡ä¸€äº›ç”µå½±ç‰¹å¾ï¼Œæ¯”å¦‚æµªæ¼«æŒ‡æ•°ã€åŠ¨ä½œæŒ‡æ•°ï¼Œ
2. äººå·¥è®¾è®¡ä¸€äº›ç”¨æˆ·ç‰¹å¾ï¼Œä¾‹å¦‚ä»–ä»¬å–œæ¬¢æµªæ¼«ç”µå½±æˆ–åŠ¨ä½œç‰‡çš„å¯èƒ½æ€§ï¼›

æœ‰äº†è¿™ä¸¤ä¸ªç»´åº¦çš„æ•°æ®ï¼ˆç‰¹å¾å‘é‡ï¼‰ä¹‹åï¼Œå¯¹äºŒè€…åšç‚¹ç§¯ï¼ˆdot productï¼‰ï¼Œ å¾—åˆ°çš„å°±æ˜¯ç”µå½±å±æ€§ä¸ç”¨æˆ·å–œæ¬¢ç¨‹åº¦ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ï¼Œç”¨å¾—åˆ†è¡¨ç¤ºï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-dot-product.png)
ç”µå½±æ¨èï¼šç”µå½±ç‰¹å¾å‘é‡ï¼ˆæµªæ¼«ã€åŠ¨ä½œã€å–œå‰§ï¼‰ä¸ç”¨æˆ·ç‰¹æ€§å‘é‡ï¼ˆå–œæ¬¢æµªæ¼«ã€åŠ¨ä½œã€å–œå‰§çš„ç¨‹åº¦ï¼‰åšç‚¹ç§¯è¿ç®—


å…³äºè®¡ç®—ç»“æœï¼ˆå¾—åˆ†ï¼‰ï¼š
- å¦‚æœç‰¹å¾çš„ç¬¦å·ç›¸åŒï¼Œä¾‹å¦‚â€œæµªæ¼«ç”µå½± && ç”¨æˆ·å–œæ¬¢æµªæ¼«ç”µå½±â€ï¼Œ æˆ–è€…â€œä¸æ˜¯æµªæ¼«ç”µå½± && ç”¨æˆ·ä¸å–œæ¬¢æµªæ¼«ç”µå½±â€ï¼Œå¾—åˆ°çš„ç‚¹ç§¯å°±æ˜¯æ­£æ•°ï¼›åä¹‹å°±æ˜¯è´Ÿæ•°ï¼›
- ç‰¹å¾å€¼çš„å¤§å°å†³å®šè¯¥ç‰¹å¾å¯¹æ€»åˆ†çš„è´¡çŒ®å¤§å°ï¼š ä¸€éƒ¨ç”µå½±å¯èƒ½æœ‰ç‚¹æµªæ¼«ï¼Œä½†ä¸æ˜¯å¾ˆæ˜æ˜¾ï¼Œæˆ–è€…ç”¨æˆ·å¯èƒ½åªæ˜¯ä¸å–œæ¬¢æµªæ¼«ï¼Œä½†ä¹Ÿæ²¡åˆ°è®¨åŒçš„ç¨‹åº¦ã€‚
è¿™ç§æ¨èæ¨¡å‹çš„å¥½å¤„æ˜¯ç®€å•ç›´æ¥ï¼Œå¾ˆå®¹æ˜“ä¸Šæ‰‹ï¼›ç¼ºç‚¹æ˜¯è§„æ¨¡å¤§äº†å¾ˆéš¾æï¼Œ å› ä¸ºå¯¹å‡ ç™¾ä¸‡éƒ¨ç”µå½±æ‰“æ ‡çš„æˆæœ¬éå¸¸é«˜ï¼Œç²¾ç¡®æ ‡è®°ç”¨æˆ·å–œæ¬¢æˆ–ä¸å–œæ¬¢ä»€ä¹ˆä¹Ÿå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚

## 2.2 åŸºäº self-attention çš„æ¨èç³»ç»Ÿ
æ¥ä¸‹æ¥çœ‹åŸºäº self-attention çš„æ¨èç³»ç»Ÿæ˜¯æ€ä¹ˆè®¾è®¡çš„ã€‚

### 2.2.1 ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä½œä¸ºæ¨¡å‹å‚æ•°ï¼ŒåŒ¹é…å·²çŸ¥çš„ç”¨æˆ·åå¥½
ä¹Ÿæ˜¯ä¸¤æ­¥ï¼š
1. **ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä¸å†ç›´æ¥åšç‚¹ç§¯è¿ç®—ï¼Œè€Œæ˜¯ä½œä¸ºæ¨¡å‹çš„å‚æ•°ï¼ˆparameters of the modelï¼‰**ï¼›
2. **æ”¶é›†å°‘é‡çš„ç”¨æˆ·åå¥½ä½œä¸ºç›®æ ‡ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾ï¼ˆæ¨¡å‹å‚æ•°ï¼‰ï¼Œ ä½¿äºŒè€…çš„ç‚¹ç§¯åŒ¹é…å·²çŸ¥çš„ç”¨æˆ·å–œå¥½**ã€‚

è¿™å°±æ˜¯ self-attention çš„åŸºæœ¬åŸç†ã€‚æ³¨æ„ï¼Œ å°½ç®¡æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æ¨¡å‹æŸä¸ªç‰¹å¾æ„å‘³ç€ä»€ä¹ˆï¼ˆè¡¨ç¤ºä»€ä¹ˆï¼‰ï¼Œ ä½†å®è·µè¯æ˜ï¼Œè®­ç»ƒä¹‹åçš„ç‰¹å¾ç¡®å®åæ˜ äº†å…³äºç”µå½±å†…å®¹çš„åˆç†è¯­ä¹‰ã€‚

>ç”¨ç´ äººæœ¯è¯­æ¥é‡æ–°æè¿°ä»¥ä¸Šè¿‡ç¨‹ï¼šæˆ‘ä»¬å‘Šè¯‰ç¥ç»ç½‘ç»œï¼Œ
1. >æˆ‘æœ‰ä¸€äº›å…³äºç”µå½±å’Œç”¨æˆ·çš„ä¿¡æ¯ï¼Œä½œä¸ºè¾“å…¥ï¼›æœ‰ä¸€äº›ç”¨æˆ·åå¥½ä¿¡æ¯ï¼Œä½œä¸ºè¾“å‡ºã€‚
2. >ä½ æŠŠè¿™ä¸¤è€…ä¸²è”èµ·æ¥ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥é¢„æµ‹è¾“å‡ºï¼Œä½ è‡ªå·±æ€ä¹ˆå®ç°æˆ‘ä¸ç®¡ï¼ŒæŠŠæœ€ç»ˆæ¨¡å‹ï¼ˆå‚æ•°ï¼‰ç»™æˆ‘å°±è¡Œäº†ã€‚è¯‘æ³¨ã€‚

![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-features.png)
å›¾:ä»ä¸€ä¸ªåŸºæœ¬çš„ matrix factorization æ¨¡å‹å­¦ä¹ åˆ°çš„å‰ä¸¤ä¸ªç‰¹å¾ã€‚ æ¨¡å‹åªç”¨åˆ°äº†â€œå“ªäº›ç”¨æˆ·å–œæ¬¢å“ªäº›ç”µå½±â€ä¿¡æ¯ï¼Œè€Œæ²¡æœ‰ç”¨åˆ°ä»»ä½•ç”µå½±å†…å®¹ä¿¡æ¯ã€‚ æ¨ªè½´ï¼šä»æµä¿—åˆ°é«˜é›…ï¼›çºµè½´ï¼šä»å°ä¼—åˆ°ä¸»æµã€‚ä¿¡æ¯æ¥è‡ª [4]ã€‚
>è¿™äº›å·²ç»è¶³å¤Ÿè¯´æ˜ dot product æ˜¯å¦‚ä½•è¡¨ç¤ºå¯¹è±¡å’Œå®ƒä»¬çš„å…³ç³»çš„ã€‚ æ›´å¤šå…³äºæ¨èç³»ç»Ÿçš„å†…å®¹ï¼Œå¯ç§»æ­¥ mlvu.github.io/lecture12ã€‚

### 2.2.2 åµŒå…¥å±‚ï¼šå¯¹è¾“å…¥è¿›è¡Œå¤„ç†

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸²å•è¯ä½œä¸ºè¾“å…¥ï¼ŒåŸç†ä¸Šåªè¦å°†å…¶ä½œä¸º input vector é€åˆ° self-attention æ¨¡å‹ã€‚ ä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦å¯¹è¿™ä¸ª input vector åšä¸€ä¸‹é¢„å¤„ç†ï¼ˆä¸‹ä¸€èŠ‚ä¼šè§£é‡Šä¸ºä»€ä¹ˆï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªä¸­é—´è¡¨ç¤ºï¼Œ è¿™å°±æ˜¯åºåˆ—å»ºæ¨¡ä¸­çš„åµŒå…¥å±‚ã€‚ å…·ä½“æ¥è¯´ï¼Œä¼šä¸ºæ¯ä¸ªå•è¯ t åˆ†é…ä¸€ä¸ªåµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰$\mathbf{v}_{\mathrm{t}}$ï¼ˆæˆ‘ä»¬åé¢å°†å­¦ä¹ åˆ°è¿™ä¸ªå€¼ï¼‰ã€‚

**åµŒå…¥å±‚å°† input vector**ï¼š
> the,cat,walks,on,the,street


**è½¬æ¢ä¸º embedding vector**ï¼ˆæ³¨æ„ï¼šæ¯ä¸ªå•è¯çš„ç»´åº¦ä» 1x1 å˜æˆäº† 1xNï¼‰ï¼š
> ğ¯the,ğ¯cat,ğ¯walks,ğ¯on,ğ¯the,ğ¯street

å°†è¿™ä¸ª embedding vector è¾“å…¥ self-attention å±‚ï¼Œ**å¾—åˆ°çš„å°±æ˜¯ output vector**ï¼š
> ğ²the,ğ²cat,ğ²walks,ğ²on,ğ²the,ğ²street

å…¶ä¸­ ğ²cat æ˜¯æ‰€æœ‰åµŒå…¥å‘é‡çš„åŠ æƒå’Œï¼ˆweighted sumï¼‰ï¼Œç”±å®ƒä»¬ä¸ ğ¯cat çš„ï¼ˆå½’ä¸€åŒ–ï¼‰ç‚¹ç§¯åŠ æƒã€‚

### 2.2.3 ç›´è§‚è§£é‡Š
ç”±äºæˆ‘ä»¬æ­£åœ¨å­¦ä¹ ï¼ˆlearningï¼‰ ğ¯t çš„å€¼æ˜¯ä»€ä¹ˆï¼Œä¸¤ä¸ªè¯çš„â€œç›¸å…³â€ç¨‹åº¦å®Œå…¨ç”±ä»»åŠ¡å†³å®šã€‚
- åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®šå† è¯ "the" ä¸å¥å­ä¸­å…¶ä»–å•è¯è¡¨ç¤ºä»€ä¹ˆæ„æ€ï¼ˆthe interpretation of the other wordsï¼‰å…³ç³»ä¸å¤§ï¼› å› æ­¤æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„åµŒå…¥å±‚ ğ¯the ä¸æ‰€æœ‰å…¶ä»–å•è¯çš„ç‚¹ç§¯å¯èƒ½å¾ˆå°æˆ–ä¸ºè´Ÿæ•°ï¼›
- å¦ä¸€æ–¹é¢ï¼Œè¦è§£é‡Šè¿™å¥è¯ä¸­ â€œwalksâ€ çš„æ„æ€ï¼Œå¼„æ¸…æ¥šè°åœ¨èµ°è·¯æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚è¿™å¾ˆå¯èƒ½ç”±åè¯è¡¨è¾¾ï¼Œ å› æ­¤å¯¹äºåƒ cat è¿™æ ·çš„åè¯å’Œåƒ walks è¿™æ ·çš„åŠ¨è¯ï¼Œæˆ‘ä»¬å¯èƒ½æœ€ç»ˆå­¦ä¹ åˆ°çš„ ğ¯cat and ğ¯walks ç‚¹ç§¯æ˜¯ä¸ªè¾ƒå¤§çš„æ­£æ•°ã€‚

è¿™å°±æ˜¯ self-attention èƒŒåçš„åŸºæœ¬ç›´è§‰ï¼š
1. ç‚¹ç§¯è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­ä¸¤ä¸ªå‘é‡çš„ç›¸å…³ç¨‹åº¦ï¼Œâ€œç›¸å…³â€ç”±å­¦ä¹ ä»»åŠ¡ï¼ˆlearning taskï¼‰å®šä¹‰ï¼Œ
2. è¾“å‡ºå‘é‡æ˜¯æ•´ä¸ªè¾“å…¥åºåˆ—çš„åŠ æƒå’Œï¼Œæƒé‡ç”±è¿™äº›ç‚¹ç§¯å†³å®šã€‚

## 2.4 self-attention ç‰¹æ®Šå±æ€§
åœ¨ç»§ç»­ä¹‹å‰ï¼Œæœ‰äº›ç‰¹æ®Šå±æ€§éœ€è¦æåŠä¸€ä¸‹ï¼Œå› ä¸ºä¸åŒäºåœ¨ä¸€èˆ¬çš„ sequence-to-sequence è¿ç®—ï¼š

**åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„ self-attention æ¨¡å‹è¿˜æ²¡æœ‰å‚æ•°ï¼ˆ è™½ç„¶ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¼šä¸º self-attention æ·»åŠ å‡ ä¸ªå‚æ•°ï¼‰ã€‚**
- æ¢å¥è¯è¯´ï¼ŒåŸºæœ¬çš„ self-attention å®é™…ä¸Šåšä»€ä¹ˆå®Œå…¨å–å†³äºç”Ÿæˆè¾“å…¥åºåˆ—çš„ä¸Šæ¸¸æœºåˆ¶ã€‚ ä¾‹å¦‚åµŒå…¥å±‚è¿™ç§æœºåˆ¶ä¼šé©±åŠ¨ç€ self-attention å­¦ä¹ åŸºäºç‚¹ç§¯çš„è¡¨ç¤ºã€‚

**self-attention å°†è¾“å…¥å½“åšä¸€ä¸ªé›†åˆï¼ˆsetï¼‰è€Œä¸æ˜¯åºåˆ—ï¼ˆsequenceï¼‰ã€‚
- å¦‚æœæˆ‘ä»¬å¯¹è¾“å…¥åºåˆ—è¿›è¡Œé‡æ’ï¼ˆpermuteï¼‰ï¼Œè¾“å‡ºåºåˆ—é™¤äº†ä¹Ÿè·Ÿç€é‡æ’ï¼Œå…¶ä»–æ–¹é¢å°†å®Œå…¨ç›¸åŒï¼Œ ä¹Ÿå°±æ˜¯è¯´ self-attention æ˜¯æ’åˆ—ç­‰å˜çš„ï¼ˆpermutation equivariantï¼‰ã€‚ åé¢ä¼šçœ‹åˆ°ï¼Œæ„å»ºå®Œæ•´çš„ transformer æ—¶ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¼šå¼•å…¥ä¸€äº›ä¸œè¥¿æ¥ä¿æŒè¾“å…¥çš„é¡ºåºä¿¡æ¯ï¼Œ ä½†è¦æ˜ç™½ **self-attention æœ¬èº«æ˜¯ä¸å…³å¿ƒè¾“å…¥çš„é¡ºåºå±æ€§çš„ï¼ˆsequential natureï¼‰**ã€‚

# 3. å®ç°ä¸€ä¸ªåŸºæœ¬çš„ self-attention
æ¥ä¸‹æ¥æˆ‘ä»¬åŸºäº pytorch å®ç°å‰é¢ä»‹ç»çš„æœ€åŸºç¡€ self-attention æ¨¡å‹ã€‚
æˆ‘ä»¬é¢ä¸´çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚ä½•ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤º self-attentionï¼š æŒ‰ç…§å®šä¹‰ï¼Œç›´æ¥éå†æ‰€æœ‰ input vectors æ¥è®¡ç®— weight å’Œ output å°±è¡Œï¼Œ ä½†æ˜¾ç„¶è¿™ç§æ–¹å¼æ•ˆç‡å¤ªä½ï¼›æ”¹è¿›çš„æ–¹å¼å°±æ˜¯ç”¨ pytorch çš„ tensor æ¥è¡¨ç¤ºï¼Œ è¿™æ˜¯ä¸€ä¸ªå¤šç»´çŸ©é˜µæ•°æ®ç»“æ„ï¼š
> A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.
> pytorch.org/docs/stable/tensors.html
- è¾“å…¥ ğ— ç”± **t ä¸ª** ã€**k-ç»´** vector ç»„æˆçš„åºåˆ—ï¼Œ
- å¼•å…¥ä¸€ä¸ª **mini-batch dimension b**ï¼Œ
å°±å¾—åˆ°äº†ä¸€ä¸ªä¸‰ç»´çŸ©é˜µ (b,t,k)ï¼Œè¿™å°±æ˜¯ä¸€ä¸ª tensor

### 3.2 è®¡ç®—æƒé‡çŸ©é˜µï¼šè¾“å…¥çŸ©é˜µ * è½¬ç½®çŸ©é˜µ
æ¥ä¸‹æ¥è®¡ç®—åŠ æƒçŸ©é˜µï¼Œå®ƒè¡¨ç¤ºçš„æ˜¯ input vector ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œ å› æ­¤ç”¨è¾“å…¥çŸ©é˜µ ğ— ä¹˜ä»¥å®ƒçš„è½¬ç½®çŸ©é˜µï¼ˆtransposeï¼‰ï¼Œç”¨ pytorch åº“æ¥è®¡ç®—éå¸¸æ–¹ä¾¿ã€‚
```
import torch
import torch.nn.functional as F

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº› tensor x ä½œä¸ºè¾“å…¥ï¼Œå®ƒæ˜¯ (b, t, k) ç»´çŸ©é˜µ
x = ...

# torch.bmm() æ˜¯æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆbatched matrix multiplicationï¼‰å‡½æ•°ï¼Œå¯¹ä¸€æ‰¹çŸ©é˜µæ‰§è¡Œä¹˜æ³•æ“ä½œ
raw_weights = torch.bmm(x, x.transpose(1, 2))
```
ç„¶åå¯¹æƒé‡çŸ©é˜µè¿›è¡Œæ­£å€¼åŒ–å’Œå½’ä¸€åŒ–ï¼Œä»¥ä½¿å¾—ä¸€ä¸ª row å†…æ‰€æœ‰æƒé‡åŠ èµ·æ¥ä¸º 1ï¼Œ
```
weights = F.softmax(raw_weights, dim=2)
```

3.3 è®¡ç®—è¾“å‡º
æœ‰äº†æƒé‡çŸ©é˜µï¼Œè®¡ç®—è¾“å‡ºå°±éå¸¸ç®€å•äº†ï¼šåªéœ€è¦å°†è¾“å…¥ ğ— å’Œæƒé‡çŸ©é˜µç›¸ä¹˜å³å¯ï¼Œä¸€è¡Œä»£ç æå®šï¼š
```
y = torch.bmm(weights, x)
```
**è¾“å‡ºçŸ©é˜µ ğ˜ å°±æ˜¯ size (b, t, k) çš„ tensorï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯å¯¹ ğ— çš„è¡Œçš„åŠ æƒã€‚**
è¿™å°±æ˜¯ æœ€åŸºç¡€çš„ self-attention æ¨¡å‹çš„å®ç°ï¼š ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•å’Œä¸€æ¬¡å½’ä¸€åŒ–ï¼ˆsoftmaxï¼‰ã€‚


# 4 ç°ä»£ transformer å¯¹ self-attention çš„æ‰©å±•

ç°ä»£ transformer ä¸­å®é™…ä½¿ç”¨çš„ self-attention ä¾èµ–äºä¸‰ä¸ªé¢å¤–æŠ€å·§ã€‚

###4.1 å¼•å…¥æ§åˆ¶å‚æ•°ï¼ˆfor queries, keys and valuesï¼‰

### 4.1.1 æ¯ä¸ª input vector éƒ½è¢«ä½¿ç”¨ä¸‰æ¬¡
ä¸Šä¸€èŠ‚å·²ç»çœ‹åˆ°ï¼Œæ¯ä¸ª input vector ğ±i åœ¨ self-attention è®¡ç®—ä¸­ä¼šè¢«ä½¿ç”¨ä¸‰æ¬¡ï¼Œ æ ¹æ®è§’è‰²çš„ä¸åŒè¿™ä¸‰æ¬¡åˆ†åˆ«ç§°ä¸º **queries**ã€**keys**ã€**values**ï¼ˆ**æŸ¥è¯¢ã€é”®å’Œå€¼**ï¼Œåé¢å†è§£é‡Šè¿™äº›åç§°çš„æ¥æºï¼‰ï¼Œ
- **query**ï¼šä¸å…¶ä»–æ‰€æœ‰ input vector **è”åˆè®¡ç®— i ä½ç½®çš„ output vector ğ²i æ‰€éœ€çš„æƒé‡**ï¼›
- **key**ï¼šä¸ query ç±»ä¼¼ï¼Œä¸å…¶ä»–æ‰€æœ‰ input vecto**r è”åˆè®¡ç®— j ä½ç½®çš„ output vector ğ²j æ‰€éœ€çš„æƒé‡**ï¼Œè¿™é‡Œ jâ‰ iï¼›
- **value**ï¼šåœ¨è®¡ç®—æ¯ä¸ª output vector æ—¶ï¼Œ**ä½œä¸ºè¾“å…¥å€¼å‚ä¸åŠ æƒæ±‚å’Œ**ã€‚
ä¹Ÿå°±æ˜¯è¯´åœ¨æˆ‘ä»¬ç›®å‰çš„åŸºæœ¬ self-attention ä¸­ï¼Œæ¯ä¸ª input vector å¿…é¡»æ‰¿æ‹…æ‰€æœ‰ä¸‰ä¸ªè§’è‰²ã€‚

### 4.1.2 å…·ä½“ä¾‹å­ï¼ˆè¯‘æ³¨ï¼‰
ä¸Šé¢çš„æè¿°æ¯”è¾ƒæŠ½è±¡ï¼Œè¿™é‡Œå‚è€ƒä¸‹å›¾æ›´ç›´è§‚è§£é‡Šä¸€ä¸‹ã€‚è¿™ä¸ªå›¾å¯¹åº” i=2ï¼Œå› æ­¤ ğ±2 ä¼šç”¨åˆ°ä¸‰æ¬¡ï¼ˆæ›´å‡†ç¡®åœ°è¯´æ˜¯ä¸‰ç§ç”¨é€”ï¼‰ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/self-attention.png)
å›¾ï¼šself-attention åŸºæœ¬è¿ç®—
- queryï¼šğ±2 ä¸ ğ±2 è”åˆè®¡ç®— ğ°22ï¼›
- keyï¼šğ±2 ä¸ ğ±j è”åˆè®¡ç®— ğ°2j æƒé‡ï¼Œè¿™é‡Œ jâ‰ 2ï¼›
- valueï¼šğ±2 ä½œä¸ºè¾“å…¥å€¼å‚ä¸åŠ æƒæ±‚å’Œã€‚

4.1.3 å¼•å…¥ä¸‰ä¸ª kÃ—k æƒé‡çŸ©é˜µ
å¯¹åŸå§‹ input vector åº”ç”¨çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿä¸ºæ¯ä¸ªè§’è‰²è¡ç”Ÿï¼ˆderiveï¼‰å‡ºä¸€ä¸ªæ–°å‘é‡ï¼Œä»è€Œç®€åŒ– self-attentionã€‚ å…·ä½“æ¥è¯´ï¼Œå¼•å…¥**ä¸‰ä¸ª kÃ—k** æƒé‡çŸ©é˜µ ğ–q, ğ–k, ğ–vï¼ˆæ¥è‡ª query/key/value é¦–å­—æ¯ï¼‰ **å¯¹æ¯ä¸ªè¾“å…¥ xi è®¡ç®—ä¸‰ä¸ªçº¿æ€§å˜æ¢**ï¼Œ
$q_{\mathrm{i}}=W_{\mathrm{q}}\mathbf{x}_{\mathrm{i}}$
$k_{\mathrm{i}}=W_{\mathrm{k}}\mathbf{x}_{\mathrm{i}}$
$v_{\mathrm{i}}=W_{\mathrm{v}}\mathbf{x}_{\mathrm{i}}$
é‚£ä¹ˆ (i,j) ä½ç½®å¤„çš„æƒé‡çŸ©é˜µå°±å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$w_{\mathrm{ij}}^{\prime}=\mathbf{q}_{\mathrm{i}}^{\textrm{T}}\mathbf{k}_{\mathrm{j}}$
åšå½’ä¸€åŒ–å¤„ç†ï¼Œ
$w_{\mathrm{ij}}=\mathrm{softmax}(w_{\mathrm{ij}}^{\prime})$
æœ€åï¼Œoutput vector ä¸­ä½ç½® j å¤„çš„å€¼ä¸ºï¼š
$\mathbf{y}_{\mathrm{i}}=\sum_{\mathrm{j}}w_{\mathrm{ij}}v_{\mathrm{j}}$
è¿™å°±ç»™ self-attention layer å¼•å…¥äº†å‡ ä¸ªå¯æ§åˆ¶çš„å‚æ•°ï¼ˆcontrollable parameters, ğ–q, ğ–k, ğ–vï¼‰ï¼Œ å¯¹åŒä¸€ä»½è¾“å…¥åº”ç”¨ä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸åŒè§’è‰²æ‰€éœ€çš„å€¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/key-query-value.png)
self-attention key/query/value transformation çš„ç›´è§‚è§£é‡Š

## 4.2 å¯¹ç‚¹ç§¯åšç¼©æ”¾å¤„ç†ï¼ˆscaling the dot productï¼‰
**softmax å‡½æ•°å¯¹éå¸¸å¤§çš„è¾“å…¥å€¼æ•æ„Ÿã€‚è¿™äº› input ä¼šæ¢¯åº¦æ¶ˆå¤±ï¼Œå­¦ä¹ å˜æ…¢ç”šè‡³å®Œå…¨åœæ­¢ã€‚ ç”±äºç‚¹ç§¯çš„å¹³å‡å€¼éšç€åµŒå…¥ç»´åº¦ k çš„å¢åŠ è€Œå¢å¤§ï¼Œå› æ­¤ç‚¹ç§¯é€åˆ° softmax ä¹‹å‰è¿›è¡Œç¼©æ”¾æœ‰åŠ©äºç¼“è§£è¿™ä¸ªé—®é¢˜**ã€‚
åŸæ¥æ‰§è¡Œ softmax ä¹‹å‰çš„æƒé‡çŸ©é˜µï¼š
$w_{\mathrm{ij}}^{\prime}=\mathbf{q}_{\mathrm{i}}^{\textrm{T}}\mathbf{k}_{\mathrm{j}}$
ç°åœ¨ï¼š
$w_{\mathrm{ij}}^{\prime}=\frac{{\bf q}_{\mathrm{i}}^{\mathrm{\normalsize~T}}{\bf k}_{\bar{\imath}}}{\sqrt{\mathrm{k}}}$

 > Why sqrt(k) Imagine a vector in â„k with values all c. Its Euclidean length is  sqrt(k) *c. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors

## 4.3 å¼•å…¥ multi-head attention
æœ€åï¼Œéœ€è¦è€ƒè™‘åˆ°ï¼ŒåŒä¸€ä¸ªå•è¯éšç€ç›¸é‚»å•è¯ä»¬çš„ä¸åŒè¡¨ç¤ºçš„æ„æ€ä¹Ÿå¯èƒ½ä¸åŒã€‚ä¾‹å¦‚ä¸‹é¢è¿™ä¸ªå¥å­ï¼š
                                       mary,gave,roses,to,susan
æˆ‘ä»¬çœ‹åˆ° â€œgaveâ€ è¿™ä¸ªè¯ä¸å¥å­çš„ä¸åŒéƒ¨åˆ†æœ‰ä¸åŒçš„å…³ç³»ï¼š
â€œmaryâ€ è¡¨ç¤ºè°åœ¨ â€œgaveâ€ï¼Œ
â€œrosesâ€ è¡¨ç¤º â€œgaveâ€ çš„æ˜¯ä»€ä¹ˆï¼Œ
â€œsusanâ€ è¡¨ç¤ºæ¥å—è€…æ˜¯è°ã€‚

### 4.3.1 éœ€æ±‚ï¼šè¾“å‡ºä¸­åµŒå…¥æ›´å¤šä¿¡æ¯
åœ¨æˆ‘ä»¬çš„åŸºæœ¬ self-attention ä¸­ï¼Œæ‰€æœ‰è¿™äº›ä¿¡æ¯æ˜¯æ··åˆåœ¨ä¸€èµ·çš„ï¼š è¾“å…¥ Xmary å’Œ Xsusan å¯ä»¥ä¸åŒç¨‹åº¦åœ°å½±å“è¾“å‡º Ygave ï¼Œè¿™å–å†³äºå®ƒä»¬ä¸ Ygave çš„ç‚¹ç§¯ã€‚
ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä»¥å…¶ä»–æ–¹å¼å½±å“è¾“å‡ºï¼Œè¿™ç§æ¨¡å‹å°±ä¸è¡Œäº†ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœ â€œrosesâ€ çš„ç»™äºˆæ–¹å’Œæ¥å—æ–¹ä¿¡æ¯éƒ½å‡ºç°åœ¨Ygave ï¼Œä½†ä½äºä¸åŒéƒ¨åˆ†ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸºæœ¬çš„ self-attention æ¬ ç¼ºäº†å¾ˆå¤šçµæ´»æ€§ã€‚
>This leaves aside how we figure out who gave the roses. We can do that based on prior knowledge about Mary and Susan, encoded in the embeddings. We can also look at the order of the words, but weâ€™ll look at how to achieve that later

### 4.3.2 è§£å†³æ–¹å¼ï¼šå¼•å…¥å¤šä¸ª self-attentionï¼ˆmulti-headï¼‰
è¦å®ç°è¿™ä¸ªç›®çš„ï¼Œå°±éœ€è¦è®©æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ›´å¼ºçš„è¾¨è¯†åŠ›ï¼Œä¸€ç§åšæ³•å°±æ˜¯ç»„åˆå¤šä¸ª self-attentionï¼ˆç”¨ r ç´¢å¼•ï¼‰ï¼Œ æ¯ä¸ªå¯¹åº”ä¸åŒçš„ query/key/value å‚æ•°çŸ©é˜µ$W_{\mathrm{q}}^{\mathrm{r}}$,$W_{\mathrm{k}}^{\mathrm{r}}$,$W_{\mathrm{v}}^{\mathrm{r}}$ï¼Œ è¿™äº›å°±ç§°ä¸º attention headsï¼ˆæ³¨æ„åŠ›å¤´ï¼‰ã€‚
å¯¹äº input $x_{\mathrm{i}}$ï¼Œæ¯ä¸ª attention head äº§ç”Ÿä¸åŒçš„ output vector $y_{\mathrm{i}}^{\mathrm{r}}$ï¼ˆä¸€éƒ¨åˆ†è¾“å‡ºï¼‰ã€‚ æœ€åå†å°†è¿™äº›éƒ¨åˆ†è¾“å‡ºè¿æ¥èµ·æ¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢æ¥é™ç»´å› kã€‚

### 4.3.3 æå‡ multi-head self-attention æ•ˆç‡ï¼šquery/key/value é™ç»´
ç†è§£ multi-head self-attention æœ€ç®€å•çš„æ–¹æ³•æ˜¯æŠŠå®ƒçœ‹ä½œ**å¤šä¸ªå¹¶è¡Œçš„ self-attention æœºåˆ¶**ï¼Œ **æ¯ä¸ªéƒ½æœ‰è‡ªå·±çš„é”®ã€å€¼å’ŒæŸ¥è¯¢è½¬æ¢**ã€‚
Multi-head self-attention çš„ç¼ºç‚¹æ˜¯æ…¢ï¼Œå¯¹äº **R å¤´ï¼Œæ…¢ R å€**ã€‚ ä¸è¿‡æœ‰åŠæ³•ä¼˜åŒ–ï¼šæˆ‘ä»¬å¯ä»¥å®ç°è¿™æ ·çš„ multi-head self-attentionï¼Œå®ƒæ—¢èƒ½åˆ©ç”¨å¤šä¸ª self-attention æå‡è¾¨è¯†åŠ›ï¼Œ åˆä¸ single-head self-attention åŸºæœ¬ä¸€æ ·å¿«ã€‚è¦å®ç°è¿™ä¸ªç›®çš„ï¼Œ**æ¯ä¸ª head éœ€è¦å¯¹ query/key/value é™ç»´ã€‚ å¦‚æœè¾“å…¥å‘é‡æœ‰ k=256 ç»´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰ h=4 ä¸ª attention headï¼Œåˆ™é™ç»´æ“ä½œåŒ…æ‹¬ï¼š**
- å°†**è¾“å…¥å‘é‡ä¹˜ä»¥ä¸€ä¸ª 256Ã—64 çŸ©é˜µï¼Œè¿™ä¼šå°† input vector ä» 256 ç»´é™åˆ° 64 ç»´**ï¼›
- **å¯¹äºæ¯ä¸ª head éœ€è¦æ‰§è¡Œ 3 æ¬¡é™ç»´ï¼šåˆ†åˆ«é’ˆå¯¹ query/key/value çš„è®¡ç®—**ã€‚
æˆ‘ä»¬ç”šè‡³åªç”¨ä¸‰æ¬¡ kÃ—k çŸ©é˜µä¹˜æ³•å°±èƒ½å®ç° multi-head åŠŸèƒ½ï¼Œ å”¯ä¸€éœ€è¦çš„é¢å¤–æ“ä½œæ˜¯å°†ç”Ÿæˆçš„ output vector é‡æ–°æŒ‰å—æ’åºï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/kqv-computation.png)
å›¾ï¼šä¸ºäº†æœ‰æ•ˆåœ°è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬å°†æŠ•å½±çš„è®¡ç®—ç»“åˆåˆ°è¾ƒä½ç»´çš„è¡¨ç¤ºä¸­ï¼Œå¹¶å°†é”®ã€æŸ¥è¯¢å’Œå€¼çš„è®¡ç®—ç»“åˆåˆ°ä¸‰ä¸ªk * kçŸ©é˜µä¸­ã€‚

### 4.3.4 å®Œæ•´å·¥ä½œæµ
ä¸‹å›¾å±•ç¤ºäº†çš„æ•´ä¸ª multi-head self-attention è¿‡ç¨‹ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/multi-head.png)
å›¾ï¼š4-head self-attention çš„ç›´è§‚è§£é‡Šã€‚å¯¹è¾“å…¥è¿›è¡Œé™ç»´ï¼Œé’ˆå¯¹ key/value/query åˆ†åˆ«è¿›è¡ŒçŸ©é˜µè¿ç®—æ¥å®ç°ã€‚
ä»å·¦åˆ°å³åˆ†ä¸º 5 åˆ—ï¼š

1. åŸå§‹ 256-ç»´ input vectorï¼›
2. è¾“å…¥é™ç»´ï¼šå°† input vector ä¹˜ä»¥ 256x64 çŸ©é˜µï¼Œé™ç»´åˆ° 64 ç»´ï¼›æ³¨æ„ï¼šå¯¹æ¯ä¸ª input vector éœ€è¦åˆ†åˆ«é’ˆå¯¹ query/key/value é™ç»´ï¼Œæ€»å…±æ˜¯ 3 éï¼›
3.  å°†é™ç»´åçš„ input åˆ†åˆ«è¾“å…¥å¤šä¸ªå¹¶è¡Œçš„ self-attentionï¼›
4. è®¡ç®—å¾—åˆ°å¤šä¸ªé™ç»´ä¹‹åçš„ output vectorï¼›
5. å¯¹ä½ç»´åº¦ output vectors è¿›è¡Œæ‹¼æ¥ï¼Œé‡æ–°å›åˆ°ä¸ input vectors ä¸€æ ·çš„ç»´åº¦ã€‚

## 4.5 multi-head vs. single-head æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”
å‚æ•°æŒ‡çš„æ˜¯åœ¨å°† input vector å˜æˆ output vector è¿‡ç¨‹ä¸­ç”¨åˆ°çš„é‚£äº›ç³»æ•°ï¼ˆæƒé‡çŸ©é˜µï¼‰ã€‚
æˆ‘ä»¬å‡è®¾è¾“å…¥çš„æ˜¯ k-ç»´ input vectorsï¼Œæ¥ä¸‹æ¥åˆ†åˆ«çœ‹ä¸‹ multi-head å’Œ single-head çš„å‚æ•°æ•°é‡ã€‚

**4.5.1 single-head**
æƒé‡çŸ©é˜µ $W_{\mathrm{ij}}$ï¼Œå…¶ä¸­ i,jâˆˆ[0,k]ï¼›
3 ä¸ªå¹³é¢ï¼šquery/key/valueï¼›
å› æ­¤æ€»å‚æ•°æ•°é‡æ˜¯ $3\mathbf{k}^{2}$

**4.5.2 multi-head**
å‡è®¾æœ‰ 4 ä¸ª headï¼Œå³ h=4ï¼Œ

æ¯ä¸ª head å¯¹åº”ä¸€ä¸ª self-attentionï¼Œæ¯ä¸ª self-attention 3 ä¸ªå¹³é¢ï¼ˆquery/key/valueï¼‰ï¼Œå› æ­¤æ€»å…± 3h ä¸ªå¹³é¢ï¼›
æ¯ä¸ªå¹³é¢çš„æƒé‡çŸ©é˜µ$W_{\mathrm{ij}}$ï¼Œå…¶ä¸­ iâˆˆ[0,k],jâˆˆ[0,k/h]ï¼›
å› æ­¤æ€»çš„å‚æ•°ä¸ªæ•°ï¼š$3{\mathrm{hk}}^{\mathrm{{k/h}}}=3{\mathrm{k}}^{2}$ï¼Œä¸ single-head self-attention çš„å‚æ•°æ•°é‡ç›¸åŒã€‚
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/multi-head%20(1).png)
>å”¯ä¸€çš„åŒºåˆ«æ˜¯ multi-head self-attention æœ€åæ‹¼æ¥ output vector æ—¶å¤šäº†ä¸€ä¸ªçŸ©é˜µ Woã€‚ä¸ single-head ç›¸æ¯”ï¼Œè¿™å¢åŠ äº† k2 ä¸ªå‚æ•°ã€‚ åœ¨å¤§å¤šæ•° Transformer ä¸­ï¼Œæ¯æ¬¡ self-attention ä¹‹åä¼šç´§è·Ÿç€ä¸€ä¸ªå‰é¦ˆå±‚ï¼ˆfeed-forward layerï¼‰ï¼Œå› æ­¤è¿™å¯èƒ½ä¸æ˜¯ç»å¯¹å¿…è¦çš„ã€‚ ä½†æˆ‘è¿˜æœªè§è¿‡èƒ½å¦æŠŠ Wo å»æ‰çš„ä¸¥è‚ƒè®¨è®ºã€‚

# 5 self-attention ä¸»è¦ä»£ç å®ç°
æ¥ä¸‹æ¥å°†æˆ‘ä»¬çš„ self-attention å®ç°ä¸ºä¸€ä¸ª python æ¨¡å—ï¼Œæ–¹ä¾¿å¤ç”¨ï¼š
```
import torch
from torch import nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, k, heads=4, mask=False):
    super().__init__()
			# è¾“å…¥å‘é‡æœ‰ kç»´
    assert k % heads == 0 # input vector size å¿…é¡»æ˜¯ heads çš„æ•´æ•°å€
    self.k, self.heads = k, heads
```
ç„¶åï¼Œ**åˆå§‹åŒ–å‡ ä¸ª k*k çš„çº¿æ€§å˜æ¢çŸ©é˜µ**ï¼Œ nn.Linear(bias=False) èƒ½å®ç°è¿™ä¸ªæ•ˆæœï¼Œå¹¶åšäº†é€‚å½“çš„åˆå§‹åŒ–ï¼š
```
# Compute the queries, keys and values for all heads
# y = Wq*x+y(å…¶ä¸­Wq=(k,k)) 
self.tokeys    = nn.Linear(k, k, bias=False)
self.toqueries = nn.Linear(k, k, bias=False)
self.tovalues  = nn.Linear(k, k, bias=False)

# This will be applied after the multi-head self-attention operation.
self.unifyheads = nn.Linear(k, k)
```
æ¥ä¸‹æ¥å°±å¯ä»¥å®ç°äº† self-attention çš„è®¡ç®—äº†ï¼Œåœ¨æ¨¡å‹ä¸­å¯¹åº”çš„æ˜¯ forward() å‡½æ•°ã€‚
```
def forward(self, x):
    # b:batch size, t:sequence length, k:embeding feature dimension
    b, t, k = x.size()
    h = self.heads

    # é¦–å…ˆï¼Œä¸ºæ‰€æœ‰ heads è®¡ç®— query/key/valueï¼Œå¾—åˆ°çš„æ˜¯å®Œæ•´åµŒå…¥ç»´åº¦çš„ k*k çŸ©é˜µ
    #(b,t,k)=(b,t,k)*(k,k)-->(b, t, k)
		 queries = self.toqueries(x) # k*k
    keys    = self.tokeys(x)  # k*k
    values  = self.tovalues(x) # k*k

    # æ¥ä¸‹æ¥å°† queries/keys/values åˆ‡å—ï¼ˆé™ç»´ï¼‰ï¼Œåˆ†åˆ«é€åˆ°ä¸åŒçš„ head
    s = k // h  #s:é™ç»´ç»´åº¦
	   # (b, t, k)-->(b, t, h, s)
    keys    = keys.view(b, t, h, s)
    queries = queries.view(b, t, h, s)
    values  = values.view(b, t, h, s)
```
è¿™å¯¹ tensors è¿›è¡Œäº†**ç®€å• reshapeï¼Œç°åœ¨ tensors å¢åŠ äº†ä¸€ä¸ª head ç»´åº¦**ã€‚ å¯¹äºæ¯ä¸ª input vectorï¼Œå¯ä»¥ç†è§£ä¸ºå°†è¿™ä¸ª **k*1 çŸ©é˜µå˜æˆäº†ä¸€ä¸ª h * k//h çŸ©é˜µ**ï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/reshape.png)
æ¥ä¸‹æ¥è®¡ç®—ç‚¹ç§¯ã€‚æ¯ä¸ª head çš„ç‚¹ç§¯è¿ç®—éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºæˆ‘ä»¬**å°† heads fold åˆ° batch dimentionã€‚ è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ torch.bmm()ï¼ˆbatch matrix multiplificationï¼‰ï¼Œè€Œ keys, queries and values å¯ä»¥çœ‹åšæ˜¯ batch**ï¼Œåªæ˜¯ batch size ç¨å¤§äº†ä¸€ç‚¹ã€‚
ç”±äº head å’Œ batch dimension æ²¡æœ‰ç›¸é‚»ï¼Œå› æ­¤æˆ‘ä»¬åœ¨ reshape ä¹‹å‰éœ€è¦è½¬ç½®ã€‚ è¿™ä¸ªæ“ä½œå¼€é”€å¾ˆå¤§ï¼Œä½†ä¼¼ä¹æ— æ³•é¿å…ï¼š
```
# - fold heads into the batch dimension
# (b*h, t, s)=(b, t, h, s)-->(b, h, t, s)-->(b*h, t, s)
keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)
queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)
values = values.transpose(1, 2).contiguous().view(b * h, t, s)
```
>æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨reshape() è€Œä¸æ˜¯viewï¼ˆï¼‰æ¥é¿å…å¯¹continuousï¼ˆï¼‰çš„è°ƒç”¨ï¼Œä½†å½“æˆ‘ä»¬å¤åˆ¶å¼ é‡å’Œåªæ˜¯æŸ¥çœ‹å®ƒæ—¶ï¼Œæˆ‘æ›´å–œæ¬¢å°†å…¶æ˜ç¡®åŒ–ã€‚è¯·å‚é˜…æ­¤ç¬”è®°æœ¬ä»¥äº†è§£å·®å¼‚çš„è§£é‡Šã€‚ 

è·Ÿä¹‹å‰ä¸€æ ·ï¼Œç‚¹ç§¯å¯ä»¥ç”¨å•ä¸ªçŸ©é˜µä¹˜æ³•å®ç°ï¼Œä½†ç°åœ¨æ˜¯ queries ä¹˜ä»¥ keysï¼Œ
```
# Get dot product of queries and keys, and scale
# (b*h, t, t) =(b * h, t, s)*(b * h, s, t)-->(b*h, t, t) 
dot = torch.bmm(queries, keys.transpose(1, 2)) # -- dot has size (b*h, t, t) containing raw weights
dot = dot / (k ** (1/2))                # scale the dot product
# (b*h, t, t)
dot = F.softmax(dot, dim=2)    # normalize, dot now contains row-wise normalized weights
```
ç„¶åç”¨å¾—åˆ°çš„æƒé‡å†å’Œ values åšç‚¹ç§¯ï¼Œå¾—åˆ°çš„å°±æ˜¯æ¯ä¸ª attention head çš„è¾“å‡ºï¼š
 ```
 # (b, h, t, s)=(b*h, t, t)*(b*h, t, s)*-->(b, h, t, s) 
 out = torch.bmm(dot, values).view(b, h, t, s) # apply the self attention to the values
 ```
 ä¸ºäº†å°†æ¯ä¸ª head çš„è¾“å‡ºé‡æ–°ä¸²è”èµ·æ¥å¾—åˆ° k-ç»´çš„æœ€ç»ˆè¾“å‡ºï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡è½¬ç½®ï¼Œç„¶åå°†è½¬ç½®åçš„çŸ©é˜µé€åˆ° unifyheads layer åšæœ€å¥½çš„ç»´åº¦å˜æ¢ï¼š
  ```
# swap h, t back, unify heads
# (b, t, s*h) =(b, h, t, s)-->(b, t, h, s) -->(b, t, s*h)
out = out.transpose(1, 2).contiguous().view(b, t, s * h)
# (b, t, s * h)
return self.unifyheads(out)
```
è‡³æ­¤ï¼Œä¸€ä¸ª multi-head, scaled dot-product self attention æ¨¡å‹å°±å®ç°å¥½äº†ã€‚
>The implementation can be made more concise using einsum notation (see an example here).

# 6 åŸºäº multi-head self-attention å®ç° transformers
## 6.1 Transformer å®šä¹‰
transformer ä¸ä»…ä»…æ˜¯ä¸€ä¸ª self-attention layerï¼Œè¿˜æ˜¯ä¸€ç§æ¶æ„ï¼ˆarchitectureï¼‰ã€‚ å¦‚ä½•ç²¾ç¡®åœ°åˆ¤æ–­ä¸€ä¸ªä¸œè¥¿æ˜¯æˆ–è€…ä¸æ˜¯ transformer è¿˜ä¸æ˜¯å¾ˆæ˜ç¡®ï¼Œæœ¬æ–‡é‡‡ç”¨å¦‚ä¸‹çš„å®šä¹‰ï¼š
>ä»»ä½•è®¾è®¡ç”¨æ¥å¤„ç†ä¸€ç»„è¿æ¥çš„å•å…ƒï¼ˆä¾‹å¦‚åºåˆ—ä¸­çš„ token æˆ–å›¾åƒä¸­çš„åƒç´ ï¼‰ï¼Œ å¦‚æœå•å…ƒä¹‹é—´çš„å”¯ä¸€äº¤äº’æ–¹å¼æ˜¯ self-attentionï¼Œé‚£è¿™æ ·çš„æ¶æ„å°±ç§°ä¸º transformerã€‚

ä¸å…¶ä»–æœºåˆ¶ï¼ˆå¦‚å·ç§¯ï¼‰ä¸€æ ·ï¼Œå¯ä»¥åŸºäº self-attention å±‚æ„å»ºæˆæ›´å¤§çš„ç½‘ç»œã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œ æˆ‘ä»¬éœ€è¦å°† self-attention é‡æ„ä¸ºä¸€ä¸ªå¯ä»¥å¤ç”¨çš„ blockã€‚

## 6.2 Transformer block
æ„å»ºåŸºæœ¬çš„ transformer æœ‰å‡ ç§ç•¥å¾®ä¸åŒçš„æ–¹å¼ï¼Œä½†å¤§å¤šæ•°ç»“æ„éƒ½å¤§è‡´å¦‚ä¸‹ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/transformer-block.png)
å„å—ä¾æ¬¡æ‰§è¡Œï¼š

1. self-attention å±‚ï¼›
2. å½’ä¸€åŒ–å±‚ï¼›
3. å‰é¦ˆå±‚ï¼ˆfeed forward layerï¼‰ï¼Œæ¯ä¸ª MLPï¼ˆmulti-layer perceptronï¼‰åˆ†åˆ«ä¸æ¯ä¸ª input åšè¿ç®—ï¼›
4. å¦ä¸€ä¸ªå±‚å½’ä¸€åŒ–ã€‚
ä¸¤æ¬¡å½’ä¸€åŒ–ä¹‹å‰éƒ½ä¼šæ·»åŠ æ®‹å·®è¿æ¥ï¼ˆresidual connectionsï¼‰ã€‚

å„ç»„ä»¶çš„é¡ºåºå¹¶ä¸æ˜¯åªèƒ½è¿™æ ·ï¼Œé‡è¦çš„æ˜¯
1. å°† self-attention ä¸å±€éƒ¨å‰é¦ˆç›¸ç»“åˆï¼ˆcombine self-attention with a local feedforwardï¼‰ï¼Œ
2. æ·»åŠ å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ã€‚
å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥æ˜¯å¸¸è§„æŠ€å·§ï¼Œç”¨äºä½¿æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ›´å¿«ã€æ›´å‡†ç¡®ã€‚ å±‚å½’ä¸€åŒ–ä»…åº”ç”¨äºåµŒå…¥ç»´åº¦ï¼ˆlayer normalization is applied over the embedding dimension onlyï¼‰ã€‚
å®ç°ï¼š
  ```
class TransformerBlock(nn.Module):
  def __init__(self, k, heads):
    super().__init__()

    self.attention = SelfAttention(k, heads=heads)

    self.norm1 = nn.LayerNorm(k)
    self.norm2 = nn.LayerNorm(k)

    self.ff = nn.Sequential(
      nn.Linear(k, 4 * k),
      nn.ReLU(),
      nn.Linear(4 * k, k))

  def forward(self, x):
    attended = self.attention(x)
    x = self.norm1(attended + x)

    fedforward = self.ff(x)
    return self.norm2(fedforward + x)
```
è¿™é‡Œæˆ‘ä»¬é€‰æ‹©äº†è®© feed forward éšè—å±‚æ¯” input/output å¤§ 4 å€ï¼Œè¿™ä¸ªå€æ•°çš„é€‰æ‹©æ˜¯éšæ„çš„ï¼Œ æ›´å°çš„å€æ•°å¯èƒ½ä¹Ÿèƒ½å·¥ä½œï¼Œå¹¶ä¸”å ç”¨å†…å­˜æ›´å°‘ï¼Œä½†æœ€å°ä¸èƒ½å°äº input/output layer å¤§å°ã€‚

## 6.3 æ–‡æœ¬åˆ†ç±»ï¼ˆtext classificationï¼‰transformer
æˆ‘ä»¬èƒ½æ„å»ºçš„æœ€ç®€å• transformer å« **sequence classifier**ï¼ˆé¡ºåºåˆ†ç±»å™¨ï¼‰ã€‚ æˆ‘ä»¬ç”¨ IMDbï¼ˆInternet Movie Databaseï¼‰sentiment classification æ•°æ®é›†ï¼š
- æ•°æ®å†…å®¹æ˜¯å½±è¯„ï¼Œ
- token åŒ–æˆäº†å•è¯åºåˆ—ï¼Œ
- åˆ†ç±»æ ‡ç­¾æ˜¯ positive å’Œ negativeï¼ˆå¯¹ç”µå½±çš„æ­£é¢/è´Ÿé¢è¯„ä»·ï¼‰

**æ¶æ„çš„æ ¸å¿ƒéƒ¨åˆ†éå¸¸ç®€å•ï¼Œå°±æ˜¯ä¸€é•¿ä¸² transformer blockã€‚æ‰€éœ€åšçš„äº‹æƒ…ï¼š**
- å¦‚ä½•å°† input sequence feed ç»™è¿™ä¸ªé•¿é“¾ï¼Œ
- å¦‚ä½•å¯¹æœ€ç»ˆ output sequence è¿›è¡Œå˜æ¢ï¼Œå¾—åˆ°å•ä¸ªåˆ†ç±»ç»“æœã€‚

### 6.3.1 è¾“å‡ºï¼šå•ä¸ªåˆ†ç±»ç»“æœ
ä» sequence-to-sequence layers æ„å»º sequence classifier çš„æœ€å¸¸è§æ–¹æ³•æ˜¯**å¯¹æœ€ç»ˆè¾“å‡ºåºåˆ—åš global average pooling**ï¼Œ**å¹¶å°†ç»“æœæ˜ å°„åˆ° softmaxed class vector**ã€‚
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/classifier.png)
å›¾ï¼šç®€å•çš„åºåˆ—åˆ†ç±»Transformeræ¦‚è¿°ã€‚å¯¹è¾“å‡ºåºåˆ—è¿›è¡Œå¹³å‡ï¼Œä»¥äº§ç”Ÿä»£è¡¨æ•´ä¸ªåºåˆ—çš„å•ä¸ªè½½ä½“ã€‚è¯¥è½½ä½“è¢«æŠ•å½±åˆ°æ¯ä¸ªç±»å…·æœ‰ä¸€ä¸ªå…ƒç´ çš„è½½ä½“ï¼Œå¹¶è¿›è¡Œè½¯æ”¾å¤§ä»¥äº§ç”Ÿæ¦‚ç‡ã€‚

### 6.3.2 è¾“å…¥ï¼šè¯åºæ•æ„Ÿï¼ˆusing the positionsï¼‰
å‰é¢å·²ç»è®¨è®ºäº†åµŒå…¥å±‚çš„åŸç†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ç”¨å®ƒæ¥è¡¨ç¤ºå•è¯ã€‚
æ­£å¦‚å‰é¢å·²ç»æåˆ°çš„ï¼Œ**æˆ‘ä»¬æ­£åœ¨å †å ï¼ˆstackingï¼‰æ’åˆ—ç­‰å˜å±‚ï¼ˆpermutation equivariant layersï¼‰ï¼Œ æœ€ç»ˆçš„ global average pooling æ˜¯æ’åˆ—ä¸å˜çš„ï¼ˆpermutation invariantï¼‰ï¼Œ å› æ­¤æ•´ä¸ªç½‘ç»œä¹Ÿæ˜¯æ’åˆ—ä¸å˜çš„**ã€‚ç”¨ç™½è¯æ¥è¯´ï¼Œ å³ä½¿æˆ‘ä»¬æ‰“ä¹±å¥å­ä¸­çš„å•è¯é¡ºåºï¼Œæ— è®ºæˆ‘ä»¬å­¦åˆ°ä»€ä¹ˆæƒé‡ï¼Œéƒ½ä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„åˆ†ç±»ç»“æœã€‚ æ˜¾ç„¶ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ª**å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è‡³å°‘å¯¹è¯åºå…·æœ‰ä¸€å®šçš„æ•æ„Ÿæ€§**ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

è§£å†³æ–¹æ¡ˆå¾ˆç®€å•ï¼š**åˆ›å»ºä¸€ä¸ªä¸ input ç­‰é•¿çš„å‘é‡è®°å½•å½“å‰å¥å­ä¸­å•è¯çš„ä½ç½®ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° word embedding ä¸­**ã€‚ å…·ä½“åˆ°å®ç°ä¸Šï¼Œæœ‰ä¸¤ç§é€‰æ‹©ï¼š

**ä½ç½®åµŒå…¥ï¼ˆposition embeddingsï¼‰**
åƒåµŒå…¥æ–‡å­—ä¸€æ ·åµŒå…¥ä½ç½®ã€‚å°±åƒåˆ›å»ºåµŒå…¥å‘é‡**V**cat å’Œ **V**susan ä¸€æ ·ï¼Œ æˆ‘ä»¬åˆ›å»ºåµŒå…¥å‘é‡ğ¯12 å’Œ ğ¯25ã€‚
**ç¼ºç‚¹æ˜¯åœ¨è®­ç»ƒæœŸé—´å¿…é¡»çœ‹åˆ°æ¯ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œå¦åˆ™ç›¸å…³çš„ä½ç½®åµŒå…¥å¾—ä¸åˆ°è®­ç»ƒ**ã€‚ ä¼˜ç‚¹æ˜¯æ•ˆæœè¿˜ä¸é”™ï¼Œè€Œä¸”å¾ˆå®¹æ˜“å®ç°ã€‚

**ä½ç½®ç¼–ç ï¼ˆposition encodingsï¼‰**
**ä½ç½®ç¼–ç ä¸ä½ç½®åµŒå…¥çš„å·¥ä½œæ–¹å¼ç±»ä¼¼ï¼Œä½†ä¸å­¦ä¹ ä½ç½®å‘é‡ï¼Œè€Œåªæ˜¯é€‰æ‹©ä¸€äº›å‡½æ•°**
f: N-->$R^{\mathrm{k}}$
å°†ä½ç½®æ˜ å°„åˆ°å®å€¼å‘é‡ï¼Œå¹¶è®©ç½‘ç»œå¼„æ¸…æ¥šå¦‚ä½•è§£é‡Šè¿™äº›ç¼–ç ã€‚
å¥½å¤„æ˜¯ï¼Œ**å¯¹äºç²¾å¿ƒé€‰æ‹©çš„å‡½æ•°ï¼Œç½‘ç»œèƒ½å¤Ÿå¤„ç†æ¯”è®­ç»ƒæœŸé—´çœ‹åˆ°çš„åºåˆ—æ›´é•¿çš„åºåˆ—**ï¼ˆåœ¨å®ƒä»¬ä¸Šè¡¨ç°åº”è¯¥ä¸ä¼šå¤ªå¥½ï¼Œä½†è‡³å°‘æˆ‘ä»¬å¯ä»¥ checkï¼‰ã€‚ **ç¼ºç‚¹æ˜¯ç¼–ç å‡½æ•°çš„é€‰æ‹©æ˜¯ä¸€ä¸ªå¤æ‚çš„è¶…å‚æ•°ï¼ˆa complicated hyperparameterï¼‰ï¼Œå®ç°èµ·æ¥æœ‰ç‚¹å¤æ‚ã€‚**

6.3.3 åŸºäº Pytorch å®ç°
ç®€å•èµ·è§ï¼Œæœ¬æ–‡ä½¿ç”¨**ä½ç½®åµŒå…¥ï¼ˆposition embeddingsï¼‰**æ¥è®°å½• input é¡ºåºã€‚
ä»¥ä¸‹å°±æ˜¯æˆ‘ä»¬çš„ text classification transformer çš„å®Œæ•´å®ç°ï¼š
```
class Transformer(nn.Module):
    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):
        super().__init__()

        self.num_tokens = num_tokens
        self.token_emb = nn.Embedding(num_tokens, k)
        self.pos_emb = nn.Embedding(seq_length, k)

        # The sequence of transformer blocks that does all the heavy lifting
        tblocks = []
        for i in range(depth):
            tblocks.append(TransformerBlock(k=k, heads=heads))
        self.tblocks = nn.Sequential(*tblocks)

        # Maps the final output sequence to class logits
        self.toprobs = nn.Linear(k, num_classes)

    def forward(self, x):
        """
        :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary).
        :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes).
        """
        # generate token embeddings
        #(b, t, k)=(b,t, k)*(?)-->(b, t, k)
					 tokens = self.token_emb(x)
        b, t, k = tokens.size()

        # generate position embeddings
        positions = torch.arange(t)
        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)

        x = tokens + positions # ä¸ºä»€ä¹ˆæ–‡æœ¬åµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ ï¼Œæ²¡æœ‰ç†è®ºï¼Œå¯èƒ½å°±æ˜¯å®éªŒä¸‹æ¥æ•ˆæœä¸é”™ã€‚
                               # https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
        x = self.tblocks(x)

        # Average-pool over the t dimension and project to class
        # probabilities
        x = self.toprobs(x.mean(dim=1))
        return F.log_softmax(x, dim=1)
```
åœ¨æ·±åº¦ä¸º 6 ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ä¸º 512 æ—¶ï¼Œè¿™ä¸ª transformer å–å¾—äº† 85% çš„å‡†ç¡®åº¦ï¼Œä¸ RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰æ¨¡å‹çš„ç»“æœç›¸å½“ï¼Œä½†è®­ç»ƒé€Ÿåº¦å¿«å¾—å¤šã€‚ è¦çœ‹åˆ°è¿™ä¸ª transformer çœŸæ­£æ¥è¿‘äººç±»çš„æ€§èƒ½ï¼Œå°±éœ€è¦åœ¨æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒæ›´æ·±çš„æ¨¡å‹ã€‚åæ–‡å°†è¯¦ç»†ä»‹ç»æ€ä¹ˆåšã€‚

