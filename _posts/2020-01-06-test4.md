---
layout: prediction_post
published: True
title:   Transformer 是如何工作的：TRANSFORMERS FROM SCRATCH
---
#  Transformer 是如何工作的：TRANSFORMERS FROM SCRATCH
* * *
整理和翻译自 2019 年（最后更新 2023 年）的一篇文章： Transformers From Scratch， 由浅入深地解释了 transformer/self-attention 背后的工作原理。
[原文链接](https://peterbloem.nl/blog/transformers)
07/07/2024 23:34
* * *
## 摘要
Transformer 是一类非常令人着迷的机器学习架构（a family of machine learning architectures）。 之前已经有一些不错的介绍文章（例如 [1, 2]），但过去几年 transformer 变得简单了很多， 因此要解释清楚现代架构（modern architectures）是如何工作的，比以前容易多了。本文试图丢掉历史包袱，开门见山地解释现代 transformer 的工作原理。

神经网络和反向传播（neural networks and backpropagation）的基本知识有助于更好地理解本文，
- [这个讲座](https://mlvu.github.io/beyondlinear/) 介绍了神经网络的基础知识；
- [这个讲座](https://mlvu.github.io/lecture07/) 介绍了神经网络如何应用于现代深度学习系统。
另外，理解本文程序需要一点 Pytorch 基础， 但没有基础关系也不大

## 1 self-attention（自注意力）模型
self-attention 运算是==所有transformer 架构的基本运算==
### 1.0 Attention（注意力）：名字由来

从最简形式上来说，神经网络是一系列**对输入进行加权计算，得到一个输出的过程**。
具体来说，比如给定一个**向量 [1,2,3,4,5] 作为输入**，**权重矩阵可能是 [0, 0, 0, 0.5, 0.5]**， 也就是说最终的 output 实际上只与 input 中的最后两个元素有关系 —— 换句话说， **这一层神经网络只关注最后两个元素（注意力在最后两个元素上）**， 其他元素是什么值对结果没有影响 —— 这就是==attention==这一名字的由来。
> 注意力模型大大降低了神经网络的计算量：经典神经网络是全连接的，而上面的例子中， 这一层神经网络不需要全连接了，每个输出连接到最后两个输入就行了，也就是从 1x5 维降低到了 1x2 维。
> 图像处理中的卷积神经网络（CNN）也是类似原理：只用一小块图像计算下一层的输出，而不是用整帧图像。

### 1.1 输入输出：vector-to-vector 运算
**Self-attention 是一个 sequence-to-sequence 运算**： 输入一个向量序列（a sequence of vectors），输出另一个向量序列。
我们用 𝐱1,𝐱2,…,𝐱t 表示输入向量，用 𝐲1,𝐲2,…,𝐲t 表示相应的输出向量，这些向量都是 k 维的。 要计算输出向量 𝐲i ，self-attention 只需对所有输入向量做加权平均（weighted average），
$\displaystyle\mathbf{y}_{\mathrm{i}}=\sum_{j}w_{ij}x_{j}$
**在传统神经网络中，权重都是（常量）参数， 但这里的权重并不是：==wij 是根据 𝐱i 和 𝐱j 计算出来的==。 计算它有很多种方式（算法），接下来看一种最简单的。**

#### 1.2 权重矩阵计算和归一化
计算权重矩阵的最简单函数就是点积（dot product）：
>点积(dot product)又叫标量积、数量积(scalar product)。它是两个数字序列的相应条目的乘积之和。

$\mathbf{w}_{\mathrm{ij}}^{\prime}=x_{\mathrm{i}}^{\textrm{T}}x_{\mathrm{j}}$
>注意到权重矩阵的计算跟它所在的位置 (i,j) 直接相关，也就是说，每个位置 (i,j) 对应的权重矩阵都不一样。

点积得到的结果取值范围是正负无穷，为了使累加和（表示概率）等于 100%， 需要对它们做归一化：用 pytorch 术语来说就是 softmax，
$w_{\mathrm{ij}}={\frac{\exp\mathbf{w}_{\mathrm{ij}}^{\prime}}{\sum_{\mathrm{j}}\exp\displaystyle w_{\mathrm{ij}}^{\prime}}}$
这会将每个权重矩阵归一化到 [0,1]，并且累加和等于 1。

#### 1.3 直观展示与小结
以上就是关于 self-attention 的基本运算。总结起来就是两点：
1. **vector-to-vector 运算**：self-attention 是对 input vector 做矩阵运算，得到一个加权结果作为 output vector；
2. **加权矩阵计算**：权重矩阵不是常量，而是跟它所在的位置 (i,j) 直接相关，根据对应位置的 input vector 计算。
用图来表示如下：
![self-attention 基本运算](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/self-attention.png)图: self-attention 基本运算
- **output vector 中的每个元素 $\mathbb{y}_{\mathrm{j}}$都是对 input vector 中所有元素的加权和；**
- **对于 $\mathbb{y}_{\mathrm{j}}$，加权矩阵由 input 元素$\mathbb{x}_{\mathrm{j}}$ 与每个 input 元素计算得到；**

要构建一个完整的 transformer 还需要一点其他东西，但最核心的运算就是以上这两个了。 更重要的是，
- 这是整个架构中，唯一在 input & output vector 之间 所做的运算；
- Transformer 架构中的其他运算都是单纯对 input vector 做运算。

## 2 self-attention 为什么有效？以电影推荐为例
步骤很简单：
1. 人工设计一些电影特征，比如浪漫指数、动作指数，
2. 人工设计一些用户特征，例如他们喜欢浪漫电影或动作片的可能性；

有了这两个维度的数据（特征向量）之后，对二者做点积（dot product）， 得到的就是电影属性与用户喜欢程度之间的匹配程度，用得分表示，
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-dot-product.png)
电影推荐：电影特征向量（浪漫、动作、喜剧）与用户特性向量（喜欢浪漫、动作、喜剧的程度）做点积运算


关于计算结果（得分）：
- 如果特征的符号相同，例如“浪漫电影 && 用户喜欢浪漫电影”， 或者“不是浪漫电影 && 用户不喜欢浪漫电影”，得到的点积就是正数；反之就是负数；
- 特征值的大小决定该特征对总分的贡献大小： 一部电影可能有点浪漫，但不是很明显，或者用户可能只是不喜欢浪漫，但也没到讨厌的程度。
这种推荐模型的好处是简单直接，很容易上手；缺点是规模大了很难搞， 因为对几百万部电影打标的成本非常高，精确标记用户喜欢或不喜欢什么也几乎是不可能的。

### 2.2 基于 self-attention 的推荐系统
接下来看基于 self-attention 的推荐系统是怎么设计的。

#### 2.2.1 电影特征和用户特征作为模型参数，匹配已知的用户偏好
也是两步：
1. **电影特征和用户特征不再直接做点积运算，而是作为模型的参数（parameters of the model）**；
2. **收集少量的用户偏好作为目标，然后通过优化用户特征和电影特征（模型参数）， 使二者的点积匹配已知的用户喜好**。

这就是 self-attention 的基本原理。注意， 尽管我们没有告诉模型某个特征意味着什么（表示什么）， 但实践证明，训练之后的特征确实反映了关于电影内容的合理语义。

>用素人术语来重新描述以上过程：我们告诉神经网络，
1. >我有一些关于电影和用户的信息，作为输入；有一些用户偏好信息，作为输出。
2. >你把这两者串联起来，能够根据输入预测输出，你自己怎么实现我不管，把最终模型（参数）给我就行了。译注。

![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-features.png)
图:从一个基本的 matrix factorization 模型学习到的前两个特征。 模型只用到了“哪些用户喜欢哪些电影”信息，而没有用到任何电影内容信息。 横轴：从流俗到高雅；纵轴：从小众到主流。信息来自 [4]。
>这些已经足够说明 dot product 是如何表示对象和它们的关系的。 更多关于推荐系统的内容，可移步 mlvu.github.io/lecture12。

#### 2.2.2 嵌入层：对输入进行处理

假设我们有一串单词作为输入，原理上只要将其作为 input vector 送到 self-attention 模型。 但实际上我们需要对这个 input vector 做一下预处理（下一节会解释为什么），生成一个中间表示， 这就是序列建模中的嵌入层。 具体来说，会为每个单词 t 分配一个嵌入向量（embedding vector）$\mathbf{v}_{\mathrm{t}}$（我们后面将学习到这个值）。

**嵌入层将 input vector**：
> the,cat,walks,on,the,street


**转换为 embedding vector**（注意：每个单词的维度从 1x1 变成了 1xN）：
> 𝐯the,𝐯cat,𝐯walks,𝐯on,𝐯the,𝐯street

将这个 embedding vector 输入 self-attention 层，**得到的就是 output vector**：
> 𝐲the,𝐲cat,𝐲walks,𝐲on,𝐲the,𝐲street

其中 𝐲cat 是所有嵌入向量的加权和（weighted sum），由它们与 𝐯cat 的（归一化）点积加权。

#### 2.2.3 直观解释
由于我们正在学习（learning） 𝐯t 的值是什么，两个词的“相关”程度完全由任务决定。
- 在大多数情况下，定冠词 "the" 与句子中其他单词表示什么意思（the interpretation of the other words）关系不大； 因此我们最终得到的嵌入层 𝐯the 与所有其他单词的点积可能很小或为负数；
- 另一方面，要解释这句话中 “walks” 的意思，弄清楚谁在走路是非常有用的。这很可能由名词表达， 因此对于像 cat 这样的名词和像 walks 这样的动词，我们可能最终学习到的 𝐯cat and 𝐯walks 点积是个较大的正数。

这就是 self-attention 背后的基本直觉：
1. 点积表示输入序列中两个向量的相关程度，“相关”由学习任务（learning task）定义，
2. 输出向量是整个输入序列的加权和，权重由这些点积决定。

### 2.4 self-attention 特殊属性
在继续之前，有些特殊属性需要提及一下，因为不同于在一般的 sequence-to-sequence 运算：

**到目前为止，我们的 self-attention 模型还没有参数（ 虽然下文中，我们还是会为 self-attention 添加几个参数）。**
- 换句话说，基本的 self-attention 实际上做什么完全取决于生成输入序列的上游机制。 例如嵌入层这种机制会驱动着 self-attention 学习基于点积的表示。

**self-attention 将输入当做一个集合（set）而不是序列（sequence）。
- 如果我们对输入序列进行重排（permute），输出序列除了也跟着重排，其他方面将完全相同， 也就是说 self-attention 是排列等变的（permutation equivariant）。 后面会看到，构建完整的 transformer 时，我们还是会引入一些东西来保持输入的顺序信息， 但要明白 **self-attention 本身是不关心输入的顺序属性的（sequential nature）**。

## 3. 实现一个基本的 self-attention
接下来我们基于 pytorch 实现前面介绍的最基础 self-attention 模型。
我们面临的第一个问题是如何用矩阵乘法表示 self-attention： 按照定义，直接遍历所有 input vectors 来计算 weight 和 output 就行， 但显然这种方式效率太低；改进的方式就是用 pytorch 的 tensor 来表示， 这是一个多维矩阵数据结构：
> A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.
> pytorch.org/docs/stable/tensors.html
- 输入 𝐗 由 **t 个** 、**k-维** vector 组成的序列，
- 引入一个 **mini-batch dimension b**，
就得到了一个三维矩阵 (b,t,k)，这就是一个 tensor

### 3.2 计算权重矩阵：输入矩阵 * 转置矩阵
接下来计算加权矩阵，它表示的是 input vector 之间的相关性， 因此用输入矩阵 𝐗 乘以它的转置矩阵（transpose），用 pytorch 库来计算非常方便。
```
import torch
import torch.nn.functional as F

# 假设我们有一些 tensor x 作为输入，它是 (b, t, k) 维矩阵
x = ...

# torch.bmm() 是批量矩阵乘法（batched matrix multiplication）函数，对一批矩阵执行乘法操作
raw_weights = torch.bmm(x, x.transpose(1, 2))
```
然后对权重矩阵进行正值化和归一化，以使得一个 row 内所有权重加起来为 1，
```
weights = F.softmax(raw_weights, dim=2)
```

3.3 计算输出
有了权重矩阵，计算输出就非常简单了：只需要将输入 𝐗 和权重矩阵相乘即可，一行代码搞定：
```
y = torch.bmm(weights, x)
```
**输出矩阵 𝐘 就是 size (b, t, k) 的 tensor，每一行都是对 𝐗 的行的加权。**
这就是 最基础的 self-attention 模型的实现： 两次矩阵乘法和一次归一化（softmax）。

