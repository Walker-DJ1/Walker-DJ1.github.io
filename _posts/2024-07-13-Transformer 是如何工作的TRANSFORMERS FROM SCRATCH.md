---
layout: post
comments: true
published: True
title: "Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šTRANSFORMERS FROM SCRATCH"
date:   2024-06-18 17:00:00
mathjax: false
---

#  Transformer æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼šTRANSFORMERS FROM SCRATCH
* * *
æ•´ç†å’Œç¿»è¯‘è‡ª 2019 å¹´ï¼ˆæœ€åæ›´æ–° 2023 å¹´ï¼‰çš„ä¸€ç¯‡æ–‡ç« ï¼š Transformers From Scratchï¼Œ ç”±æµ…å…¥æ·±åœ°è§£é‡Šäº† transformer/self-attention èƒŒåçš„å·¥ä½œåŸç†ã€‚
[åŸæ–‡é“¾æ¥](https://peterbloem.nl/blog/transformers)
[è¯‘æ–‡é“¾æ¥](https://arthurchiao.art/blog/transformers-from-scratch-zh/)
07/07/2024 23:34
* * *
# æ‘˜è¦
Transformer æ˜¯ä¸€ç±»éå¸¸ä»¤äººç€è¿·çš„æœºå™¨å­¦ä¹ æ¶æ„ï¼ˆa family of machine learning architecturesï¼‰ã€‚ ä¹‹å‰å·²ç»æœ‰ä¸€äº›ä¸é”™çš„ä»‹ç»æ–‡ç« ï¼ˆä¾‹å¦‚ [1, 2]ï¼‰ï¼Œä½†è¿‡å»å‡ å¹´ transformer å˜å¾—ç®€å•äº†å¾ˆå¤šï¼Œ å› æ­¤è¦è§£é‡Šæ¸…æ¥šç°ä»£æ¶æ„ï¼ˆmodern architecturesï¼‰æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œæ¯”ä»¥å‰å®¹æ˜“å¤šäº†ã€‚æœ¬æ–‡è¯•å›¾ä¸¢æ‰å†å²åŒ…è¢±ï¼Œå¼€é—¨è§å±±åœ°è§£é‡Šç°ä»£ transformer çš„å·¥ä½œåŸç†ã€‚

ç¥ç»ç½‘ç»œå’Œåå‘ä¼ æ’­ï¼ˆneural networks and backpropagationï¼‰çš„åŸºæœ¬çŸ¥è¯†æœ‰åŠ©äºæ›´å¥½åœ°ç†è§£æœ¬æ–‡ï¼Œ
- [è¿™ä¸ªè®²åº§](https://mlvu.github.io/beyondlinear/) ä»‹ç»äº†ç¥ç»ç½‘ç»œçš„åŸºç¡€çŸ¥è¯†ï¼›
- [è¿™ä¸ªè®²åº§](https://mlvu.github.io/lecture07/) ä»‹ç»äº†ç¥ç»ç½‘ç»œå¦‚ä½•åº”ç”¨äºç°ä»£æ·±åº¦å­¦ä¹ ç³»ç»Ÿã€‚
å¦å¤–ï¼Œç†è§£æœ¬æ–‡ç¨‹åºéœ€è¦ä¸€ç‚¹ Pytorch åŸºç¡€ï¼Œ ä½†æ²¡æœ‰åŸºç¡€å…³ç³»ä¹Ÿä¸å¤§

# 1 self-attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰æ¨¡å‹
self-attention è¿ç®—æ˜¯==æ‰€æœ‰transformer æ¶æ„çš„åŸºæœ¬è¿ç®—==
### 1.0 Attentionï¼ˆæ³¨æ„åŠ›ï¼‰ï¼šåå­—ç”±æ¥

ä»æœ€ç®€å½¢å¼ä¸Šæ¥è¯´ï¼Œç¥ç»ç½‘ç»œæ˜¯ä¸€ç³»åˆ—**å¯¹è¾“å…¥è¿›è¡ŒåŠ æƒè®¡ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªè¾“å‡ºçš„è¿‡ç¨‹**ã€‚
å…·ä½“æ¥è¯´ï¼Œæ¯”å¦‚ç»™å®šä¸€ä¸ª**å‘é‡ [1,2,3,4,5] ä½œä¸ºè¾“å…¥**ï¼Œ**æƒé‡çŸ©é˜µå¯èƒ½æ˜¯ [0, 0, 0, 0.5, 0.5]**ï¼Œ ä¹Ÿå°±æ˜¯è¯´æœ€ç»ˆçš„ output å®é™…ä¸Šåªä¸ input ä¸­çš„æœ€åä¸¤ä¸ªå…ƒç´ æœ‰å…³ç³» â€”â€” æ¢å¥è¯è¯´ï¼Œ **è¿™ä¸€å±‚ç¥ç»ç½‘ç»œåªå…³æ³¨æœ€åä¸¤ä¸ªå…ƒç´ ï¼ˆæ³¨æ„åŠ›åœ¨æœ€åä¸¤ä¸ªå…ƒç´ ä¸Šï¼‰**ï¼Œ å…¶ä»–å…ƒç´ æ˜¯ä»€ä¹ˆå€¼å¯¹ç»“æœæ²¡æœ‰å½±å“ â€”â€” è¿™å°±æ˜¯==attention==è¿™ä¸€åå­—çš„ç”±æ¥ã€‚
> æ³¨æ„åŠ›æ¨¡å‹å¤§å¤§é™ä½äº†ç¥ç»ç½‘ç»œçš„è®¡ç®—é‡ï¼šç»å…¸ç¥ç»ç½‘ç»œæ˜¯å…¨è¿æ¥çš„ï¼Œè€Œä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œ è¿™ä¸€å±‚ç¥ç»ç½‘ç»œä¸éœ€è¦å…¨è¿æ¥äº†ï¼Œæ¯ä¸ªè¾“å‡ºè¿æ¥åˆ°æœ€åä¸¤ä¸ªè¾“å…¥å°±è¡Œäº†ï¼Œä¹Ÿå°±æ˜¯ä» 1x5 ç»´é™ä½åˆ°äº† 1x2 ç»´ã€‚
> å›¾åƒå¤„ç†ä¸­çš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä¹Ÿæ˜¯ç±»ä¼¼åŸç†ï¼šåªç”¨ä¸€å°å—å›¾åƒè®¡ç®—ä¸‹ä¸€å±‚çš„è¾“å‡ºï¼Œè€Œä¸æ˜¯ç”¨æ•´å¸§å›¾åƒã€‚

### 1.1 è¾“å…¥è¾“å‡ºï¼švector-to-vector è¿ç®—
**Self-attention æ˜¯ä¸€ä¸ª sequence-to-sequence è¿ç®—**ï¼š è¾“å…¥ä¸€ä¸ªå‘é‡åºåˆ—ï¼ˆa sequence of vectorsï¼‰ï¼Œè¾“å‡ºå¦ä¸€ä¸ªå‘é‡åºåˆ—ã€‚
æˆ‘ä»¬ç”¨ ğ±1,ğ±2,â€¦,ğ±t è¡¨ç¤ºè¾“å…¥å‘é‡ï¼Œç”¨ ğ²1,ğ²2,â€¦,ğ²t è¡¨ç¤ºç›¸åº”çš„è¾“å‡ºå‘é‡ï¼Œè¿™äº›å‘é‡éƒ½æ˜¯ k ç»´çš„ã€‚ è¦è®¡ç®—è¾“å‡ºå‘é‡ ğ²i ï¼Œself-attention åªéœ€å¯¹æ‰€æœ‰è¾“å…¥å‘é‡åšåŠ æƒå¹³å‡ï¼ˆweighted averageï¼‰ï¼Œ
$\displaystyle\mathbf{y}_{\mathrm{i}}=\sum_{j}w_{ij}x_{j}$
**åœ¨ä¼ ç»Ÿç¥ç»ç½‘ç»œä¸­ï¼Œæƒé‡éƒ½æ˜¯ï¼ˆå¸¸é‡ï¼‰å‚æ•°ï¼Œ ä½†è¿™é‡Œçš„æƒé‡å¹¶ä¸æ˜¯ï¼š==wij æ˜¯æ ¹æ® ğ±i å’Œ ğ±j è®¡ç®—å‡ºæ¥çš„==ã€‚ è®¡ç®—å®ƒæœ‰å¾ˆå¤šç§æ–¹å¼ï¼ˆç®—æ³•ï¼‰ï¼Œæ¥ä¸‹æ¥çœ‹ä¸€ç§æœ€ç®€å•çš„ã€‚**

#### 1.2 æƒé‡çŸ©é˜µè®¡ç®—å’Œå½’ä¸€åŒ–
è®¡ç®—æƒé‡çŸ©é˜µçš„æœ€ç®€å•å‡½æ•°å°±æ˜¯ç‚¹ç§¯ï¼ˆdot productï¼‰ï¼š
>ç‚¹ç§¯(dot product)åˆå«æ ‡é‡ç§¯ã€æ•°é‡ç§¯(scalar product)ã€‚å®ƒæ˜¯ä¸¤ä¸ªæ•°å­—åºåˆ—çš„ç›¸åº”æ¡ç›®çš„ä¹˜ç§¯ä¹‹å’Œã€‚

$\mathbf{w}_{\mathrm{ij}}^{\prime}=x_{\mathrm{i}}^{\textrm{T}}x_{\mathrm{j}}$
>æ³¨æ„åˆ°æƒé‡çŸ©é˜µçš„è®¡ç®—è·Ÿå®ƒæ‰€åœ¨çš„ä½ç½® (i,j) ç›´æ¥ç›¸å…³ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯ä¸ªä½ç½® (i,j) å¯¹åº”çš„æƒé‡çŸ©é˜µéƒ½ä¸ä¸€æ ·ã€‚

ç‚¹ç§¯å¾—åˆ°çš„ç»“æœå–å€¼èŒƒå›´æ˜¯æ­£è´Ÿæ— ç©·ï¼Œä¸ºäº†ä½¿ç´¯åŠ å’Œï¼ˆè¡¨ç¤ºæ¦‚ç‡ï¼‰ç­‰äº 100%ï¼Œ éœ€è¦å¯¹å®ƒä»¬åšå½’ä¸€åŒ–ï¼šç”¨ pytorch æœ¯è¯­æ¥è¯´å°±æ˜¯ softmaxï¼Œ
$w_{\mathrm{ij}}={\frac{\exp\mathbf{w}_{\mathrm{ij}}^{\prime}}{\sum_{\mathrm{j}}\exp\displaystyle w_{\mathrm{ij}}^{\prime}}}$
è¿™ä¼šå°†æ¯ä¸ªæƒé‡çŸ©é˜µå½’ä¸€åŒ–åˆ° [0,1]ï¼Œå¹¶ä¸”ç´¯åŠ å’Œç­‰äº 1ã€‚

#### 1.3 ç›´è§‚å±•ç¤ºä¸å°ç»“
ä»¥ä¸Šå°±æ˜¯å…³äº self-attention çš„åŸºæœ¬è¿ç®—ã€‚æ€»ç»“èµ·æ¥å°±æ˜¯ä¸¤ç‚¹ï¼š
1. **vector-to-vector è¿ç®—**ï¼šself-attention æ˜¯å¯¹ input vector åšçŸ©é˜µè¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªåŠ æƒç»“æœä½œä¸º output vectorï¼›
2. **åŠ æƒçŸ©é˜µè®¡ç®—**ï¼šæƒé‡çŸ©é˜µä¸æ˜¯å¸¸é‡ï¼Œè€Œæ˜¯è·Ÿå®ƒæ‰€åœ¨çš„ä½ç½® (i,j) ç›´æ¥ç›¸å…³ï¼Œæ ¹æ®å¯¹åº”ä½ç½®çš„ input vector è®¡ç®—ã€‚
ç”¨å›¾æ¥è¡¨ç¤ºå¦‚ä¸‹ï¼š
![self-attention åŸºæœ¬è¿ç®—](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/self-attention.png)å›¾: self-attention åŸºæœ¬è¿ç®—
- **output vector ä¸­çš„æ¯ä¸ªå…ƒç´  $\mathbb{y}_{\mathrm{j}}$éƒ½æ˜¯å¯¹ input vector ä¸­æ‰€æœ‰å…ƒç´ çš„åŠ æƒå’Œï¼›**
- **å¯¹äº $\mathbb{y}_{\mathrm{j}}$ï¼ŒåŠ æƒçŸ©é˜µç”± input å…ƒç´ $\mathbb{x}_{\mathrm{j}}$ ä¸æ¯ä¸ª input å…ƒç´ è®¡ç®—å¾—åˆ°ï¼›**

è¦æ„å»ºä¸€ä¸ªå®Œæ•´çš„ transformer è¿˜éœ€è¦ä¸€ç‚¹å…¶ä»–ä¸œè¥¿ï¼Œä½†æœ€æ ¸å¿ƒçš„è¿ç®—å°±æ˜¯ä»¥ä¸Šè¿™ä¸¤ä¸ªäº†ã€‚ æ›´é‡è¦çš„æ˜¯ï¼Œ
- è¿™æ˜¯æ•´ä¸ªæ¶æ„ä¸­ï¼Œå”¯ä¸€åœ¨ input & output vector ä¹‹é—´ æ‰€åšçš„è¿ç®—ï¼›
- Transformer æ¶æ„ä¸­çš„å…¶ä»–è¿ç®—éƒ½æ˜¯å•çº¯å¯¹ input vector åšè¿ç®—ã€‚

# 2 self-attention ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿä»¥ç”µå½±æ¨èä¸ºä¾‹
æ­¥éª¤å¾ˆç®€å•ï¼š
1. äººå·¥è®¾è®¡ä¸€äº›ç”µå½±ç‰¹å¾ï¼Œæ¯”å¦‚æµªæ¼«æŒ‡æ•°ã€åŠ¨ä½œæŒ‡æ•°ï¼Œ
2. äººå·¥è®¾è®¡ä¸€äº›ç”¨æˆ·ç‰¹å¾ï¼Œä¾‹å¦‚ä»–ä»¬å–œæ¬¢æµªæ¼«ç”µå½±æˆ–åŠ¨ä½œç‰‡çš„å¯èƒ½æ€§ï¼›

æœ‰äº†è¿™ä¸¤ä¸ªç»´åº¦çš„æ•°æ®ï¼ˆç‰¹å¾å‘é‡ï¼‰ä¹‹åï¼Œå¯¹äºŒè€…åšç‚¹ç§¯ï¼ˆdot productï¼‰ï¼Œ å¾—åˆ°çš„å°±æ˜¯ç”µå½±å±æ€§ä¸ç”¨æˆ·å–œæ¬¢ç¨‹åº¦ä¹‹é—´çš„åŒ¹é…ç¨‹åº¦ï¼Œç”¨å¾—åˆ†è¡¨ç¤ºï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-dot-product.png)
ç”µå½±æ¨èï¼šç”µå½±ç‰¹å¾å‘é‡ï¼ˆæµªæ¼«ã€åŠ¨ä½œã€å–œå‰§ï¼‰ä¸ç”¨æˆ·ç‰¹æ€§å‘é‡ï¼ˆå–œæ¬¢æµªæ¼«ã€åŠ¨ä½œã€å–œå‰§çš„ç¨‹åº¦ï¼‰åšç‚¹ç§¯è¿ç®—


å…³äºè®¡ç®—ç»“æœï¼ˆå¾—åˆ†ï¼‰ï¼š
- å¦‚æœç‰¹å¾çš„ç¬¦å·ç›¸åŒï¼Œä¾‹å¦‚â€œæµªæ¼«ç”µå½± && ç”¨æˆ·å–œæ¬¢æµªæ¼«ç”µå½±â€ï¼Œ æˆ–è€…â€œä¸æ˜¯æµªæ¼«ç”µå½± && ç”¨æˆ·ä¸å–œæ¬¢æµªæ¼«ç”µå½±â€ï¼Œå¾—åˆ°çš„ç‚¹ç§¯å°±æ˜¯æ­£æ•°ï¼›åä¹‹å°±æ˜¯è´Ÿæ•°ï¼›
- ç‰¹å¾å€¼çš„å¤§å°å†³å®šè¯¥ç‰¹å¾å¯¹æ€»åˆ†çš„è´¡çŒ®å¤§å°ï¼š ä¸€éƒ¨ç”µå½±å¯èƒ½æœ‰ç‚¹æµªæ¼«ï¼Œä½†ä¸æ˜¯å¾ˆæ˜æ˜¾ï¼Œæˆ–è€…ç”¨æˆ·å¯èƒ½åªæ˜¯ä¸å–œæ¬¢æµªæ¼«ï¼Œä½†ä¹Ÿæ²¡åˆ°è®¨åŒçš„ç¨‹åº¦ã€‚
è¿™ç§æ¨èæ¨¡å‹çš„å¥½å¤„æ˜¯ç®€å•ç›´æ¥ï¼Œå¾ˆå®¹æ˜“ä¸Šæ‰‹ï¼›ç¼ºç‚¹æ˜¯è§„æ¨¡å¤§äº†å¾ˆéš¾æï¼Œ å› ä¸ºå¯¹å‡ ç™¾ä¸‡éƒ¨ç”µå½±æ‰“æ ‡çš„æˆæœ¬éå¸¸é«˜ï¼Œç²¾ç¡®æ ‡è®°ç”¨æˆ·å–œæ¬¢æˆ–ä¸å–œæ¬¢ä»€ä¹ˆä¹Ÿå‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚

## 2.2 åŸºäº self-attention çš„æ¨èç³»ç»Ÿ
æ¥ä¸‹æ¥çœ‹åŸºäº self-attention çš„æ¨èç³»ç»Ÿæ˜¯æ€ä¹ˆè®¾è®¡çš„ã€‚

### 2.2.1 ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä½œä¸ºæ¨¡å‹å‚æ•°ï¼ŒåŒ¹é…å·²çŸ¥çš„ç”¨æˆ·åå¥½
ä¹Ÿæ˜¯ä¸¤æ­¥ï¼š
1. **ç”µå½±ç‰¹å¾å’Œç”¨æˆ·ç‰¹å¾ä¸å†ç›´æ¥åšç‚¹ç§¯è¿ç®—ï¼Œè€Œæ˜¯ä½œä¸ºæ¨¡å‹çš„å‚æ•°ï¼ˆparameters of the modelï¼‰**ï¼›
2. **æ”¶é›†å°‘é‡çš„ç”¨æˆ·åå¥½ä½œä¸ºç›®æ ‡ï¼Œç„¶åé€šè¿‡ä¼˜åŒ–ç”¨æˆ·ç‰¹å¾å’Œç”µå½±ç‰¹å¾ï¼ˆæ¨¡å‹å‚æ•°ï¼‰ï¼Œ ä½¿äºŒè€…çš„ç‚¹ç§¯åŒ¹é…å·²çŸ¥çš„ç”¨æˆ·å–œå¥½**ã€‚

è¿™å°±æ˜¯ self-attention çš„åŸºæœ¬åŸç†ã€‚æ³¨æ„ï¼Œ å°½ç®¡æˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰æ¨¡å‹æŸä¸ªç‰¹å¾æ„å‘³ç€ä»€ä¹ˆï¼ˆè¡¨ç¤ºä»€ä¹ˆï¼‰ï¼Œ ä½†å®è·µè¯æ˜ï¼Œè®­ç»ƒä¹‹åçš„ç‰¹å¾ç¡®å®åæ˜ äº†å…³äºç”µå½±å†…å®¹çš„åˆç†è¯­ä¹‰ã€‚

>ç”¨ç´ äººæœ¯è¯­æ¥é‡æ–°æè¿°ä»¥ä¸Šè¿‡ç¨‹ï¼šæˆ‘ä»¬å‘Šè¯‰ç¥ç»ç½‘ç»œï¼Œ
1. >æˆ‘æœ‰ä¸€äº›å…³äºç”µå½±å’Œç”¨æˆ·çš„ä¿¡æ¯ï¼Œä½œä¸ºè¾“å…¥ï¼›æœ‰ä¸€äº›ç”¨æˆ·åå¥½ä¿¡æ¯ï¼Œä½œä¸ºè¾“å‡ºã€‚
2. >ä½ æŠŠè¿™ä¸¤è€…ä¸²è”èµ·æ¥ï¼Œèƒ½å¤Ÿæ ¹æ®è¾“å…¥é¢„æµ‹è¾“å‡ºï¼Œä½ è‡ªå·±æ€ä¹ˆå®ç°æˆ‘ä¸ç®¡ï¼ŒæŠŠæœ€ç»ˆæ¨¡å‹ï¼ˆå‚æ•°ï¼‰ç»™æˆ‘å°±è¡Œäº†ã€‚è¯‘æ³¨ã€‚

![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/movie-features.png)
å›¾:ä»ä¸€ä¸ªåŸºæœ¬çš„ matrix factorization æ¨¡å‹å­¦ä¹ åˆ°çš„å‰ä¸¤ä¸ªç‰¹å¾ã€‚ æ¨¡å‹åªç”¨åˆ°äº†â€œå“ªäº›ç”¨æˆ·å–œæ¬¢å“ªäº›ç”µå½±â€ä¿¡æ¯ï¼Œè€Œæ²¡æœ‰ç”¨åˆ°ä»»ä½•ç”µå½±å†…å®¹ä¿¡æ¯ã€‚ æ¨ªè½´ï¼šä»æµä¿—åˆ°é«˜é›…ï¼›çºµè½´ï¼šä»å°ä¼—åˆ°ä¸»æµã€‚ä¿¡æ¯æ¥è‡ª [4]ã€‚
>è¿™äº›å·²ç»è¶³å¤Ÿè¯´æ˜ dot product æ˜¯å¦‚ä½•è¡¨ç¤ºå¯¹è±¡å’Œå®ƒä»¬çš„å…³ç³»çš„ã€‚ æ›´å¤šå…³äºæ¨èç³»ç»Ÿçš„å†…å®¹ï¼Œå¯ç§»æ­¥ mlvu.github.io/lecture12ã€‚

### 2.2.2 åµŒå…¥å±‚ï¼šå¯¹è¾“å…¥è¿›è¡Œå¤„ç†

å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸²å•è¯ä½œä¸ºè¾“å…¥ï¼ŒåŸç†ä¸Šåªè¦å°†å…¶ä½œä¸º input vector é€åˆ° self-attention æ¨¡å‹ã€‚ ä½†å®é™…ä¸Šæˆ‘ä»¬éœ€è¦å¯¹è¿™ä¸ª input vector åšä¸€ä¸‹é¢„å¤„ç†ï¼ˆä¸‹ä¸€èŠ‚ä¼šè§£é‡Šä¸ºä»€ä¹ˆï¼‰ï¼Œç”Ÿæˆä¸€ä¸ªä¸­é—´è¡¨ç¤ºï¼Œ è¿™å°±æ˜¯åºåˆ—å»ºæ¨¡ä¸­çš„åµŒå…¥å±‚ã€‚ å…·ä½“æ¥è¯´ï¼Œä¼šä¸ºæ¯ä¸ªå•è¯ t åˆ†é…ä¸€ä¸ªåµŒå…¥å‘é‡ï¼ˆembedding vectorï¼‰$\mathbf{v}_{\mathrm{t}}$ï¼ˆæˆ‘ä»¬åé¢å°†å­¦ä¹ åˆ°è¿™ä¸ªå€¼ï¼‰ã€‚

**åµŒå…¥å±‚å°† input vector**ï¼š
> the,cat,walks,on,the,street


**è½¬æ¢ä¸º embedding vector**ï¼ˆæ³¨æ„ï¼šæ¯ä¸ªå•è¯çš„ç»´åº¦ä» 1x1 å˜æˆäº† 1xNï¼‰ï¼š
> ğ¯the,ğ¯cat,ğ¯walks,ğ¯on,ğ¯the,ğ¯street

å°†è¿™ä¸ª embedding vector è¾“å…¥ self-attention å±‚ï¼Œ**å¾—åˆ°çš„å°±æ˜¯ output vector**ï¼š
> ğ²the,ğ²cat,ğ²walks,ğ²on,ğ²the,ğ²street

å…¶ä¸­ ğ²cat æ˜¯æ‰€æœ‰åµŒå…¥å‘é‡çš„åŠ æƒå’Œï¼ˆweighted sumï¼‰ï¼Œç”±å®ƒä»¬ä¸ ğ¯cat çš„ï¼ˆå½’ä¸€åŒ–ï¼‰ç‚¹ç§¯åŠ æƒã€‚

### 2.2.3 ç›´è§‚è§£é‡Š
ç”±äºæˆ‘ä»¬æ­£åœ¨å­¦ä¹ ï¼ˆlearningï¼‰ ğ¯t çš„å€¼æ˜¯ä»€ä¹ˆï¼Œä¸¤ä¸ªè¯çš„â€œç›¸å…³â€ç¨‹åº¦å®Œå…¨ç”±ä»»åŠ¡å†³å®šã€‚
- åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œå®šå† è¯ "the" ä¸å¥å­ä¸­å…¶ä»–å•è¯è¡¨ç¤ºä»€ä¹ˆæ„æ€ï¼ˆthe interpretation of the other wordsï¼‰å…³ç³»ä¸å¤§ï¼› å› æ­¤æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°çš„åµŒå…¥å±‚ ğ¯the ä¸æ‰€æœ‰å…¶ä»–å•è¯çš„ç‚¹ç§¯å¯èƒ½å¾ˆå°æˆ–ä¸ºè´Ÿæ•°ï¼›
- å¦ä¸€æ–¹é¢ï¼Œè¦è§£é‡Šè¿™å¥è¯ä¸­ â€œwalksâ€ çš„æ„æ€ï¼Œå¼„æ¸…æ¥šè°åœ¨èµ°è·¯æ˜¯éå¸¸æœ‰ç”¨çš„ã€‚è¿™å¾ˆå¯èƒ½ç”±åè¯è¡¨è¾¾ï¼Œ å› æ­¤å¯¹äºåƒ cat è¿™æ ·çš„åè¯å’Œåƒ walks è¿™æ ·çš„åŠ¨è¯ï¼Œæˆ‘ä»¬å¯èƒ½æœ€ç»ˆå­¦ä¹ åˆ°çš„ ğ¯cat and ğ¯walks ç‚¹ç§¯æ˜¯ä¸ªè¾ƒå¤§çš„æ­£æ•°ã€‚

è¿™å°±æ˜¯ self-attention èƒŒåçš„åŸºæœ¬ç›´è§‰ï¼š
1. ç‚¹ç§¯è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­ä¸¤ä¸ªå‘é‡çš„ç›¸å…³ç¨‹åº¦ï¼Œâ€œç›¸å…³â€ç”±å­¦ä¹ ä»»åŠ¡ï¼ˆlearning taskï¼‰å®šä¹‰ï¼Œ
2. è¾“å‡ºå‘é‡æ˜¯æ•´ä¸ªè¾“å…¥åºåˆ—çš„åŠ æƒå’Œï¼Œæƒé‡ç”±è¿™äº›ç‚¹ç§¯å†³å®šã€‚

## 2.4 self-attention ç‰¹æ®Šå±æ€§
åœ¨ç»§ç»­ä¹‹å‰ï¼Œæœ‰äº›ç‰¹æ®Šå±æ€§éœ€è¦æåŠä¸€ä¸‹ï¼Œå› ä¸ºä¸åŒäºåœ¨ä¸€èˆ¬çš„ sequence-to-sequence è¿ç®—ï¼š

**åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬çš„ self-attention æ¨¡å‹è¿˜æ²¡æœ‰å‚æ•°ï¼ˆ è™½ç„¶ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¼šä¸º self-attention æ·»åŠ å‡ ä¸ªå‚æ•°ï¼‰ã€‚**
- æ¢å¥è¯è¯´ï¼ŒåŸºæœ¬çš„ self-attention å®é™…ä¸Šåšä»€ä¹ˆå®Œå…¨å–å†³äºç”Ÿæˆè¾“å…¥åºåˆ—çš„ä¸Šæ¸¸æœºåˆ¶ã€‚ ä¾‹å¦‚åµŒå…¥å±‚è¿™ç§æœºåˆ¶ä¼šé©±åŠ¨ç€ self-attention å­¦ä¹ åŸºäºç‚¹ç§¯çš„è¡¨ç¤ºã€‚

**self-attention å°†è¾“å…¥å½“åšä¸€ä¸ªé›†åˆï¼ˆsetï¼‰è€Œä¸æ˜¯åºåˆ—ï¼ˆsequenceï¼‰ã€‚
- å¦‚æœæˆ‘ä»¬å¯¹è¾“å…¥åºåˆ—è¿›è¡Œé‡æ’ï¼ˆpermuteï¼‰ï¼Œè¾“å‡ºåºåˆ—é™¤äº†ä¹Ÿè·Ÿç€é‡æ’ï¼Œå…¶ä»–æ–¹é¢å°†å®Œå…¨ç›¸åŒï¼Œ ä¹Ÿå°±æ˜¯è¯´ self-attention æ˜¯æ’åˆ—ç­‰å˜çš„ï¼ˆpermutation equivariantï¼‰ã€‚ åé¢ä¼šçœ‹åˆ°ï¼Œæ„å»ºå®Œæ•´çš„ transformer æ—¶ï¼Œæˆ‘ä»¬è¿˜æ˜¯ä¼šå¼•å…¥ä¸€äº›ä¸œè¥¿æ¥ä¿æŒè¾“å…¥çš„é¡ºåºä¿¡æ¯ï¼Œ ä½†è¦æ˜ç™½ **self-attention æœ¬èº«æ˜¯ä¸å…³å¿ƒè¾“å…¥çš„é¡ºåºå±æ€§çš„ï¼ˆsequential natureï¼‰**ã€‚

# 3. å®ç°ä¸€ä¸ªåŸºæœ¬çš„ self-attention
æ¥ä¸‹æ¥æˆ‘ä»¬åŸºäº pytorch å®ç°å‰é¢ä»‹ç»çš„æœ€åŸºç¡€ self-attention æ¨¡å‹ã€‚
æˆ‘ä»¬é¢ä¸´çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚ä½•ç”¨çŸ©é˜µä¹˜æ³•è¡¨ç¤º self-attentionï¼š æŒ‰ç…§å®šä¹‰ï¼Œç›´æ¥éå†æ‰€æœ‰ input vectors æ¥è®¡ç®— weight å’Œ output å°±è¡Œï¼Œ ä½†æ˜¾ç„¶è¿™ç§æ–¹å¼æ•ˆç‡å¤ªä½ï¼›æ”¹è¿›çš„æ–¹å¼å°±æ˜¯ç”¨ pytorch çš„ tensor æ¥è¡¨ç¤ºï¼Œ è¿™æ˜¯ä¸€ä¸ªå¤šç»´çŸ©é˜µæ•°æ®ç»“æ„ï¼š
> A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.
> pytorch.org/docs/stable/tensors.html
- è¾“å…¥ ğ— ç”± **t ä¸ª** ã€**k-ç»´** vector ç»„æˆçš„åºåˆ—ï¼Œ
- å¼•å…¥ä¸€ä¸ª **mini-batch dimension b**ï¼Œ
å°±å¾—åˆ°äº†ä¸€ä¸ªä¸‰ç»´çŸ©é˜µ (b,t,k)ï¼Œè¿™å°±æ˜¯ä¸€ä¸ª tensor

### 3.2 è®¡ç®—æƒé‡çŸ©é˜µï¼šè¾“å…¥çŸ©é˜µ * è½¬ç½®çŸ©é˜µ
æ¥ä¸‹æ¥è®¡ç®—åŠ æƒçŸ©é˜µï¼Œå®ƒè¡¨ç¤ºçš„æ˜¯ input vector ä¹‹é—´çš„ç›¸å…³æ€§ï¼Œ å› æ­¤ç”¨è¾“å…¥çŸ©é˜µ ğ— ä¹˜ä»¥å®ƒçš„è½¬ç½®çŸ©é˜µï¼ˆtransposeï¼‰ï¼Œç”¨ pytorch åº“æ¥è®¡ç®—éå¸¸æ–¹ä¾¿ã€‚
```
import torch
import torch.nn.functional as F

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€äº› tensor x ä½œä¸ºè¾“å…¥ï¼Œå®ƒæ˜¯ (b, t, k) ç»´çŸ©é˜µ
x = ...

# torch.bmm() æ˜¯æ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼ˆbatched matrix multiplicationï¼‰å‡½æ•°ï¼Œå¯¹ä¸€æ‰¹çŸ©é˜µæ‰§è¡Œä¹˜æ³•æ“ä½œ
raw_weights = torch.bmm(x, x.transpose(1, 2))
```
ç„¶åå¯¹æƒé‡çŸ©é˜µè¿›è¡Œæ­£å€¼åŒ–å’Œå½’ä¸€åŒ–ï¼Œä»¥ä½¿å¾—ä¸€ä¸ª row å†…æ‰€æœ‰æƒé‡åŠ èµ·æ¥ä¸º 1ï¼Œ
```
weights = F.softmax(raw_weights, dim=2)
```

3.3 è®¡ç®—è¾“å‡º
æœ‰äº†æƒé‡çŸ©é˜µï¼Œè®¡ç®—è¾“å‡ºå°±éå¸¸ç®€å•äº†ï¼šåªéœ€è¦å°†è¾“å…¥ ğ— å’Œæƒé‡çŸ©é˜µç›¸ä¹˜å³å¯ï¼Œä¸€è¡Œä»£ç æå®šï¼š
```
y = torch.bmm(weights, x)
```
**è¾“å‡ºçŸ©é˜µ ğ˜ å°±æ˜¯ size (b, t, k) çš„ tensorï¼Œæ¯ä¸€è¡Œéƒ½æ˜¯å¯¹ ğ— çš„è¡Œçš„åŠ æƒã€‚**
è¿™å°±æ˜¯ æœ€åŸºç¡€çš„ self-attention æ¨¡å‹çš„å®ç°ï¼š ä¸¤æ¬¡çŸ©é˜µä¹˜æ³•å’Œä¸€æ¬¡å½’ä¸€åŒ–ï¼ˆsoftmaxï¼‰ã€‚


# 4 ç°ä»£ transformer å¯¹ self-attention çš„æ‰©å±•

ç°ä»£ transformer ä¸­å®é™…ä½¿ç”¨çš„ self-attention ä¾èµ–äºä¸‰ä¸ªé¢å¤–æŠ€å·§ã€‚

###4.1 å¼•å…¥æ§åˆ¶å‚æ•°ï¼ˆfor queries, keys and valuesï¼‰

### 4.1.1 æ¯ä¸ª input vector éƒ½è¢«ä½¿ç”¨ä¸‰æ¬¡
ä¸Šä¸€èŠ‚å·²ç»çœ‹åˆ°ï¼Œæ¯ä¸ª input vector ğ±i åœ¨ self-attention è®¡ç®—ä¸­ä¼šè¢«ä½¿ç”¨ä¸‰æ¬¡ï¼Œ æ ¹æ®è§’è‰²çš„ä¸åŒè¿™ä¸‰æ¬¡åˆ†åˆ«ç§°ä¸º **queries**ã€**keys**ã€**values**ï¼ˆ**æŸ¥è¯¢ã€é”®å’Œå€¼**ï¼Œåé¢å†è§£é‡Šè¿™äº›åç§°çš„æ¥æºï¼‰ï¼Œ
- **query**ï¼šä¸å…¶ä»–æ‰€æœ‰ input vector **è”åˆè®¡ç®— i ä½ç½®çš„ output vector ğ²i æ‰€éœ€çš„æƒé‡**ï¼›
- **key**ï¼šä¸ query ç±»ä¼¼ï¼Œä¸å…¶ä»–æ‰€æœ‰ input vecto**r è”åˆè®¡ç®— j ä½ç½®çš„ output vector ğ²j æ‰€éœ€çš„æƒé‡**ï¼Œè¿™é‡Œ jâ‰ iï¼›
- **value**ï¼šåœ¨è®¡ç®—æ¯ä¸ª output vector æ—¶ï¼Œ**ä½œä¸ºè¾“å…¥å€¼å‚ä¸åŠ æƒæ±‚å’Œ**ã€‚
ä¹Ÿå°±æ˜¯è¯´åœ¨æˆ‘ä»¬ç›®å‰çš„åŸºæœ¬ self-attention ä¸­ï¼Œæ¯ä¸ª input vector å¿…é¡»æ‰¿æ‹…æ‰€æœ‰ä¸‰ä¸ªè§’è‰²ã€‚

### 4.1.2 å…·ä½“ä¾‹å­ï¼ˆè¯‘æ³¨ï¼‰
ä¸Šé¢çš„æè¿°æ¯”è¾ƒæŠ½è±¡ï¼Œè¿™é‡Œå‚è€ƒä¸‹å›¾æ›´ç›´è§‚è§£é‡Šä¸€ä¸‹ã€‚è¿™ä¸ªå›¾å¯¹åº” i=2ï¼Œå› æ­¤ ğ±2 ä¼šç”¨åˆ°ä¸‰æ¬¡ï¼ˆæ›´å‡†ç¡®åœ°è¯´æ˜¯ä¸‰ç§ç”¨é€”ï¼‰ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/self-attention.png)
å›¾ï¼šself-attention åŸºæœ¬è¿ç®—
- queryï¼šğ±2 ä¸ ğ±2 è”åˆè®¡ç®— ğ°22ï¼›
- keyï¼šğ±2 ä¸ ğ±j è”åˆè®¡ç®— ğ°2j æƒé‡ï¼Œè¿™é‡Œ jâ‰ 2ï¼›
- valueï¼šğ±2 ä½œä¸ºè¾“å…¥å€¼å‚ä¸åŠ æƒæ±‚å’Œã€‚

4.1.3 å¼•å…¥ä¸‰ä¸ª kÃ—k æƒé‡çŸ©é˜µ
å¯¹åŸå§‹ input vector åº”ç”¨çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å°±èƒ½å¤Ÿä¸ºæ¯ä¸ªè§’è‰²è¡ç”Ÿï¼ˆderiveï¼‰å‡ºä¸€ä¸ªæ–°å‘é‡ï¼Œä»è€Œç®€åŒ– self-attentionã€‚ å…·ä½“æ¥è¯´ï¼Œå¼•å…¥**ä¸‰ä¸ª kÃ—k** æƒé‡çŸ©é˜µ ğ–q, ğ–k, ğ–vï¼ˆæ¥è‡ª query/key/value é¦–å­—æ¯ï¼‰ **å¯¹æ¯ä¸ªè¾“å…¥ xi è®¡ç®—ä¸‰ä¸ªçº¿æ€§å˜æ¢**ï¼Œ
$q_{\mathrm{i}}=W_{\mathrm{q}}\mathbf{x}_{\mathrm{i}}$
$k_{\mathrm{i}}=W_{\mathrm{k}}\mathbf{x}_{\mathrm{i}}$
$v_{\mathrm{i}}=W_{\mathrm{v}}\mathbf{x}_{\mathrm{i}}$
é‚£ä¹ˆ (i,j) ä½ç½®å¤„çš„æƒé‡çŸ©é˜µå°±å¯ä»¥è¡¨ç¤ºä¸ºï¼š
$w_{\mathrm{ij}}^{\prime}=\mathbf{q}_{\mathrm{i}}^{\textrm{T}}\mathbf{k}_{\mathrm{j}}$
åšå½’ä¸€åŒ–å¤„ç†ï¼Œ
///
æœ€åï¼Œoutput vector ä¸­ä½ç½® j å¤„çš„å€¼ä¸ºï¼š
$\mathbf{y}_{\mathrm{i}}=\sum_{\mathrm{j}}w_{\mathrm{ij}}v_{\mathrm{j}}$
è¿™å°±ç»™ self-attention layer å¼•å…¥äº†å‡ ä¸ªå¯æ§åˆ¶çš„å‚æ•°ï¼ˆcontrollable parameters, ğ–q, ğ–k, ğ–vï¼‰ï¼Œ å¯¹åŒä¸€ä»½è¾“å…¥åº”ç”¨ä¸åŒçš„çº¿æ€§å˜æ¢ï¼Œå°±å¯ä»¥å¾—åˆ°ä¸åŒè§’è‰²æ‰€éœ€çš„å€¼ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/key-query-value.png)
self-attention key/query/value transformation çš„ç›´è§‚è§£é‡Š

## 4.2 å¯¹ç‚¹ç§¯åšç¼©æ”¾å¤„ç†ï¼ˆscaling the dot productï¼‰
**softmax å‡½æ•°å¯¹éå¸¸å¤§çš„è¾“å…¥å€¼æ•æ„Ÿã€‚è¿™äº› input ä¼šæ¢¯åº¦æ¶ˆå¤±ï¼Œå­¦ä¹ å˜æ…¢ç”šè‡³å®Œå…¨åœæ­¢ã€‚ ç”±äºç‚¹ç§¯çš„å¹³å‡å€¼éšç€åµŒå…¥ç»´åº¦ k çš„å¢åŠ è€Œå¢å¤§ï¼Œå› æ­¤ç‚¹ç§¯é€åˆ° softmax ä¹‹å‰è¿›è¡Œç¼©æ”¾æœ‰åŠ©äºç¼“è§£è¿™ä¸ªé—®é¢˜**ã€‚
åŸæ¥æ‰§è¡Œ softmax ä¹‹å‰çš„æƒé‡çŸ©é˜µï¼š
$w_{\mathrm{ij}}^{\prime}=\mathbf{q}_{\mathrm{i}}^{\textrm{T}}\mathbf{k}_{\mathrm{j}}$
ç°åœ¨ï¼š

 > Why sqrt(k) Imagine a vector in â„k with values all c. Its Euclidean length is  sqrt(k) *c. Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors

## 4.3 å¼•å…¥ multi-head attention
æœ€åï¼Œéœ€è¦è€ƒè™‘åˆ°ï¼ŒåŒä¸€ä¸ªå•è¯éšç€ç›¸é‚»å•è¯ä»¬çš„ä¸åŒè¡¨ç¤ºçš„æ„æ€ä¹Ÿå¯èƒ½ä¸åŒã€‚ä¾‹å¦‚ä¸‹é¢è¿™ä¸ªå¥å­ï¼š
                                       mary,gave,roses,to,susan
æˆ‘ä»¬çœ‹åˆ° â€œgaveâ€ è¿™ä¸ªè¯ä¸å¥å­çš„ä¸åŒéƒ¨åˆ†æœ‰ä¸åŒçš„å…³ç³»ï¼š
â€œmaryâ€ è¡¨ç¤ºè°åœ¨ â€œgaveâ€ï¼Œ
â€œrosesâ€ è¡¨ç¤º â€œgaveâ€ çš„æ˜¯ä»€ä¹ˆï¼Œ
â€œsusanâ€ è¡¨ç¤ºæ¥å—è€…æ˜¯è°ã€‚

### 4.3.1 éœ€æ±‚ï¼šè¾“å‡ºä¸­åµŒå…¥æ›´å¤šä¿¡æ¯
åœ¨æˆ‘ä»¬çš„åŸºæœ¬ self-attention ä¸­ï¼Œæ‰€æœ‰è¿™äº›ä¿¡æ¯æ˜¯æ··åˆåœ¨ä¸€èµ·çš„ï¼š è¾“å…¥ Xmary å’Œ Xsusan å¯ä»¥ä¸åŒç¨‹åº¦åœ°å½±å“è¾“å‡º Ygave ï¼Œè¿™å–å†³äºå®ƒä»¬ä¸ Ygave çš„ç‚¹ç§¯ã€‚
ä½†æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä»¥å…¶ä»–æ–¹å¼å½±å“è¾“å‡ºï¼Œè¿™ç§æ¨¡å‹å°±ä¸è¡Œäº†ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœ â€œrosesâ€ çš„ç»™äºˆæ–¹å’Œæ¥å—æ–¹ä¿¡æ¯éƒ½å‡ºç°åœ¨Ygave ï¼Œä½†ä½äºä¸åŒéƒ¨åˆ†ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼ŒåŸºæœ¬çš„ self-attention æ¬ ç¼ºäº†å¾ˆå¤šçµæ´»æ€§ã€‚
>This leaves aside how we figure out who gave the roses. We can do that based on prior knowledge about Mary and Susan, encoded in the embeddings. We can also look at the order of the words, but weâ€™ll look at how to achieve that later

### 4.3.2 è§£å†³æ–¹å¼ï¼šå¼•å…¥å¤šä¸ª self-attentionï¼ˆmulti-headï¼‰
è¦å®ç°è¿™ä¸ªç›®çš„ï¼Œå°±éœ€è¦è®©æˆ‘ä»¬çš„æ¨¡å‹æœ‰æ›´å¼ºçš„è¾¨è¯†åŠ›ï¼Œä¸€ç§åšæ³•å°±æ˜¯ç»„åˆå¤šä¸ª self-attentionï¼ˆç”¨ r ç´¢å¼•ï¼‰ï¼Œ æ¯ä¸ªå¯¹åº”ä¸åŒçš„ query/key/value å‚æ•°çŸ©é˜µ$W_{\mathrm{q}}^{\mathrm{r}}$,$W_{\mathrm{k}}^{\mathrm{r}}$,$W_{\mathrm{v}}^{\mathrm{r}}$ï¼Œ è¿™äº›å°±ç§°ä¸º attention headsï¼ˆæ³¨æ„åŠ›å¤´ï¼‰ã€‚
å¯¹äº input $x_{\mathrm{i}}$ï¼Œæ¯ä¸ª attention head äº§ç”Ÿä¸åŒçš„ output vector $y_{\mathrm{i}}^{\mathrm{r}}$ï¼ˆä¸€éƒ¨åˆ†è¾“å‡ºï¼‰ã€‚ æœ€åå†å°†è¿™äº›éƒ¨åˆ†è¾“å‡ºè¿æ¥èµ·æ¥ï¼Œé€šè¿‡çº¿æ€§å˜æ¢æ¥é™ç»´å› kã€‚

### 4.3.3 æå‡ multi-head self-attention æ•ˆç‡ï¼šquery/key/value é™ç»´
ç†è§£ multi-head self-attention æœ€ç®€å•çš„æ–¹æ³•æ˜¯æŠŠå®ƒçœ‹ä½œ**å¤šä¸ªå¹¶è¡Œçš„ self-attention æœºåˆ¶**ï¼Œ **æ¯ä¸ªéƒ½æœ‰è‡ªå·±çš„é”®ã€å€¼å’ŒæŸ¥è¯¢è½¬æ¢**ã€‚
Multi-head self-attention çš„ç¼ºç‚¹æ˜¯æ…¢ï¼Œå¯¹äº **R å¤´ï¼Œæ…¢ R å€**ã€‚ ä¸è¿‡æœ‰åŠæ³•ä¼˜åŒ–ï¼šæˆ‘ä»¬å¯ä»¥å®ç°è¿™æ ·çš„ multi-head self-attentionï¼Œå®ƒæ—¢èƒ½åˆ©ç”¨å¤šä¸ª self-attention æå‡è¾¨è¯†åŠ›ï¼Œ åˆä¸ single-head self-attention åŸºæœ¬ä¸€æ ·å¿«ã€‚è¦å®ç°è¿™ä¸ªç›®çš„ï¼Œ**æ¯ä¸ª head éœ€è¦å¯¹ query/key/value é™ç»´ã€‚ å¦‚æœè¾“å…¥å‘é‡æœ‰ k=256 ç»´ï¼Œæˆ‘ä»¬çš„æ¨¡å‹æœ‰ h=4 ä¸ª attention headï¼Œåˆ™é™ç»´æ“ä½œåŒ…æ‹¬ï¼š**
- å°†**è¾“å…¥å‘é‡ä¹˜ä»¥ä¸€ä¸ª 256Ã—64 çŸ©é˜µï¼Œè¿™ä¼šå°† input vector ä» 256 ç»´é™åˆ° 64 ç»´**ï¼›
- **å¯¹äºæ¯ä¸ª head éœ€è¦æ‰§è¡Œ 3 æ¬¡é™ç»´ï¼šåˆ†åˆ«é’ˆå¯¹ query/key/value çš„è®¡ç®—**ã€‚
æˆ‘ä»¬ç”šè‡³åªç”¨ä¸‰æ¬¡ kÃ—k çŸ©é˜µä¹˜æ³•å°±èƒ½å®ç° multi-head åŠŸèƒ½ï¼Œ å”¯ä¸€éœ€è¦çš„é¢å¤–æ“ä½œæ˜¯å°†ç”Ÿæˆçš„ output vector é‡æ–°æŒ‰å—æ’åºï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/kqv-computation.png)
å›¾ï¼šä¸ºäº†æœ‰æ•ˆåœ°è®¡ç®—å¤šå¤´æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬å°†æŠ•å½±çš„è®¡ç®—ç»“åˆåˆ°è¾ƒä½ç»´çš„è¡¨ç¤ºä¸­ï¼Œå¹¶å°†é”®ã€æŸ¥è¯¢å’Œå€¼çš„è®¡ç®—ç»“åˆåˆ°ä¸‰ä¸ªk * kçŸ©é˜µä¸­ã€‚

### 4.3.4 å®Œæ•´å·¥ä½œæµ
ä¸‹å›¾å±•ç¤ºäº†æ•´ä¸ª multi-head self-attention è¿‡ç¨‹ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/multi-head.png)
å›¾ï¼š4-head self-attention çš„ç›´è§‚è§£é‡Šã€‚å¯¹è¾“å…¥è¿›è¡Œé™ç»´ï¼Œé’ˆå¯¹ key/value/query åˆ†åˆ«è¿›è¡ŒçŸ©é˜µè¿ç®—æ¥å®ç°ã€‚
ä»å·¦åˆ°å³åˆ†ä¸º 5 åˆ—ï¼š

1. åŸå§‹ 256-ç»´ input vectorï¼›
2. è¾“å…¥é™ç»´ï¼šå°† input vector ä¹˜ä»¥ 256x64 çŸ©é˜µï¼Œé™ç»´åˆ° 64 ç»´ï¼›æ³¨æ„ï¼šå¯¹æ¯ä¸ª input vector éœ€è¦åˆ†åˆ«é’ˆå¯¹ query/key/value é™ç»´ï¼Œæ€»å…±æ˜¯ 3 éï¼›
3.  å°†é™ç»´åçš„ input åˆ†åˆ«è¾“å…¥å¤šä¸ªå¹¶è¡Œçš„ self-attentionï¼›
4. è®¡ç®—å¾—åˆ°å¤šä¸ªé™ç»´ä¹‹åçš„ output vectorï¼›
5. å¯¹ä½ç»´åº¦ output vectors è¿›è¡Œæ‹¼æ¥ï¼Œé‡æ–°å›åˆ°ä¸ input vectors ä¸€æ ·çš„ç»´åº¦ã€‚

## 4.5 multi-head vs. single-head æ¨¡å‹å‚æ•°æ•°é‡å¯¹æ¯”
å‚æ•°æŒ‡çš„æ˜¯åœ¨å°† input vector å˜æˆ output vector è¿‡ç¨‹ä¸­ç”¨åˆ°çš„é‚£äº›ç³»æ•°ï¼ˆæƒé‡çŸ©é˜µï¼‰ã€‚
æˆ‘ä»¬å‡è®¾è¾“å…¥çš„æ˜¯ k-ç»´ input vectorsï¼Œæ¥ä¸‹æ¥åˆ†åˆ«çœ‹ä¸‹ multi-head å’Œ single-head çš„å‚æ•°æ•°é‡ã€‚

**4.5.1 single-head**
æƒé‡çŸ©é˜µ $W_{\mathrm{ij}}$ï¼Œå…¶ä¸­ i,jâˆˆ[0,k]ï¼›
3 ä¸ªå¹³é¢ï¼šquery/key/valueï¼›
å› æ­¤æ€»å‚æ•°æ•°é‡æ˜¯ $3\mathbf{k}^{2}$

**4.5.2 multi-head**
å‡è®¾æœ‰ 4 ä¸ª headï¼Œå³ h=4ï¼Œ

æ¯ä¸ª head å¯¹åº”ä¸€ä¸ª self-attentionï¼Œæ¯ä¸ª self-attention 3 ä¸ªå¹³é¢ï¼ˆquery/key/valueï¼‰ï¼Œå› æ­¤æ€»å…± 3h ä¸ªå¹³é¢ï¼›
æ¯ä¸ªå¹³é¢çš„æƒé‡çŸ©é˜µ$W_{\mathrm{ij}}$ï¼Œå…¶ä¸­ iâˆˆ[0,k],jâˆˆ[0,k/h]ï¼›
å› æ­¤æ€»çš„å‚æ•°ä¸ªæ•°ï¼š$3{\mathrm{hk}}^{\mathrm{{k/h}}}=3{\mathrm{k}}^{2}$ï¼Œä¸ single-head self-attention çš„å‚æ•°æ•°é‡ç›¸åŒã€‚
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/multi-head%20(1).png)
>å”¯ä¸€çš„åŒºåˆ«æ˜¯ multi-head self-attention æœ€åæ‹¼æ¥ output vector æ—¶å¤šäº†ä¸€ä¸ªçŸ©é˜µ Woã€‚ä¸ single-head ç›¸æ¯”ï¼Œè¿™å¢åŠ äº† k2 ä¸ªå‚æ•°ã€‚ åœ¨å¤§å¤šæ•° Transformer ä¸­ï¼Œæ¯æ¬¡ self-attention ä¹‹åä¼šç´§è·Ÿç€ä¸€ä¸ªå‰é¦ˆå±‚ï¼ˆfeed-forward layerï¼‰ï¼Œå› æ­¤è¿™å¯èƒ½ä¸æ˜¯ç»å¯¹å¿…è¦çš„ã€‚ ä½†æˆ‘è¿˜æœªè§è¿‡èƒ½å¦æŠŠ Wo å»æ‰çš„ä¸¥è‚ƒè®¨è®ºã€‚

# 5 self-attention ä¸»è¦ä»£ç å®ç°
æ¥ä¸‹æ¥å°†æˆ‘ä»¬çš„ self-attention å®ç°ä¸ºä¸€ä¸ª python æ¨¡å—ï¼Œæ–¹ä¾¿å¤ç”¨ï¼š
```
import torch
from torch import nn
import torch.nn.functional as F

class SelfAttention(nn.Module):
    def __init__(self, k, heads=4, mask=False):
    super().__init__()
			# è¾“å…¥å‘é‡æœ‰ kç»´
    assert k % heads == 0 # input vector size å¿…é¡»æ˜¯ heads çš„æ•´æ•°å€
    self.k, self.heads = k, heads
```
ç„¶åï¼Œ**åˆå§‹åŒ–å‡ ä¸ª k*k çš„çº¿æ€§å˜æ¢çŸ©é˜µ**ï¼Œ nn.Linear(bias=False) èƒ½å®ç°è¿™ä¸ªæ•ˆæœï¼Œå¹¶åšäº†é€‚å½“çš„åˆå§‹åŒ–ï¼š
```
# Compute the queries, keys and values for all heads
# y = Wq*x+y(å…¶ä¸­Wq=(k,k)) 
self.tokeys    = nn.Linear(k, k, bias=False)
self.toqueries = nn.Linear(k, k, bias=False)
self.tovalues  = nn.Linear(k, k, bias=False)

# This will be applied after the multi-head self-attention operation.
self.unifyheads = nn.Linear(k, k)
```
æ¥ä¸‹æ¥å°±å¯ä»¥å®ç°äº† self-attention çš„è®¡ç®—äº†ï¼Œåœ¨æ¨¡å‹ä¸­å¯¹åº”çš„æ˜¯ forward() å‡½æ•°ã€‚
```
def forward(self, x):
    # b:batch size, t:sequence length, k:embeding feature dimension
    b, t, k = x.size()
    h = self.heads

    # é¦–å…ˆï¼Œä¸ºæ‰€æœ‰ heads è®¡ç®— query/key/valueï¼Œå¾—åˆ°çš„æ˜¯å®Œæ•´åµŒå…¥ç»´åº¦çš„ k*k çŸ©é˜µ
    #(b,t,k)=(b,t,k)*(k,k)-->(b, t, k)
		 queries = self.toqueries(x) # k*k
    keys    = self.tokeys(x)  # k*k
    values  = self.tovalues(x) # k*k

    # æ¥ä¸‹æ¥å°† queries/keys/values åˆ‡å—ï¼ˆé™ç»´ï¼‰ï¼Œåˆ†åˆ«é€åˆ°ä¸åŒçš„ head
    s = k // h  #s:é™ç»´ç»´åº¦
	   # (b, t, k)-->(b, t, h, s)
    keys    = keys.view(b, t, h, s)
    queries = queries.view(b, t, h, s)
    values  = values.view(b, t, h, s)
```
è¿™å¯¹ tensors è¿›è¡Œäº†**ç®€å• reshapeï¼Œç°åœ¨ tensors å¢åŠ äº†ä¸€ä¸ª head ç»´åº¦**ã€‚ å¯¹äºæ¯ä¸ª input vectorï¼Œå¯ä»¥ç†è§£ä¸ºå°†è¿™ä¸ª **k*1 çŸ©é˜µå˜æˆäº†ä¸€ä¸ª h * k//h çŸ©é˜µ**ï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/reshape.png)
æ¥ä¸‹æ¥è®¡ç®—ç‚¹ç§¯ã€‚æ¯ä¸ª head çš„ç‚¹ç§¯è¿ç®—éƒ½æ˜¯ä¸€æ ·çš„ï¼Œå› ä¸ºæˆ‘ä»¬**å°† heads fold åˆ° batch dimentionã€‚ è¿™æ ·æˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨ torch.bmm()ï¼ˆbatch matrix multiplificationï¼‰ï¼Œè€Œ keys, queries and values å¯ä»¥çœ‹åšæ˜¯ batch**ï¼Œåªæ˜¯ batch size ç¨å¤§äº†ä¸€ç‚¹ã€‚
ç”±äº head å’Œ batch dimension æ²¡æœ‰ç›¸é‚»ï¼Œå› æ­¤æˆ‘ä»¬åœ¨ reshape ä¹‹å‰éœ€è¦è½¬ç½®ã€‚ è¿™ä¸ªæ“ä½œå¼€é”€å¾ˆå¤§ï¼Œä½†ä¼¼ä¹æ— æ³•é¿å…ï¼š
```
# - fold heads into the batch dimension
# (b*h, t, s)=(b, t, h, s)-->(b, h, t, s)-->(b*h, t, s)
keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)
queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)
values = values.transpose(1, 2).contiguous().view(b * h, t, s)
```
>æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨reshape() è€Œä¸æ˜¯viewï¼ˆï¼‰æ¥é¿å…å¯¹continuousï¼ˆï¼‰çš„è°ƒç”¨ï¼Œä½†å½“æˆ‘ä»¬å¤åˆ¶å¼ é‡å’Œåªæ˜¯æŸ¥çœ‹å®ƒæ—¶ï¼Œæˆ‘æ›´å–œæ¬¢å°†å…¶æ˜ç¡®åŒ–ã€‚è¯·å‚é˜…æ­¤ç¬”è®°æœ¬ä»¥äº†è§£å·®å¼‚çš„è§£é‡Šã€‚ 

è·Ÿä¹‹å‰ä¸€æ ·ï¼Œç‚¹ç§¯å¯ä»¥ç”¨å•ä¸ªçŸ©é˜µä¹˜æ³•å®ç°ï¼Œä½†ç°åœ¨æ˜¯ queries ä¹˜ä»¥ keysï¼Œ
```
# Get dot product of queries and keys, and scale
# (b*h, t, t) =(b * h, t, s)*(b * h, s, t)-->(b*h, t, t) 
dot = torch.bmm(queries, keys.transpose(1, 2)) # -- dot has size (b*h, t, t) containing raw weights
dot = dot / (k ** (1/2))                # scale the dot product
# (b*h, t, t)
dot = F.softmax(dot, dim=2)    # normalize, dot now contains row-wise normalized weights
```
ç„¶åç”¨å¾—åˆ°çš„æƒé‡å†å’Œ values åšç‚¹ç§¯ï¼Œå¾—åˆ°çš„å°±æ˜¯æ¯ä¸ª attention head çš„è¾“å‡ºï¼š
 ```
 # (b, h, t, s)=(b*h, t, t)*(b*h, t, s)*-->(b, h, t, s) 
 out = torch.bmm(dot, values).view(b, h, t, s) # apply the self attention to the values
 ```
 ä¸ºäº†å°†æ¯ä¸ª head çš„è¾“å‡ºé‡æ–°ä¸²è”èµ·æ¥å¾—åˆ° k-ç»´çš„æœ€ç»ˆè¾“å‡ºï¼Œæˆ‘ä»¬éœ€è¦å†æ¬¡è½¬ç½®ï¼Œç„¶åå°†è½¬ç½®åçš„çŸ©é˜µé€åˆ° unifyheads layer åšæœ€å¥½çš„ç»´åº¦å˜æ¢ï¼š
  ```
# swap h, t back, unify heads
# (b, t, s*h) =(b, h, t, s)-->(b, t, h, s) -->(b, t, s*h)
out = out.transpose(1, 2).contiguous().view(b, t, s * h)
# (b, t, s * h)
return self.unifyheads(out)
```
è‡³æ­¤ï¼Œä¸€ä¸ª multi-head, scaled dot-product self attention æ¨¡å‹å°±å®ç°å¥½äº†ã€‚
>The implementation can be made more concise using einsum notation (see an example here).

# 6 åŸºäº multi-head self-attention å®ç° transformers
## 6.1 Transformer å®šä¹‰
transformer ä¸ä»…ä»…æ˜¯ä¸€ä¸ª self-attention layerï¼Œè¿˜æ˜¯ä¸€ç§æ¶æ„ï¼ˆarchitectureï¼‰ã€‚ å¦‚ä½•ç²¾ç¡®åœ°åˆ¤æ–­ä¸€ä¸ªä¸œè¥¿æ˜¯æˆ–è€…ä¸æ˜¯ transformer è¿˜ä¸æ˜¯å¾ˆæ˜ç¡®ï¼Œæœ¬æ–‡é‡‡ç”¨å¦‚ä¸‹çš„å®šä¹‰ï¼š
>ä»»ä½•è®¾è®¡ç”¨æ¥å¤„ç†ä¸€ç»„è¿æ¥çš„å•å…ƒï¼ˆä¾‹å¦‚åºåˆ—ä¸­çš„ token æˆ–å›¾åƒä¸­çš„åƒç´ ï¼‰ï¼Œ å¦‚æœå•å…ƒä¹‹é—´çš„å”¯ä¸€äº¤äº’æ–¹å¼æ˜¯ self-attentionï¼Œé‚£è¿™æ ·çš„æ¶æ„å°±ç§°ä¸º transformerã€‚

ä¸å…¶ä»–æœºåˆ¶ï¼ˆå¦‚å·ç§¯ï¼‰ä¸€æ ·ï¼Œå¯ä»¥åŸºäº self-attention å±‚æ„å»ºæˆæ›´å¤§çš„ç½‘ç»œã€‚ä½†åœ¨æ­¤ä¹‹å‰ï¼Œ æˆ‘ä»¬éœ€è¦å°† self-attention é‡æ„ä¸ºä¸€ä¸ªå¯ä»¥å¤ç”¨çš„ blockã€‚

## 6.2 Transformer block
æ„å»ºåŸºæœ¬çš„ transformer æœ‰å‡ ç§ç•¥å¾®ä¸åŒçš„æ–¹å¼ï¼Œä½†å¤§å¤šæ•°ç»“æ„éƒ½å¤§è‡´å¦‚ä¸‹ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/transformer-block.png)
å„å—ä¾æ¬¡æ‰§è¡Œï¼š

1. self-attention å±‚ï¼›
2. å½’ä¸€åŒ–å±‚ï¼›
3. å‰é¦ˆå±‚ï¼ˆfeed forward layerï¼‰ï¼Œæ¯ä¸ª MLPï¼ˆmulti-layer perceptronï¼‰åˆ†åˆ«ä¸æ¯ä¸ª input åšè¿ç®—ï¼›
4. å¦ä¸€ä¸ªå±‚å½’ä¸€åŒ–ã€‚
ä¸¤æ¬¡å½’ä¸€åŒ–ä¹‹å‰éƒ½ä¼šæ·»åŠ æ®‹å·®è¿æ¥ï¼ˆresidual connectionsï¼‰ã€‚

å„ç»„ä»¶çš„é¡ºåºå¹¶ä¸æ˜¯åªèƒ½è¿™æ ·ï¼Œé‡è¦çš„æ˜¯
1. å°† self-attention ä¸å±€éƒ¨å‰é¦ˆç›¸ç»“åˆï¼ˆcombine self-attention with a local feedforwardï¼‰ï¼Œ
2. æ·»åŠ å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥ã€‚
å½’ä¸€åŒ–å’Œæ®‹å·®è¿æ¥æ˜¯å¸¸è§„æŠ€å·§ï¼Œç”¨äºä½¿æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ›´å¿«ã€æ›´å‡†ç¡®ã€‚ å±‚å½’ä¸€åŒ–ä»…åº”ç”¨äºåµŒå…¥ç»´åº¦ï¼ˆlayer normalization is applied over the embedding dimension onlyï¼‰ã€‚
å®ç°ï¼š
  ```
class TransformerBlock(nn.Module):
  def __init__(self, k, heads):
    super().__init__()

    self.attention = SelfAttention(k, heads=heads)

    self.norm1 = nn.LayerNorm(k)
    self.norm2 = nn.LayerNorm(k)

    self.ff = nn.Sequential(
      nn.Linear(k, 4 * k),
      nn.ReLU(),
      nn.Linear(4 * k, k))

  def forward(self, x):
    attended = self.attention(x)
    x = self.norm1(attended + x)

    fedforward = self.ff(x)
    return self.norm2(fedforward + x)
```
è¿™é‡Œæˆ‘ä»¬é€‰æ‹©äº†è®© feed forward éšè—å±‚æ¯” input/output å¤§ 4 å€ï¼Œè¿™ä¸ªå€æ•°çš„é€‰æ‹©æ˜¯éšæ„çš„ï¼Œ æ›´å°çš„å€æ•°å¯èƒ½ä¹Ÿèƒ½å·¥ä½œï¼Œå¹¶ä¸”å ç”¨å†…å­˜æ›´å°‘ï¼Œä½†æœ€å°ä¸èƒ½å°äº input/output layer å¤§å°ã€‚

## 6.3 æ–‡æœ¬åˆ†ç±»ï¼ˆtext classificationï¼‰transformer
æˆ‘ä»¬èƒ½æ„å»ºçš„æœ€ç®€å• transformer å« **sequence classifier**ï¼ˆé¡ºåºåˆ†ç±»å™¨ï¼‰ã€‚ æˆ‘ä»¬ç”¨ IMDbï¼ˆInternet Movie Databaseï¼‰sentiment classification æ•°æ®é›†ï¼š
- æ•°æ®å†…å®¹æ˜¯å½±è¯„ï¼Œ
- token åŒ–æˆäº†å•è¯åºåˆ—ï¼Œ
- åˆ†ç±»æ ‡ç­¾æ˜¯ positive å’Œ negativeï¼ˆå¯¹ç”µå½±çš„æ­£é¢/è´Ÿé¢è¯„ä»·ï¼‰

**æ¶æ„çš„æ ¸å¿ƒéƒ¨åˆ†éå¸¸ç®€å•ï¼Œå°±æ˜¯ä¸€é•¿ä¸² transformer blockã€‚æ‰€éœ€åšçš„äº‹æƒ…ï¼š**
- å¦‚ä½•å°† input sequence feed ç»™è¿™ä¸ªé•¿é“¾ï¼Œ
- å¦‚ä½•å¯¹æœ€ç»ˆ output sequence è¿›è¡Œå˜æ¢ï¼Œå¾—åˆ°å•ä¸ªåˆ†ç±»ç»“æœã€‚

### 6.3.1 è¾“å‡ºï¼šå•ä¸ªåˆ†ç±»ç»“æœ
ä» sequence-to-sequence layers æ„å»º sequence classifier çš„æœ€å¸¸è§æ–¹æ³•æ˜¯**å¯¹æœ€ç»ˆè¾“å‡ºåºåˆ—åš global average pooling**ï¼Œ**å¹¶å°†ç»“æœæ˜ å°„åˆ° softmaxed class vector**ã€‚
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/classifier.png)
å›¾ï¼šç®€å•çš„åºåˆ—åˆ†ç±»Transformeræ¦‚è¿°ã€‚å¯¹è¾“å‡ºåºåˆ—è¿›è¡Œå¹³å‡ï¼Œä»¥äº§ç”Ÿä»£è¡¨æ•´ä¸ªåºåˆ—çš„å•ä¸ªè½½ä½“ã€‚è¯¥è½½ä½“è¢«æŠ•å½±åˆ°æ¯ä¸ªç±»å…·æœ‰ä¸€ä¸ªå…ƒç´ çš„è½½ä½“ï¼Œå¹¶è¿›è¡Œè½¯æ”¾å¤§ä»¥äº§ç”Ÿæ¦‚ç‡ã€‚

### 6.3.2 è¾“å…¥ï¼šè¯åºæ•æ„Ÿï¼ˆusing the positionsï¼‰
å‰é¢å·²ç»è®¨è®ºäº†åµŒå…¥å±‚çš„åŸç†ï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬å°†ç”¨å®ƒæ¥è¡¨ç¤ºå•è¯ã€‚
æ­£å¦‚å‰é¢å·²ç»æåˆ°çš„ï¼Œ**æˆ‘ä»¬æ­£åœ¨å †å ï¼ˆstackingï¼‰æ’åˆ—ç­‰å˜å±‚ï¼ˆpermutation equivariant layersï¼‰ï¼Œ æœ€ç»ˆçš„ global average pooling æ˜¯æ’åˆ—ä¸å˜çš„ï¼ˆpermutation invariantï¼‰ï¼Œ å› æ­¤æ•´ä¸ªç½‘ç»œä¹Ÿæ˜¯æ’åˆ—ä¸å˜çš„**ã€‚ç”¨ç™½è¯æ¥è¯´ï¼Œ å³ä½¿æˆ‘ä»¬æ‰“ä¹±å¥å­ä¸­çš„å•è¯é¡ºåºï¼Œæ— è®ºæˆ‘ä»¬å­¦åˆ°ä»€ä¹ˆæƒé‡ï¼Œéƒ½ä¼šå¾—åˆ°å®Œå…¨ç›¸åŒçš„åˆ†ç±»ç»“æœã€‚ æ˜¾ç„¶ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™ä¸ª**å…ˆè¿›çš„è¯­è¨€æ¨¡å‹è‡³å°‘å¯¹è¯åºå…·æœ‰ä¸€å®šçš„æ•æ„Ÿæ€§**ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

è§£å†³æ–¹æ¡ˆå¾ˆç®€å•ï¼š**åˆ›å»ºä¸€ä¸ªä¸ input ç­‰é•¿çš„å‘é‡è®°å½•å½“å‰å¥å­ä¸­å•è¯çš„ä½ç½®ï¼Œå¹¶å°†å…¶æ·»åŠ åˆ° word embedding ä¸­**ã€‚ å…·ä½“åˆ°å®ç°ä¸Šï¼Œæœ‰ä¸¤ç§é€‰æ‹©ï¼š

**ä½ç½®åµŒå…¥ï¼ˆposition embeddingsï¼‰**
åƒåµŒå…¥æ–‡å­—ä¸€æ ·åµŒå…¥ä½ç½®ã€‚å°±åƒåˆ›å»ºåµŒå…¥å‘é‡**V**cat å’Œ **V**susan ä¸€æ ·ï¼Œ æˆ‘ä»¬åˆ›å»ºåµŒå…¥å‘é‡ğ¯12 å’Œ ğ¯25ã€‚
**ç¼ºç‚¹æ˜¯åœ¨è®­ç»ƒæœŸé—´å¿…é¡»çœ‹åˆ°æ¯ä¸ªä¸åŒé•¿åº¦çš„åºåˆ—ï¼Œå¦åˆ™ç›¸å…³çš„ä½ç½®åµŒå…¥å¾—ä¸åˆ°è®­ç»ƒ**ã€‚ ä¼˜ç‚¹æ˜¯æ•ˆæœè¿˜ä¸é”™ï¼Œè€Œä¸”å¾ˆå®¹æ˜“å®ç°ã€‚

**ä½ç½®ç¼–ç ï¼ˆposition encodingsï¼‰**
**ä½ç½®ç¼–ç ä¸ä½ç½®åµŒå…¥çš„å·¥ä½œæ–¹å¼ç±»ä¼¼ï¼Œä½†ä¸å­¦ä¹ ä½ç½®å‘é‡ï¼Œè€Œåªæ˜¯é€‰æ‹©ä¸€äº›å‡½æ•°**
f: N-->$R^{\mathrm{k}}$
å°†ä½ç½®æ˜ å°„åˆ°å®å€¼å‘é‡ï¼Œå¹¶è®©ç½‘ç»œå¼„æ¸…æ¥šå¦‚ä½•è§£é‡Šè¿™äº›ç¼–ç ã€‚
å¥½å¤„æ˜¯ï¼Œ**å¯¹äºç²¾å¿ƒé€‰æ‹©çš„å‡½æ•°ï¼Œç½‘ç»œèƒ½å¤Ÿå¤„ç†æ¯”è®­ç»ƒæœŸé—´çœ‹åˆ°çš„åºåˆ—æ›´é•¿çš„åºåˆ—**ï¼ˆåœ¨å®ƒä»¬ä¸Šè¡¨ç°åº”è¯¥ä¸ä¼šå¤ªå¥½ï¼Œä½†è‡³å°‘æˆ‘ä»¬å¯ä»¥ checkï¼‰ã€‚ **ç¼ºç‚¹æ˜¯ç¼–ç å‡½æ•°çš„é€‰æ‹©æ˜¯ä¸€ä¸ªå¤æ‚çš„è¶…å‚æ•°ï¼ˆa complicated hyperparameterï¼‰ï¼Œå®ç°èµ·æ¥æœ‰ç‚¹å¤æ‚ã€‚**

### 6.3.3 åŸºäº Pytorch å®ç°
ç®€å•èµ·è§ï¼Œæœ¬æ–‡ä½¿ç”¨**ä½ç½®åµŒå…¥ï¼ˆposition embeddingsï¼‰**æ¥è®°å½• input é¡ºåºã€‚
ä»¥ä¸‹å°±æ˜¯æˆ‘ä»¬çš„ text classification transformer çš„å®Œæ•´å®ç°ï¼š
```
class Transformer(nn.Module):
    def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):
        super().__init__()

        self.num_tokens = num_tokens
        self.token_emb = nn.Embedding(num_tokens, k)
        self.pos_emb = nn.Embedding(seq_length, k)

        # The sequence of transformer blocks that does all the heavy lifting
        tblocks = []
        for i in range(depth):
            tblocks.append(TransformerBlock(k=k, heads=heads))
        self.tblocks = nn.Sequential(*tblocks)

        # Maps the final output sequence to class logits
        self.toprobs = nn.Linear(k, num_classes)

    def forward(self, x):
        """
        :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary).
        :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes).
        """
        # generate token embeddings
        #(b, t, k)=(b,t, k)*(?)-->(b, t, k)
					 tokens = self.token_emb(x)
        b, t, k = tokens.size()

        # generate position embeddings
        positions = torch.arange(t)
        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)

        x = tokens + positions # ä¸ºä»€ä¹ˆæ–‡æœ¬åµŒå…¥å’Œä½ç½®åµŒå…¥ç›¸åŠ ï¼Œæ²¡æœ‰ç†è®ºï¼Œå¯èƒ½å°±æ˜¯å®éªŒä¸‹æ¥æ•ˆæœä¸é”™ã€‚
                               # https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/
        x = self.tblocks(x)

        # Average-pool over the t dimension and project to class
        # probabilities
        x = self.toprobs(x.mean(dim=1))
        return F.log_softmax(x, dim=1)
```
åœ¨æ·±åº¦ä¸º 6 ï¼Œæœ€å¤§åºåˆ—é•¿åº¦ä¸º 512 æ—¶ï¼Œè¿™ä¸ª transformer å–å¾—äº† 85% çš„å‡†ç¡®åº¦ï¼Œä¸ RNNï¼ˆå¾ªç¯ç¥ç»ç½‘ç»œï¼‰æ¨¡å‹çš„ç»“æœç›¸å½“ï¼Œä½†è®­ç»ƒé€Ÿåº¦å¿«å¾—å¤šã€‚ è¦çœ‹åˆ°è¿™ä¸ª transformer çœŸæ­£æ¥è¿‘äººç±»çš„æ€§èƒ½ï¼Œå°±éœ€è¦åœ¨æ›´å¤šæ•°æ®ä¸Šè®­ç»ƒæ›´æ·±çš„æ¨¡å‹ã€‚åæ–‡å°†è¯¦ç»†ä»‹ç»æ€ä¹ˆåšã€‚
### 6.4 æ–‡æœ¬ç”Ÿæˆï¼ˆtext generationï¼‰transformer
æ¥ä¸‹æ¥å°è¯•ä¸€ä¸‹è‡ªå›å½’æ¨¡å‹ï¼ˆautoregressive modelï¼‰ï¼š è®­ç»ƒä¸€ä¸ªå­—ç¬¦çº§åˆ«ï¼ˆcharacter levelï¼‰çš„ transformer æ¥é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚
### 6.4.1 è‡ªå›å½’æ¨¡å‹å’Œæ©ç 
è®­ç»ƒæ–¹å¼å¾ˆç®€å•ï¼ˆå¹¶ä¸”åœ¨ transformer å‡ºç°ä¹‹å‰å°±å·²ç»å­˜åœ¨å¾ˆä¹…äº†ï¼‰ã€‚ æˆ‘ä»¬ç»™ sequence-to-sequence æ¨¡å‹ä¸€ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥ï¼Œç„¶åè¦æ±‚å®ƒé¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªä½ç½®çš„å­—ç¬¦ã€‚ æ¢å¥è¯è¯´ï¼Œç›®æ ‡è¾“å‡ºæ˜¯å‘å·¦ç§»åŠ¨ä¸€ä¸ªå­—ç¬¦çš„ç›¸åŒåºåˆ—ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/generator.png)
- å¦‚æœæ˜¯ RNN æ¨¡å‹ï¼Œé‚£è¿™å°±æ˜¯æˆ‘ä»¬æ‰€éœ€åšçš„æ‰€æœ‰äº‹æƒ…ï¼Œ å› ä¸ºå®ƒä¸èƒ½å¾€å‰çœ‹ï¼Œoutput i åªä¾èµ– inputs 0 ~ iã€‚
- è€Œå¯¹äº transformerï¼Œoutput å–å†³äºæ•´ä¸ª input sequenceï¼Œ å› æ­¤é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯å°±ç®€å•å¤šäº†ï¼Œåªéœ€ä» input ä¸­æŒ‘é€‰ã€‚
è¦å°† self-attention ç”¨ä½œè‡ªå›å½’æ¨¡å‹ï¼Œéœ€è¦ç¡®ä¿å®ƒä¸èƒ½ look forward input åºåˆ—ã€‚ åœ¨ softmax ä¹‹å‰å¯¹ç‚¹ç§¯çŸ©é˜µåº”ç”¨ä¸€ä¸ªæ©ç ï¼Œç¦ç”¨çŸ©é˜µå¯¹è§’çº¿ä¹‹ä¸Šçš„æ‰€æœ‰å…ƒç´ ï¼Œ å°±èƒ½å¸®æˆ‘ä»¬å®ç°è¿™ä¸€ç›®çš„ã€‚
å¯¹ self-attention è¿›è¡Œ masking æ“ä½œï¼Œç¡®ä¿ input sequence ä¸­åªæœ‰å½“å‰ä½ç½®ä¹‹å‰çš„ input elements èƒ½å‚ä¸è®¡ç®—ã€‚ æ³¨æ„å›¾ä¸­çš„ä¹˜æ³•ç¬¦å·å…¶å®æœ‰ä¸€ç‚¹ç‚¹è¯¯å¯¼æ€§ï¼šæˆ‘ä»¬å®é™…ä¸Šæ˜¯å°†å³ä¸Šè§’çš„å…ƒç´ è®¾ç½®ä¸ºè´Ÿæ— ç©·å¤§ âˆ’âˆ
ç”±äºæˆ‘ä»¬å¸Œæœ›è¿™äº›å…ƒç´ åœ¨ softmax ä¹‹åå…¨æ˜¯ 0ï¼Œå› æ­¤å°†å®ƒä»¬è®¾ç½®ä¸º âˆ’âˆã€‚ç›¸åº”çš„ä»£ç ï¼š
```
dot = torch.bmm(queries, keys.transpose(1, 2))

indices = torch.triu_indices(t, t, offset=1)
dot[:, indices[0], indices[1]] = float('-inf')

dot = F.softmax(dot, dim=2)
```
è¿™æ ·ä¿®æ”¹ self-attention æ¨¡å—ä¹‹åï¼Œæ¨¡å‹å°±ä¸èƒ½å† look forward input sequence äº†ã€‚
### 6.4.2 è®­ç»ƒï¼šåŸºäºç»´åŸºç™¾ç§‘æ•°æ®é›† enwik8
æˆ‘ä»¬åœ¨æ ‡å‡† enwik8 æ•°æ®é›†ï¼ˆå–è‡ª Hutter prizeï¼‰ ä¸Šè¿›è¡Œè®­ç»ƒï¼Œè¯¥æ•°æ®é›†åŒ…å« 108 ä¸ªç»´åŸºç™¾ç§‘æ–‡æœ¬ä¸­çš„å­—ç¬¦ã€‚åœ¨è®­ç»ƒæœŸé—´ï¼Œé€šè¿‡ä»æ•°æ®ä¸­éšæœºæŠ½å–å­åºåˆ—æ¥ç”Ÿæˆæ‰¹æ¬¡ã€‚

æˆ‘ä»¬ä½¿ç”¨ç”± 12 ä¸ª transformer block å’Œ 256 ä¸ªåµŒå…¥ç»´åº¦ç»„æˆçš„ transformerï¼Œå¯¹é•¿åº¦ä¸º 256 çš„åºåˆ—è¿›è¡Œè®­ç»ƒã€‚ åœ¨ RTX 2080Tiï¼ˆå¤§çº¦ 170K ä¸ªå¤§å°ä¸º 32 çš„æ‰¹æ¬¡ï¼‰ä¸Šè®­ç»ƒäº†å¤§çº¦ 24 å°æ—¶åï¼Œ è®©æ¨¡å‹ä» 256 ä¸ªå­—ç¬¦çš„ç§å­å¼€å§‹ç”Ÿæˆï¼šå¯¹äºæ¯ä¸ªå­—ç¬¦ï¼Œè¾“å…¥å®ƒå‰é¢çš„ 256 ä¸ªå­—ç¬¦ï¼Œ ç„¶åé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚ æˆ‘ä»¬ä»temperature ä¸º 0.5 çš„é‚£ä¸ªå¼€å§‹é‡‡æ ·ï¼Œç„¶åç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

è¾“å‡ºå¦‚ä¸‹æ‰€ç¤ºï¼š
1228X Human & Rousseau. Because many of his stories were originally published in long-forgotten magazines and journals, there are a number of [[anthology|anthologies]] by different collators each containing a different selection. His original books have been considered an anthologie in the [[Middle Ages]], and were likely to be one of the most common in the [[Indian Ocean]] in the [[1st century]]. As a result of his death, the Bible was recognised as a counter-attack by the [[Gospel of Matthew]] (1177-1133), and the [[Saxony|Saxons]] of the [[Isle of Matthew]] (1100-1138), the third was a topic of the [[Saxony|Saxon]] throne, and the [[Roman Empire|Roman]] troops of [[Antiochia]] (1145-1148). The [[Roman Empire|Romans]] resigned in [[1148]] and [[1148]] began to collapse. The [[Saxony|Saxons]] of the [[Battle of Valasander]] reported the y

### 6.4.3 æ–‡æœ¬ç”Ÿæˆç»“æœåˆ†æ
å¯¹äºä¸Šé¢çš„è¾“å‡ºï¼Œåº”è¯¥æ³¨æ„åˆ°ï¼Œ

1. è¾“å‡ºçš„æ–‡æœ¬ä¸­æ­£ç¡®ä½¿ç”¨äº†ç»´åŸºç™¾ç§‘é“¾æ¥æ ‡ç­¾è¯­æ³•ï¼Œé“¾æ¥å†…çš„æ–‡æœ¬å‡†ç¡®è¡¨è¾¾äº†é“¾æ¥ä¸»é¢˜ã€‚
2. ç”Ÿæˆçš„å†…å®¹ä¹Ÿä¸ä¸»é¢˜å¤§è‡´ä¸€è‡´ï¼šç”Ÿæˆçš„æ–‡æœ¬ä»¥åœ£ç»å’Œç½—é©¬å¸å›½ä¸ºä¸»é¢˜ï¼Œåœ¨ä¸åŒçš„åœ°æ–¹ä½¿ç”¨ä¸åŒçš„ç›¸å…³æœ¯è¯­ã€‚
3. è¿˜æœ‰ä¸€ä¸ªä¸é‚£ä¹ˆæ˜æ˜¾çš„åœ°æ–¹ï¼šâ€œBattle of Valasanderâ€ï¼Œè¿™åœºâ€œæˆ˜äº‰â€ä¼¼ä¹æ˜¯è¿™ä¸ªç¥ç»ç½‘ç»œè‡ªå·±æœæ’°çš„ã€‚
è¿™è™½ç„¶ä¸ GPT-2 ç­‰æ¨¡å‹çš„æ€§èƒ½ç›¸å»ç”šè¿œï¼Œä½†ä¸ RNN ç­‰æ¨¡å‹ç›¸æ¯”ä¼˜åŠ¿å·²ç»å¾ˆæ˜æ˜¾ï¼šæ›´å¿«çš„è®­ç»ƒé€Ÿåº¦ï¼ˆç±»ä¼¼çš„ RNN æ¨¡å‹éœ€è¦å¾ˆå¤šå¤©æ¥è®­ç»ƒï¼‰å’Œæ›´å¥½çš„é•¿æœŸä¸€è‡´æ€§ã€‚

å¦å¤–ï¼Œè¯¥æ¨¡å‹åœ¨éªŒè¯é›†ä¸Šå®ç°äº† 1.343bit/byte çš„å‹ç¼©ï¼Œ è¿™ä¸ GPT-2 æ¨¡å‹ï¼ˆä¸‹æ–‡ä¼šå±•å¼€ä»‹ç»ï¼‰å®ç°çš„æ¯å­—èŠ‚ 0.93 ä½çš„ç›¸å·®ä¸è¿œã€‚

## 6.5 è®¾è®¡è€ƒè™‘ï¼šTransformer ä¸ RNN/å·ç§¯ å¯¹æ¯”
åœ¨ transformer ä¹‹å‰ï¼Œæœ€å…ˆè¿›çš„æ¶æ„æ˜¯ RNNï¼ˆé€šå¸¸æ˜¯ LSTM æˆ– GRUï¼‰ï¼Œä½†å®ƒä»¬å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚

RNN å±•å¼€ï¼ˆunrolledï¼‰åé•¿è¿™æ ·ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/recurrent-connection.png)
RNN æœ€å¤§çš„é—®é¢˜æ˜¯çº§è”ï¼ˆrecurrent connectionï¼‰ï¼š è™½ç„¶å®ƒä½¿å¾—ä¿¡æ¯èƒ½æ²¿ç€ input sequence ä¸€è·¯ä¼ å¯¼ï¼Œ ä½†ä¹Ÿæ„å‘³ç€åœ¨è®¡ç®—å‡º iâˆ’1 å•å…ƒä¹‹å‰ï¼Œæ— æ³•è®¡ç®—å‡º i å•å…ƒçš„è¾“å‡ºã€‚

ä¸ RNN æ­¤å¯¹æ¯”ï¼Œä¸€ç»´å·ç§¯ï¼ˆ1D convolutionï¼‰å¦‚ä¸‹ï¼š
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/convolutional-connection.png)
åœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæ‰€æœ‰è¾“å‡ºå‘é‡éƒ½å¯ä»¥å¹¶è¡Œè®¡ç®—ï¼Œå› æ­¤é€Ÿåº¦éå¸¸å¿«ã€‚ä½†ç¼ºç‚¹æ˜¯å®ƒä»¬ åœ¨ long range dependencies å»ºæ¨¡æ–¹é¢éå¸¸å¼±ã€‚åœ¨ä¸€ä¸ªå·ç§¯å±‚ä¸­ï¼Œåªæœ‰è·ç¦»æ¯” kernel size å°çš„å•è¯ä¹‹é—´æ‰èƒ½å½¼æ­¤äº¤äº’ã€‚å¯¹äºæ›´é•¿çš„ä¾èµ–ï¼Œå°±éœ€è¦å †å è®¸å¤šå·ç§¯ã€‚

Transformer è¯•å›¾å…¼é¡¾äºŒè€…çš„ä¼˜ç‚¹ï¼š

- å¯ä»¥åƒå¯¹å½¼æ­¤ç›¸é‚»çš„å•è¯ä¸€æ ·ï¼Œè½»æ¾åœ°å¯¹è¾“å…¥åºåˆ—çš„æ•´ä¸ªèŒƒå›´å†…çš„ä¾èµ–å…³ç³»è¿›è¡Œå»ºæ¨¡ï¼ˆäº‹å®ä¸Šï¼Œå¦‚æœæ²¡æœ‰ä½ç½®å‘é‡ï¼ŒäºŒè€…å°±æ²¡æœ‰åŒºåˆ«ï¼‰ï¼›
- åŒæ—¶ï¼Œé¿å… recurrent connectionsï¼Œå› æ­¤æ•´ä¸ªæ¨¡å‹å¯ä»¥ç”¨éå¸¸é«˜æ•ˆçš„ feed forward æ–¹å¼è®¡ç®—ã€‚
Transformer çš„å…¶ä½™è®¾è®¡ä¸»è¦åŸºäºä¸€ä¸ªè€ƒè™‘å› ç´  â€”â€” æ·±åº¦ â€”â€” å¤§å¤šæ•°é€‰æ‹©éƒ½æ˜¯è®­ç»ƒå¤§é‡ transformer block å±‚ï¼Œä¾‹å¦‚ï¼Œtransformer ä¸­åªæœ‰ä¸¤ä¸ªéçº¿æ€§çš„åœ°æ–¹ï¼š

- self-attention ä¸­çš„ softmaxï¼›
- å‰é¦ˆå±‚ä¸­çš„ ReLUã€‚
æ¨¡å‹çš„å…¶ä½™éƒ¨åˆ†å®Œå…¨ç”±çº¿æ€§å˜æ¢ç»„æˆï¼Œå®Œç¾åœ°ä¿ç•™äº†æ¢¯åº¦ã€‚

>I suppose the layer normalization is also nonlinear, but that is one nonlinearity that actually helps to keep the gradient stable as it propagates back down the network.

# 7 å†å²åŒ…è¢±
å¦‚æœåœ¨ç½‘ä¸Šçœ‹ä¸€äº›ä»‹ç» transformer çš„æ–‡ç« ï¼Œå¯èƒ½ä¼šç»æ³¨æ„å®ƒä»¬æåˆ°çš„ä¸€äº›æ¦‚å¿µå’Œæœ¯è¯­æœ¬æ–‡å¹¶æ²¡æœ‰ä»‹ç»ã€‚ è¿™æ˜¯å› ä¸ºæˆ‘è®¤ä¸ºé‚£äº›ä¸œè¥¿å¹¶ä¸æ˜¯ç†è§£ç°ä»£ transformer æ‰€å¿…éœ€çš„ã€‚ è¯è™½å¦‚æ­¤ï¼Œæœ‰ä¸¤ä¸ªæ–¹é¢è¿˜æ˜¯å¯ä»¥ä»‹ç»ä¸€ä¸‹ï¼Œå› ä¸ºå®ƒä»¬å¯¹äºç†è§£ç½‘ä¸Šçš„é‚£äº›å…³äºç°ä»£ transformer çš„æ–‡ç« è¿˜æ˜¯æœ‰å¸®åŠ©çš„ã€‚

## 7.1 ä¸ºä»€ä¹ˆå« self-attentionï¼Ÿ
é‡ç‚¹åœ¨ attention è¿™ä¸ªå•è¯ä¸Šã€‚

åœ¨ self-attention æå‡ºä¹‹å‰ï¼Œsequence models ä¸»è¦æŒ‡çš„æ˜¯ç”± recurrent networks æˆ– convolutions å †å ï¼ˆstackï¼‰è€Œæˆçš„ç½‘ç»œã€‚ ä¹‹åäººä»¬å‘ç°ï¼Œå¦‚æœä¸æ˜¯å°†ä¸Šä¸€å±‚çš„è¾“å‡ºç›´æ¥ feed åˆ°ä¸‹ä¸€å±‚çš„è¾“å…¥ï¼Œ è€Œæ˜¯å¼•å…¥ä¸€ç§ä¸­é—´æœºåˆ¶æ¥åˆ¤æ–­è¾“å…¥ä¸­çš„å“ªäº›å…ƒç´ ä¸è¾“å‡ºä¸­çš„æŸä¸ªç‰¹å®šå•è¯ç›¸å…³ï¼Œ å°±èƒ½ç»™ sequence models å¸¦æ¥å¾ˆå¤§æ”¹å–„ã€‚å…·ä½“æ¥è¯´ï¼Œ
- æˆ‘ä»¬æŠŠ input ç§°ä¸º valuesï¼ˆå› ä¸ºå®ƒä»¬æ˜¯å®å®åœ¨åœ¨çš„å€¼ï¼Œæˆ‘ä»¬å°†åŸºäºè¿™äº›å€¼è®¡ç®—è¾“å‡ºï¼‰ï¼›
- ç„¶åï¼Œä¸€äº›ï¼ˆtrainableï¼‰æœºåˆ¶ä¸ºæ¯ä¸ª value åˆ†é…ä¸€ä¸ª keyï¼›
- æœ€åï¼Œå¯¹æ¯ä¸ª outputï¼Œä¸€äº›å…¶ä»–æœºåˆ¶åˆ†é…ä¸€ä¸ª queryã€‚
è¿™äº›åç§°æºè‡ªé”®å€¼å­˜å‚¨ï¼ˆkey-value storeï¼‰æ•°æ®ç»“æ„ã€‚ åœ¨ key-value store åœºæ™¯ä¸­ï¼Œå¯¹äºæ¯ä¸ª queryï¼ˆæŸ¥è¯¢ï¼‰ï¼Œstore ä¸­ï¼ˆæœ€å¤šï¼‰åªæœ‰ä¸€ä¸ª item èƒ½åŒ¹é…åˆ°ï¼Œ è¿™ä¸ª item æœ‰å”¯ä¸€çš„ keyï¼Œè¿”å›è¿™ä¸ª key å¯¹åº”çš„ valueã€‚

Attentionï¼ˆæ³¨æ„åŠ›ï¼‰æ¨¡å‹æ˜¯ key-value store æ¨¡å‹çš„å®½æ¾ç‰ˆï¼š

- store ä¸­çš„æ¯ä¸ª key éƒ½èƒ½åœ¨æŸç§ç¨‹åº¦ä¸Šï¼ˆè€Œä¸æ˜¯ç²¾ç¡® 100% æˆ– 0%ï¼‰åŒ¹é…åˆ° queryï¼›
- å¦å¤–ï¼Œquery è¿”å›çš„ä¹Ÿä¸æ˜¯å•ä¸ª valueï¼Œè€Œæ˜¯æ‰€æœ‰ valueï¼Œæˆ‘ä»¬æ ¹æ®æ¯ä¸ª key ä¸ query åŒ¹é…çš„ç¨‹åº¦å¯¹ç›¸åº” value å–ä¸€ä¸ªåŠ æƒå’Œã€‚
self-attention çš„é‡å¤§çªç ´åœ¨äºï¼Œattention æœ¬èº«å°±æ˜¯ä¸€ç§è¶³å¤Ÿå¼ºå¤§çš„æœºåˆ¶ï¼Œèƒ½å®Œæˆæ‰€æœ‰å­¦ä¹ ã€‚ æ­£å¦‚ä½œè€…æ‰€è¯´ï¼ŒAttention is all you needã€‚
- Key/value/query éƒ½æ¥è‡ªåŒä¸€ä¸ª input vectorï¼ˆåªæ˜¯å„è‡ªç»è¿‡äº†ç•¥å¾®ä¸åŒçš„çº¿æ€§å˜æ¢ï¼‰ï¼›
- ä»–ä»¬å…³æ³¨è‡ªå·±ï¼ˆattend to themselvesï¼‰ï¼Œå› æ­¤å« self-attentionï¼›
- è¿™ç§ self-attention ç»è¿‡å¤šå±‚å †å ä¹‹åï¼Œå°±èƒ½æä¾›è¶³å¤Ÿçš„éçº¿æ€§å’Œè¡¨å¾èƒ½åŠ›ï¼ˆnonlinearity and representational powerï¼‰æ¥å­¦ä¹ éå¸¸å¤æ‚çš„åŠŸèƒ½ã€‚


## 7.2 æœ€åˆçš„ transformer: encoders and decoders
å½“æ—¶çš„ sequence-to-sequence model çš„æ ‡å‡†ç»“æ„æ˜¯å¸¦ teacher forcing çš„ encoder-decoder æ¶æ„ï¼Œ
![](https://github.com/Walker-DJ1/blog_data/raw/main/Transformers_From_Scratch/encoder-decoder.png)
Encoder è·å–è¾“å…¥åºåˆ—å¹¶å°†æ•´ä¸ª sequence æ˜ å°„ä¸ºä¸€ä¸ª latent representationsï¼Œ è¿™å¯ä»¥æ˜¯ä¸€ç³»åˆ— latent vectorsï¼Œä¹Ÿå¯ä»¥æ˜¯å¦‚ä¸Šå›¾ä¸­çš„å•ä¸ªå‘é‡ã€‚ ç„¶åå°†è¯¥å‘é‡ä¼ é€’ç»™ decoderï¼Œåè€…å°†å…¶è§£ç ä¸ºæœŸæœ›çš„ç›®æ ‡åºåˆ—ï¼ˆä¾‹å¦‚ï¼ŒåŒä¸€å¥è¯çš„å¦ä¸€ç§è¯­è¨€è¡¨ç¤ºï¼‰ã€‚

Teacher forcing æŒ‡çš„æ˜¯å…è®¸ decoder è®¿é—® input çš„æŠ€æœ¯ â€”â€” ä½†ä»¥è‡ªå›å½’ï¼ˆautoregressiveï¼‰çš„æ–¹å¼ã€‚ ä¹Ÿå°±æ˜¯è¯´ï¼Œ decoder åŸºäº latent vectors å’Œå®ƒè‡ªå·±å·²ç»ç”Ÿæˆçš„å•è¯ï¼Œé€å•è¯ç”Ÿæˆè¾“å‡ºå¥å­ã€‚ è¿™å‡è½»äº† latent representations çš„ä¸€äº›å‹åŠ›ï¼š

- decoder å¯ä»¥ä½¿ç”¨é€è¯é‡‡æ ·ï¼ˆword-by-word samplingï¼‰æ¥å¤„ç†è¯­æ³•ï¼ˆsyntax and grammarï¼‰ç­‰ä½çº§ç»“æ„ï¼Œ
- è€Œä½¿ç”¨ latent vectors æ¥ capture æ›´é«˜çº§åˆ«çš„è¯­ä¹‰ç»“æ„ï¼ˆsemantic structureï¼‰ã€‚
ç†æƒ³æƒ…å†µä¸‹ï¼Œä½¿ç”¨ç›¸åŒçš„ latent representations è¿›è¡Œä¸¤æ¬¡ decoding ä¼šå¾—åˆ°ä¸¤ä¸ªå…·æœ‰ç›¸åŒå«ä¹‰çš„ä¸åŒå¥å­ã€‚

åœ¨åæ¥çš„ transformer ä¸­ï¼Œå¦‚ BERT å’Œ GPT-2ï¼Œ encoder/decoder è¢«å®Œå…¨å»æ‰äº†ã€‚ ç®€å•çš„ transformer block åšå †å ï¼ˆstackï¼‰å°±è¶³ä»¥åœ¨è®¸å¤šåŸºäºåºåˆ—çš„ä»»åŠ¡ä¸­å®ç°æœ€å…ˆè¿›çš„æ•ˆæœã€‚ è¿™ç§æ¨¡å‹æœ‰æ—¶è¢«ç§°ä¸º decoder-only transformerï¼ˆå¯¹äºè‡ªå›å½’æ¨¡å‹ï¼‰ æˆ– encoder-only transformerï¼ˆå¯¹äºæ²¡æœ‰ masking çš„æ¨¡å‹ï¼‰ã€‚

# 8 ç°ä»£ transformers
æ¥çœ‹å‡ ä¸ªæœ‰ä»£è¡¨æ€§çš„ç°ä»£ transformersã€‚

# 8.1 Google BERTï¼š340M å‚æ•°
BERT (Bidirectional Encoder Representations from Transformers) æ˜¯é¦–æ‰¹è¯æ˜ transformer å¯ä»¥åœ¨å„ç§åŸºäºè¯­è¨€çš„ä»»åŠ¡ä¸Š ï¼ˆquestion answering, sentiment classification or classifying whether two sentences naturally follow one anotherï¼‰ è¾¾åˆ°äººç±»æ°´å¹³çš„æ¨¡å‹ä¹‹ä¸€ã€‚

BERT ç”±ä¸€äº›ä¸æœ¬æ–‡æè¿°çš„ç±»ä¼¼çš„ç®€å• transformer block å †å è€Œæˆï¼Œç„¶ååœ¨ä¸€ä¸ªå¤§å‹é€šç”¨é¢†åŸŸè¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œ è¯¥è¯­æ–™åº“ç”±åŒ…å« 8 äº¿ä¸ªï¼ˆ800Mï¼‰å•è¯çš„è‹±æ–‡ä¹¦ç±ï¼ˆç°ä»£ä½œå“ï¼Œfrom unpublished authorsï¼‰ å’ŒåŒ…å« 25 äº¿ï¼ˆ2.5Bï¼‰ä¸ªå•è¯è‹±æ–‡ç»´åŸºç™¾ç§‘æ–‡ç« ï¼ˆå»æ‰äº† markupï¼‰ç»„æˆã€‚

é¢„è®­ç»ƒç”±ä¸¤ä¸ªä»»åŠ¡ç»„æˆï¼š

Masking
A certain number of words in the input sequence are: masked out, replaced with a random word or kept as is. The model is then asked to predict, for these words, what the original words were. Note that the model doesn't need to predict the entire denoised sentence, just the modified words. Since the model doesn't know which words it will be asked about, it learns a representation for every word in the sequence.
Next sequence classification
Two sequences of about 256 words are sampled that either (a) follow each other directly in the corpus, or (b) are both taken from random places. The model must then predict whether a or b is the case.
BERT uses WordPiece tokenization, which is somewhere in between word-level and character level sequences. It breaks words like walking up into the tokens walk and ##ing. This allows the model to make some inferences based on word structure: two verbs ending in -ing have similar grammatical functions, and two verbs starting with walk- have similar semantic function.

The input is prepended with a special <cls> token. The output vector corresponding to this token is used as a sentence representation in sequence classification tasks like the next sentence classification (as opposed to the global average pooling over all vectors that we used in our classification model above).

After pretraining, a single task-specific layer is placed after the body of transformer blocks, which maps the general purpose representation to a task specific output. For classification tasks, this simply maps the first output token to softmax probabilities over the classes. For more complex tasks, a final sequence-to-sequence layer is designed specifically for the task.

The whole model is then re-trained to finetune the model for the specific task at hand.
ä½œè€…å±•ç¤ºäº†ä¸ä¹‹å‰çš„æ¨¡å‹ç›¸æ¯”ï¼Œæœ€å¤§çš„æ”¹è¿›æ¥è‡ª BERT çš„åŒå‘ç‰¹æ€§ï¼ˆbidirectional natureï¼‰ã€‚ ä¹‹å‰çš„æ¨¡å‹ï¼Œä¾‹å¦‚ GPTï¼Œä½¿ç”¨çš„æ˜¯ autoregressive maskï¼Œåªå…è®¸ attention ä½¿ç”¨å‰é¢çš„ tokenã€‚ åœ¨ BERT ä¸­ï¼Œall attention is over the whole sequenceï¼Œè¿™æ˜¯æ€§èƒ½æå‡çš„ä¸»è¦æ¥æºã€‚

> è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ â€œBERTâ€ ä¸­çš„ B è¡¨ç¤º â€œbidirectionalâ€ã€‚

æœ€å¤§çš„ BERT model ä½¿ç”¨äº† 24 transformer blocksï¼Œembedding dimension 1024ï¼Œ16 attention headsï¼Œ æ€»å‚æ•°æ•°é‡ä¸º 3.4 äº¿ï¼ˆ340Mï¼‰ã€‚

## 8.2 OpenAI GPT-2ï¼š1.5B å‚æ•°
They show state-of-the art performance on many tasks. On the wikipedia compression task that we tried above, they achieve 0.93 bits per byte.

GPT-2 æ˜¯ç¬¬ä¸€ä¸ªçœŸæ­£è¿›å…¥ä¸»æµæ–°é—»çš„ transformer æ¨¡å‹ï¼ŒåŸå› æ˜¯ GPT-2 å¯ä»¥ç”Ÿæˆçœ‹èµ·æ¥è¶³å¤Ÿå¯ä¿¡çš„æ–‡æœ¬ï¼Œå¦‚æœ 2016 å¹´æœ‰è¿™ç§æŠ€æœ¯ï¼Œ é‚£å½“å¹´ç¾å›½æ€»ç»Ÿå¤§é€‰ä¸­å‡ºç°çš„é‚£ç§å¤§è§„æ¨¡å‡æ–°é—»æ´»åŠ¨åªéœ€è¦ä¸€ä¸ªäººå°±èƒ½å®Œæˆäº†ã€‚

å¯¹äº GPT-2ï¼ŒOpenAI ä¹Ÿåšå‡ºäº†ä¸€ä¸ªé¢‡å—äº‰è®®çš„å†³å®š â€”â€” ä¸å…¬å¸ƒå®Œæ•´æ¨¡å‹ã€‚

GPT-2 ç¬¬ä¸€ä¸ªæŠ€å·§æ˜¯æ„å»ºä¸€ä¸ªæ–°çš„é«˜è´¨é‡æ•°æ®é›†ï¼Œ

- è™½ç„¶ BERT ä½¿ç”¨äº†é«˜è´¨é‡çš„æ•°æ®ï¼Œä½†æ•°æ®çš„æ¥æºï¼ˆç²¾å¿ƒç¼–å†™çš„ä¹¦ç±å’Œç»´åŸºç™¾ç§‘æ–‡ç« ï¼‰åœ¨å†™ä½œé£æ ¼ä¸Šç¼ºä¹å¤šæ ·æ€§ï¼›
- ä¸ºäº†åœ¨ä¸ç‰ºç‰²è´¨é‡çš„å‰æä¸‹æ”¶é›†æ›´å¤šä¸åŒçš„æ•°æ®ï¼Œä½œè€…ä½¿ç”¨ç¤¾äº¤åª’ä½“ç½‘ç«™ Reddit ä¸Šçš„é“¾æ¥æ¥æ”¶é›†å¤§é‡æ–‡æœ¬ã€‚
GPT2 æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªè¯­è¨€ç”Ÿæˆæ¨¡å‹ï¼ˆlanguage generation modelï¼‰ï¼Œ å› æ­¤åƒæˆ‘ä»¬è‡ªå·±è®¾è®¡çš„ text generation transformer ä¸€æ ·ï¼Œå®ƒä¹Ÿä½¿ç”¨äº† masked self-attentionã€‚ å®ƒä½¿ç”¨å­—èŠ‚å¯¹ç¼–ç ï¼ˆbyte-pair encodingï¼‰æ¥ tokenize the languageï¼Œ è¿™ä¸ WordPiece encoding ä¸€æ ·å°†å•è¯æ‹†åˆ†ä¸ºæ¯”â€œæ¯”å•è¯çŸ­ã€æ¯”å•ä¸ªå­—æ¯é•¿â€çš„ tokensã€‚

GPT2 ä¸æˆ‘ä»¬çš„ text generation transformer éå¸¸ç›¸ä¼¼ï¼Œåªæœ‰å¾ˆå°çš„å±‚çº§é¡ºåºå·®å¼‚ï¼Œä»¥åŠå¢åŠ äº†è®­ç»ƒæ·±åº¦ã€‚ æœ€å¤§çš„æ¨¡å‹ä½¿ç”¨ 48 ä¸ª transformer blockï¼Œåºåˆ—é•¿åº¦ä¸º 1024ï¼ŒåµŒå…¥ç»´åº¦ä¸º 1600ï¼Œæ€»å…± 1.5B å‚æ•°ã€‚

GPT2 åœ¨å¾ˆå¤šä»»åŠ¡ä¸Šéƒ½è¡¨ç°å‡ºäº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚åœ¨ä¸Šé¢æåˆ°çš„ç»´åŸºç™¾ç§‘å‹ç¼©ä»»åŠ¡ä¸­ï¼Œå®ƒå–å¾—äº†æ¯å­—èŠ‚ 0.93 ä½çš„å‹ç¼©æ•ˆç‡ã€‚
## 8.3 Transformer-XL
While the transformer represents a massive leap forward in modeling long-range dependency, the models we have seen so far are still fundamentally limited by the size of the input. Since the size of the dot-product matrix grows quadratically in the sequence length, this quickly becomes the bottleneck as we try to extend the length of the input sequence. Transformer-XL is one of the first succesful transformer models to tackle this problem.

During training, a long sequence of text (longer than the model could deal with) is broken up into shorter segments. Each segment is processed in sequence, with self-attention computed over the tokens in the curent segment and the previous segment. Gradients are only computed over the current segment, but information still propagates as the segment window moves through the text. In theory at layer n, information may be used from n segments ago.

A similar trick in RNN training is called truncated backpropagation through time. We feed the model a very long sequence, but backpropagate only over part of it. The first part of the sequence, for which no gradients are computed, still influences the values of the hidden states in the part for which they are.

To make this work, the authors had to let go of the standard position encoding/embedding scheme. Since the position encoding is absolute, it would change for each segment and not lead to a consistent embedding over the whole sequence. Instead they use a relative encoding. For each output vector, a different sequence of position vectors is used that denotes not the absolute position, but the distance to the current output.

This requires moving the position encoding into the attention mechanism (which is detailed in the paper). One benefit is that the resulting transformer will likely generalize much better to sequences of unseen length.

## 8.4 Sparse transformers
Sparse transformers tackle the problem of quadratic memory use head-on. Instead of computing a dense matrix of attention weights (which grows quadratically), they compute the self-attention only for particular pairs of input tokens, resulting in a sparse attention matrix, with only nnâˆ’âˆ’âˆš explicit elements.

This allows models with very large context sizes, for instance for generative modeling over images, with large dependencies between pixels. The tradeoff is that the sparsity structure is not learned, so by the choice of sparse matrix, we are disabling some interactions between input tokens that might otherwise have been useful. However, two units that are not directly related may still interact in higher layers of the transformer (similar to the way a convolutional net builds up a larger receptive field with more convolutional layers).

Beyond the simple benefit of training transformers with very large sequence lengths, the sparse transformer also allows a very elegant way of designing an inductive bias. We take our input as a collection of units (words, characters, pixels in an image, nodes in a graph) and we specify, through the sparsity of the attention matrix, which units we believe to be related. The rest is just a matter of building the transformer up as deep as it will go and seeing if it trains.

# 9 å¤§å‹æ¨¡å‹ä¼˜åŒ–
è®­ç»ƒ transformer çš„ä¸€å¤§ç“¶é¢ˆæ˜¯ self attention ä¸­çš„ç‚¹ç§¯çŸ©é˜µï¼Œ

- å¯¹äºåºåˆ—é•¿åº¦ tï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å« t2 ä¸ªå…ƒç´ çš„ç¨ å¯†çŸ©é˜µã€‚
- åœ¨æ ‡å‡†çš„ 32 ä½ç²¾åº¦ä¸‹ï¼Œå½“ t=1000 æ—¶ï¼Œ16 çŸ©é˜µä½œä¸ºä¸€ä¸ª batchï¼Œè¿™ä¸ª batch å ç”¨å¤§çº¦ 250Mb çš„æ˜¾å­˜ã€‚
- ç”±äºæˆ‘ä»¬æ¯ä¸ª self-attention æ“ä½œè‡³å°‘éœ€è¦å››ä¸ªå±‚ï¼ˆåœ¨ softmax ä¹‹å‰å’Œä¹‹åï¼ŒåŠ ä¸Šå®ƒä»¬çš„æ¢¯åº¦ï¼‰ï¼Œè¿™é™åˆ¶äº†åœ¨æ ‡å‡† 12Gb GPU ä¸­æœ€å¤šåªèƒ½ä½¿ç”¨ 12 å±‚ã€‚
å®é™…ä¸Šæˆ‘ä»¬èƒ½ç”¨åˆ°çš„å±‚æ•°æ›´å°‘ï¼Œå› ä¸ºè¾“å…¥å’Œè¾“å‡ºä¹Ÿå ç”¨äº†å¤§é‡æ˜¾å­˜ï¼ˆå°½ç®¡ç‚¹ç§¯å ä¸»å¯¼åœ°ä½ï¼‰ã€‚

> ç½‘ä¸Šæœ‰äº›æ¨¡å‹åŒ…å«è¶…è¿‡ 12000 çš„åºåˆ—é•¿åº¦ï¼Œæœ‰ 48 å±‚ï¼Œ ä½¿ç”¨å¯†å®çš„ç‚¹ç§¯çŸ©é˜µã€‚è¿™äº›æ¨¡å‹æ˜¯åœ¨é›†ç¾¤ä¸Šè®­ç»ƒçš„ï¼Œä½†æ˜¯å•ä¸ªå‰å‘/åå‘ propagation ä»ç„¶åªèƒ½ç”±å•ä¸ª GPU æ¥å®Œæˆã€‚

å¦‚ä½•å°†å¦‚æ­¤å·¨å¤§çš„ transformer æ”¾å…¥ 12Gb å†…å­˜ä¸­ï¼Ÿä¸»è¦æœ‰ä¸‰ä¸ªæŠ€å·§ã€‚

## 9.1 åŠç²¾åº¦ï¼ˆhalf precisionï¼‰
åœ¨ç°ä»£ GPU å’Œ TPU ä¸Šï¼Œtensor è®¡ç®—å¯ä»¥åœ¨ 16 ä½æµ®ç‚¹ä¸Šé«˜æ•ˆå®Œæˆã€‚ ä½†å¹¶ä¸æ˜¯å°† tensor çš„ dtype è®¾ç½®ä¸º torch.float16 é‚£ä¹ˆç®€å•ã€‚å¯¹äºæŸäº›éƒ¨åˆ†ï¼Œå¦‚ lossï¼Œä»ç„¶éœ€è¦ 32 ä½ç²¾åº¦ã€‚ ä½†å…¶ä¸­å¤§éƒ¨åˆ†å¯ä»¥é€šè¿‡ç°æœ‰åº“ç›¸å¯¹è½»æ¾åœ°æå®šã€‚

åŠç²¾åº¦ä¼˜åŒ–èƒ½ä½¿å†…å­˜å ç”¨å‡åŠï¼Œæˆ–è€…è¯´èƒ½ä½¿æœ‰æ•ˆå†…å­˜ç¿»å€ã€‚

## 9.2 æ¢¯åº¦ç§¯ç´¯ï¼ˆgradient accumulationï¼‰
å¯¹äºå¤§å‹æ¨¡å‹ï¼Œæˆ‘ä»¬å¯èƒ½åªèƒ½å¯¹å•ä¸ªå®ä¾‹æ‰§è¡Œå‰å‘/åå‘ä¼ é€’ï¼ˆforward/backward passï¼‰ã€‚ batch size = 1 ä¸å¤ªå¯èƒ½äº§ç”Ÿç¨³å®šçš„å­¦ä¹ ã€‚

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥å¯¹æ›´å¤§ batch size ä¸­çš„æ¯ä¸ªå®ä¾‹æ‰§è¡Œå•ä¸ªå‰å‘/åå‘ï¼Œå¹¶å¯¹æˆ‘ä»¬æ‰¾åˆ°çš„æ¢¯åº¦ç®€å•åœ°æ±‚å’Œ ï¼ˆè¿™æ˜¯å¤šå…ƒé“¾å¼æ³•åˆ™multivariate chain ruleçš„ç»“æœï¼‰ã€‚ å½“æˆ‘ä»¬åˆ°è¾¾ batch çš„æœ«å°¾æ—¶ï¼Œæ‰§è¡Œå•æ­¥æ¢¯åº¦ä¸‹é™ï¼Œå¹¶å°†æ¢¯åº¦å½’é›¶ï¼ˆzero outï¼‰ã€‚ åœ¨ Pytorch ä¸­è¿™éå¸¸å®¹æ˜“ï¼Œoptimizer.zero_grad() å°±è¡Œäº†ã€‚

9.3 æ¢¯åº¦ checkpointï¼ˆgradient checkpointingï¼‰
å¦‚æœæ¨¡å‹å¤ªå¤§ä»¥è‡³äºå³ä½¿æ˜¯å•ä¸ª forward/backward ä¹Ÿæ— æ³•æ”¾å…¥å†…å­˜ï¼Œé‚£å°±åªèƒ½ç‰ºç‰²æ›´å¤šçš„è®¡ç®—æ¥æé«˜å†…å­˜æ•ˆç‡ã€‚

åœ¨ gradient checkpointing ä¸­ï¼Œå°†æ¨¡å‹åˆ†æˆå‡ ä¸ªéƒ¨åˆ†ï¼ˆsectionsï¼‰ã€‚å¯¹æ¯ä¸ªéƒ¨åˆ†æ‰§è¡Œå•ç‹¬çš„ forward/backward æ¢¯åº¦è®¡ç®—ï¼Œè€Œæ— éœ€ä¸ºå…¶ä½™éƒ¨åˆ†ä¿ç•™ä¸­é—´å€¼ã€‚ Pytorch ç›¸å…³çš„å‡½æ•°ç›´æ¥å¯ç”¨ã€‚ æ›´å¤šä¿¡æ¯å¯å‚è€ƒ è¿™ç¯‡åšå®¢ã€‚

## 10 ç»“æŸè¯­
Transformer å¾ˆå¯èƒ½æ˜¯æœªæ¥å‡ åå¹´å ä¸»å¯¼åœ°ä½çš„æœ€ç®€å•æœºå™¨å­¦ä¹ æ¶æ„ã€‚ä½œä¸ºä»ä¸šè€…ï¼Œæœ‰å……åˆ†çš„ç†ç”±å…³æ³¨å®ƒä»¬ã€‚

é¦–å…ˆï¼Œç›®å‰çš„æ€§èƒ½ç“¶é¢ˆçº¯ç²¹åœ¨ç¡¬ä»¶ä¸Šã€‚ä¸å·ç§¯æˆ– LSTM ä¸åŒï¼Œ transformer ç›®å‰çš„é™åˆ¶å®Œå…¨å–å†³äºæˆ‘ä»¬èƒ½æŠŠå¤šå¤§çš„æ¨¡å‹æ”¾åˆ° GPU å†…å­˜ä¸­ï¼Œ ä»¥åŠæˆ‘ä»¬å¯ä»¥åœ¨åˆç†çš„æ—¶é—´å†…è¾“å…¥å¤šå°‘æ•°æ®è¿›å»ã€‚ æˆ‘æ¯«ä¸æ€€ç–‘æˆ‘ä»¬æœ€ç»ˆä¼šè¾¾åˆ°è¿™æ ·çš„åœ°æ­¥ï¼š æ›´å¤šå±‚å’Œæ›´å¤šæ•°æ®ä¸å†æœ‰å¸®åŠ©ï¼Œä½†ç›®å‰ä¼¼ä¹è¿˜æ²¡æœ‰è¾¾åˆ°è¿™ä¸ªåœ°æ­¥ã€‚

å…¶æ¬¡ï¼Œtransformer æå…¶é€šç”¨ã€‚ åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformer ä¸»è¦åœ¨è¯­è¨€å»ºæ¨¡æ–¹é¢å–å¾—äº†å·¨å¤§æˆåŠŸï¼Œ åœ¨å›¾åƒå’ŒéŸ³ä¹åˆ†ææ–¹é¢ä¹Ÿå–å¾—äº†ä¸€å®šçš„æˆåŠŸï¼Œä½† transformer å…·æœ‰ä¸€å®šç¨‹åº¦çš„é€šç”¨æ€§ï¼Œå…¶ä»–é¢†åŸŸçš„åº”ç”¨è¿˜æœ‰å¾…å¼€å‘ã€‚

- åŸºæœ¬ transformer æ˜¯ä¸€ä¸ª set-to-set æ¨¡å‹ã€‚ åªè¦æ•°æ®æ˜¯åŸºæœ¬å•ä½ç»„æˆçš„é›†åˆï¼ˆa set of unitsï¼‰ï¼Œå°±å¯ä»¥åº”ç”¨ transformerï¼›
- æ•°æ®çš„å…¶ä»–ä¿¡æ¯ï¼ˆå¦‚å±€éƒ¨ç»“æ„ï¼‰ï¼Œå¯ä»¥é€šè¿‡ä½ç½®åµŒå…¥æˆ–é€šè¿‡ manipulate æ³¨æ„åŠ›çŸ©é˜µçš„ç»“æ„ï¼ˆä½¿å…¶ç¨€ç–æˆ–å±è”½éƒ¨åˆ†ï¼‰æ¥æ·»åŠ ï¼Œ è¿™åœ¨å¤šæ¨¡æ€å­¦ä¹ ï¼ˆmulti-modal learningï¼‰ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥è½»æ¾åœ°å°†å¸¦å­—å¹•çš„å›¾åƒ åˆ†è§£ä¸ºåƒç´ é›†åˆå’Œå­—ç¬¦é›†åˆï¼Œç„¶åè®¾è®¡ä¸€äº›ç²¾å·§çš„åµŒå…¥å’Œç¨€ç–ç»“æ„æ¥å¸®åŠ©æ¨¡å‹ç»„åˆå’Œå¯¹é½äºŒè€…ã€‚ å¦‚æœæˆ‘ä»¬å°†å…³äºæŸä¸€é¢†åŸŸçš„å…¨éƒ¨çŸ¥è¯†ç»„åˆæˆä¸€ä¸ªå…³ç³»å‹ç»“æ„ï¼ˆrelational structureï¼‰ï¼Œ å¦‚å¤šæ¨¡æ€çŸ¥è¯†å›¾è°±ï¼ˆmulti-modal knowledge graphï¼Œ[3]ï¼‰ï¼Œé‚£å°±å¯ä»¥ä½¿ç”¨ç®€å•çš„ transformer block åœ¨å¤šæ¨¡æ€å•å…ƒä¹‹é—´ä¼ æ’­ä¿¡æ¯ï¼Œ ç„¶åé€šè¿‡ç¨€ç–ç»“æ„æ§åˆ¶ä¸å“ªäº›å•å…ƒç›´æ¥äº¤äº’ã€‚
åˆ°ç›®å‰ä¸ºæ­¢ï¼Œtransformer è¿˜ä¸»è¦è¢«è§†ä¸ºä¸€ç§è¯­è¨€æ¨¡å‹ã€‚å¸Œæœ›éšç€æ—¶é—´æ¨ç§»ï¼Œ æˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒåœ¨å…¶ä»–é¢†åŸŸå¾—åˆ°æ›´å¤šé‡‡ç”¨ï¼Œä¸ä»…æ˜¯æé«˜è¿™äº›é¢†åŸŸçš„æ•ˆç‡ï¼Œè¿˜åŒ…æ‹¬ç®€åŒ–è¿™äº›é¢†åŸŸçš„ç°æœ‰æ¨¡å‹ï¼Œ è®©ä»ä¸šè€…èƒ½æ›´ç›´è§‚åœ°æ§åˆ¶ä»–ä»¬æ¨¡å‹çš„å½’çº³åå·®ã€‚


å‚è€ƒèµ„æ–™
> The illustrated transformer, Jay Allamar.
> The annotated transformer, Alexander Rush.
> The knowledge graph as the default data model for learning on heterogeneous knowledge Xander Wilcke, Peter Bloem, Victor de Boer
> Matrix factorization techniques for recommender systems Yehuda Koren et al.
