---
layout: post
comments: true
published: True
title: "QWEN2技术报告"
excerpt: "本报告介绍了 Qwen2 系列，这是我们大型语言模型和大型多模态模型的最新成员。 我们发布了一整套基础和指令调整的语言模型，参数范围从 0.5 到 720 亿，具有密集模型和专家混合模型。  Qwen2 超越了大多数先前的开放权重模型，包括其前身 Qwen1.5，并且在语言理解、生成、多语言熟练程度、编码、数学和推理等多个基准上表现出相对于专有模型的竞争性能。
"
date:   2024-08-06 15:30:00
mathjax: false
---

<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=utf-8">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:Wingdings;
	panose-1:5 0 0 0 0 0 0 0 0 0;}
@font-face
	{font-family:"MS Gothic";
	panose-1:2 11 6 9 7 2 5 8 2 4;}
@font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:DengXian;
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"DengXian Light";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"Microsoft YaHei";
	panose-1:2 11 5 3 2 2 4 2 2 4;}
@font-face
	{font-family:"Segoe UI";
	panose-1:2 11 5 2 4 2 4 2 2 3;}
@font-face
	{font-family:"\@Microsoft YaHei";}
@font-face
	{font-family:"\@DengXian Light";}
@font-face
	{font-family:"\@DengXian";
	panose-1:2 1 6 0 3 1 1 1 1 1;}
@font-face
	{font-family:"\@MS Gothic";
	panose-1:2 11 6 9 7 2 5 8 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:10.0pt;
	line-height:22.0pt;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
h1
	{mso-style-link:"Heading 1 Char";
	margin-top:17.0pt;
	margin-right:0in;
	margin-bottom:16.5pt;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:240%;
	page-break-after:avoid;
	font-size:22.0pt;
	font-family:"Times New Roman",serif;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:13.0pt;
	margin-right:0in;
	margin-bottom:13.0pt;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:172%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"DengXian Light";}
h3
	{mso-style-link:"Heading 3 Char";
	margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:12.0pt;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:20.8pt;
	page-break-after:avoid;
	font-size:10.5pt;
	font-family:"Times New Roman",serif;}
h4
	{mso-style-link:"Heading 4 Char";
	margin-top:6.0pt;
	margin-right:0in;
	margin-bottom:6.0pt;
	margin-left:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	line-height:18.8pt;
	page-break-after:avoid;
	font-size:9.0pt;
	font-family:"DengXian Light";}
p.MsoToc1, li.MsoToc1, div.MsoToc1
	{margin:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:10.0pt;
	line-height:22.0pt;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
p.MsoToc2, li.MsoToc2, div.MsoToc2
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:21.0pt;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:10.0pt;
	line-height:22.0pt;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
p.MsoToc3, li.MsoToc3, div.MsoToc3
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:42.0pt;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:10.0pt;
	line-height:22.0pt;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
a:link, span.MsoHyperlink
	{color:#0563C1;
	text-decoration:underline;}
p.MsoListParagraph, li.MsoListParagraph, div.MsoListParagraph
	{margin:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:21.0pt;
	line-height:22.0pt;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
span.MsoSubtleEmphasis
	{color:#404040;
	font-style:italic;}
p.MsoTocHeading, li.MsoTocHeading, div.MsoTocHeading
	{margin-top:12.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:107%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"DengXian Light";
	color:#2E74B5;}
span.Heading1Char
	{mso-style-name:"Heading 1 Char";
	mso-style-link:"Heading 1";
	font-family:"Times New Roman",serif;
	font-weight:bold;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"DengXian Light";
	font-weight:bold;}
p.a, li.a, div.a
	{mso-style-name:图表;
	mso-style-link:"图表 字符";
	margin:0in;
	text-align:justify;
	text-justify:inter-ideograph;
	text-indent:10.0pt;
	line-height:150%;
	font-size:9.0pt;
	font-family:"Times New Roman",serif;}
span.Heading3Char
	{mso-style-name:"Heading 3 Char";
	mso-style-link:"Heading 3";
	font-family:"Times New Roman",serif;
	font-weight:bold;}
span.a0
	{mso-style-name:"图表 字符";
	mso-style-link:图表;
	font-family:"Times New Roman",serif;}
span.Heading4Char
	{mso-style-name:"Heading 4 Char";
	mso-style-link:"Heading 4";
	font-family:"DengXian Light";
	font-weight:bold;}
.MsoChpDefault
	{font-size:10.5pt;
	font-family:DengXian;}
 /* Page Definitions */
 @page WordSection1
	{size:595.3pt 841.9pt;
	margin:1.0in 1.25in 1.0in 1.25in;
	layout-grid:15.6pt;}
div.WordSection1
	{page:WordSection1;}
 /* List Definitions */
 ol
	{margin-bottom:0in;}
ul
	{margin-bottom:0in;}
-->
</style>

</head>

<body lang=EN-US link="#0563C1" vlink="#954F72" style='word-wrap:break-word;
text-justify-trim:punctuation'>

<div class=WordSection1 style='layout-grid:15.6pt'>

<h1><a name="_Toc173831960">QWEN2</a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>技术报告</span></h1>

<p class=MsoTocHeading style='text-indent:.25in'><span lang=ZH-CN>目录</span></p>

<p class=MsoToc1 style='text-indent:.25in'><a href="#_Toc173831960">QWEN2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>技术报告</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>1</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831961"><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
background:white'>摘要</span><span style='color:windowtext;display:none;
text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>1</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831962">1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>简介</span>introduction<span
style='color:windowtext;display:none;text-decoration:none'>. </span><span
style='color:windowtext;display:none;text-decoration:none'>2</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831963">2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分词器和模型</span>TOKENIZER
&amp; MODEL<span style='color:windowtext;display:none;text-decoration:none'>. </span><span
style='color:windowtext;display:none;text-decoration:none'>3</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831964">2.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分词器</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>3</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831965">2.2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型架构</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>3</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831966">3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>预训练</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>5</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831967">3.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>预训练数据</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>5</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831968">3.2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>长上下文训练</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>6</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831969">4 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>后训练</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>6</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831970">4.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>训练后数据</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>7</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831971">4.2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>有监督的微调</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>8</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831972">4.3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>根据人类反馈进行强化学习</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>8</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831973">5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>评估</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>8</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831974">5.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>基础语言模型</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>9</span></a></p>

<p class=MsoToc3 style='margin-left:.5in;text-indent:.25in'><a
href="#_Toc173831975">5.2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>指令调整模型</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>13</span></a></p>

<p class=MsoToc2 style='margin-left:.25in;text-indent:.25in'><a
href="#_Toc173831976">6 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结论</span><span
style='color:windowtext;display:none;text-decoration:none'>... </span><span
style='color:windowtext;display:none;text-decoration:none'>21</span></a></p>

<p class=MsoNormal style='text-indent:.25in'>&nbsp;</p>

<p class=MsoNormal style='text-indent:.25in'>&nbsp;</p>

<h2><a name="_Toc173831961"><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:black;background:white'>摘要</span></a></h2>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>本报告介绍了</span> Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>系列，这是我们大型语言模型和大型多模态模型的最新成员。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们发布了<b><span
style='background:yellow'>一整套基础和指令调整的语言模型，参数范围从</span></b></span><b><span
style='background:yellow'> 0.5 </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>到</span><span style='background:
yellow'> 720 </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
background:yellow'>亿，具有密集模型和专家混合模型</span></b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>。</span>  Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>超越了大多数先前的开放权重模型，包括其前身</span>
Qwen1.5<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，并且在语言理解、生成、多语言熟练程度、编码、数学和推理等多个基准上表现出相对于专有模型的竞争性能。</span></p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>旗舰模型</span> Qwen2-72B <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>表现出色：</span>MMLU 84.2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GPQA 37.9<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>HumanEval
64.6<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GSM8K
89.5<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>BBH
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>作为基础语言模型</span>
82.4<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>经过指令调整的变体</span>
Qwen2-72B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>
MT-Bench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
9.1<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
Arena-Hard <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
48.1<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
LiveCodeBench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
35.7<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span><span
style='color:#00B0F0'>Qwen2 </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:#00B0F0'>还展现了强大的多语言能力，精通约</span><span style='color:#00B0F0'> 30 </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:#00B0F0'>种语言，涵盖英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语、越南语等</span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，凸显了其多功能性和全球影响力。旗舰模型</span>
Qwen2-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出色：</span>MMLU
84.2<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GPQA
37.9<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>HumanEval
64.6<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GSM8K
89.5<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>BBH
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>作为基础语言模型</span>
82.4<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>经过指令调整的变体</span>
Qwen2-72B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>
MT-Bench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
9.1<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
Arena-Hard <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
48.1<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
LiveCodeBench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
35.7<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>还展现了强大的多语言能力，精通约</span>
30 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>种语言，涵盖英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语、越南语等，凸显了其多功能性和全球影响力。</span></p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>为了促进社区创新和可访问性，我们在</span> Hugging Face<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span> ModelScope<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上公开提供</span> Qwen2 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型权重，并在</span>
GitHub <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上公开提供包括示例代码在内的补充材料</span>3<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这些平台还包括用于量化、微调和部署的资源，促进广泛的应用和研究工作。</span></p>

<h2><a name="_Toc173831962">1 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>简介</span>introduction</h2>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>随着</span> ChatGPT<span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>（</span>OpenAI<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2022<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）的出现，全球范围内对大型语言模型（</span>LLM<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）的热情不断升温。</span> 
Llama <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列的发布（</span>Touvron
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）进一步激发了开源社区的兴趣，特别是对于</span>
GPT <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>级别的本地法学硕士。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>最近，</span>ChatGPT
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的更新模型</span>
Claude-3 Opus (Anthropic, 2024) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
GPT-4o (omni) (OpenAI, 2024) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相继登上了</span>
Chatbot Arena <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的顶峰</span>
(Chiang et al., 2024)  <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该平台因其对法学硕士的人工评估而广受好评。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span>Llama3<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>AI@Meta<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）已成为最先进的开放权重模型系列，缩小了与领先专有模型的性能差距，并被广泛认为是</span>
GPT-4 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>级别。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>越来越多有竞争力的法学硕士现在正在追求类似于</span>
OpenAI <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的</span>
GPT <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列所取得的进步。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>其中许多模型，包括</span>
Qwen (Bai et al., 2023a)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Mistral
(Jiang et al., 2023a)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Gemma
(Mesnard et al., 2024) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等，都以开放权重的方式发布。</span></p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>近几个月来，我们<b>陆续推出了</b></span><b>Qwen</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列（</span>Bai et
al., 2023a</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）并进展到</span>Qwen1.5</b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen Team,
2024a<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与此同时，我们推出了<b>视觉语言模型</b></span><b>
Qwen-VL</b> (Bai et al., 2023b)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，并推出了<b>音频语言模型</b></span><b>
Qwen-Audio</b> (Chu et al., 2023)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在这项工作中，我们介绍了</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>大型语言模型和大型多模态模型家族的最新成员：</span>Qwen2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span>  <b><span
style='background:yellow'>Qwen2 </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>是一系列</span><span
style='background:yellow'> LLM</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>，基于</span><span
style='background:yellow'> Transformer </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>架构（</span><span
style='background:yellow'>Vaswani </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>等人，</span><span
style='background:yellow'>2017</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>），使用下一个令牌预测进行训练。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该模型系列包括<b>基础语言模型</b>（经过预训练但不符合人类偏好）和<b>指令调整模型</b>（根据适用于聊天和代理目的的数据集对单轮和多轮指令进行微调）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们的版本包括<b>四个密集模型，参数数量分别为</b></span><b><span
lang=ZH-CN> </span>0.5 B</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>1.5
B</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>7B
</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
72B</b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，加上一个具有</span><span
lang=ZH-CN> </span><b>57B</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个参数的专家混合</span>
(MoE) </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型，其中每个代币激活</span>
14B</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个参数</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>较小的型号，特别是</span>
Qwen2-0.5B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen2-1.5B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，专为轻松部署在智能手机、耳机和智能眼镜等便携式设备上而设计。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相反，较大的模型适合跨不同规模的</span>
GPU <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的部署。</span></p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>所有模型均在包含<b><span style='background:yellow'>超过</span></b></span><b><span
style='background:yellow'> 7 </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>万亿个令牌、涵盖广泛领域和语言的高质量、大规模数据集上进行预训练。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与之前版本的</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>包含更广泛的语言数据，提高了代码和数学内容的数量和质量。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>假设这种丰富可以提高法学硕士的推理能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>关于训练后，所有模型都经过监督微调和直接偏好优化（</span>DPO<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>Rafailov <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），通过学习人类反馈使它们与人类偏好保持一致。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这个过程赋予模型有效遵循指令的能力。</span></p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>我们对</span> Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>进行了全面评估，同时还选择了一系列基准模型，包括通过</span>
API <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>访问的开放权重模型和专有模型。</span> 
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在基本语言能力和指令调整功能的评估中均优于竞争模型。具体而言，</span>Qwen2-72B-Instruct<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（我们的指令调整变体）在</span>
MT-Bench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上得分</span>
9.1<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Zheng
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），在</span>
Arena-Hard <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上得分</span>
48.1  <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Chiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），以及</span>
LiveCodeBench <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上的</span>
35.7<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Jain
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>同时，基础语言模型</span>
Qwen2-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>
MMLU <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
84.2 (Hendrycks et al., 2021a)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
GPQA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
37.9 (Rein et al., 2023)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
HumanEval <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
64.6 (Chen et al., 2021)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，在</span>
GPQA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上获得</span>
89.5  GSM8K<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Cobbe
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2021<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span> BBH <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上的</span> 82.4<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Suzgun <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span></p>

<h2><a name="_Toc173831963">2 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分词器和模型</span>TOKENIZER
&amp; MODEL</h2>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>本节介绍</span>Qwen2<span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>的分词器和模型设计。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们详细介绍了不同模型尺寸的模型架构和配置。</span></p>

<h3><a name="_Toc173831964">2.1 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分词器</span></h3>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>继</span>Qwen<span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>（</span>Bai et al., 2023a<span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>）之后，我们采用<b><span
style='background:yellow'>基于字节级字节对编码</span></b>的相同分词器。</span><span lang=ZH-CN> </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>值得注意的是，该<b>分词器表现出较高的编码效率，其相对于替代方案具有更好的压缩率</b>就证明了这一点，从而促进了</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的多语言功能。</span></p>

<p class=MsoNormal style='text-indent:.25in'> <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>各种规模的模型都采用由</span><b><span
style='background:yellow'> 151,643 </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>个常规标记</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>和</span><span
lang=ZH-CN style='background:yellow'> </span><b><span style='background:yellow'>3
</span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
background:yellow'>个控制标记</span></b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
background:yellow'>组成的通用词汇表</span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>欲了解更多信息，请参阅</span>
Bai <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人。（</span>2023a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>需要注意的是，出于分布式训练的考虑，嵌入的有效尺寸更大。</span></p>

<h3><a name="_Toc173831965">2.2 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型架构</span><span
lang=ZH-CN> </span></h3>

<p class=MsoNormal style='text-indent:.25in'>Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>系列从根本上构成了<b>基于</b></span><b>
Transformer </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>架构的大型语言模型，具有因果掩模的自注意力</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Vaswani <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2017<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，该系列包含</span>
4 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个尺度的密集语言模型和专家混合</span>
(MoE) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在深入研究</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的独特属性之前，我们先介绍密集模型的细节。</span></p>

<h4>2.2.1 QWEN2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>密集模型</span></h4>

<p class=MsoNormal style='text-indent:.25in'> Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>密集模型的架构包括多个</span> Transformer
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>层，每个层都配备了因果注意机制和前馈神经网络（</span>FFN<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的主要区别如下所述：</span><span
lang=ZH-CN> </span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分组查询注意力</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们<b><span
style='background:yellow'>采用分组查询注意力（</span></b></span><b><span
style='background:yellow'>GQA</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>，</span><span style='background:
yellow'>Ainslie et al., 2023</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>）代替传统的多头注意力（</span><span
style='background:yellow'>MHA</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>）</span></b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>。</span>  <b>GQA </b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>优化了推理过程中</span> KV </b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>缓存的使用，显着提高了吞吐量。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>各种型号尺寸的详细</span>
KV <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>头配置在第</span>
2.2.3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>节中报告。</span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>使用</span>
YARN </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的双块注意力</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了扩展</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的上下文窗口，我们实现了双块注意力（</span>DCA<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>An <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），它将<b><span
style='background:yellow'>长序列分割成可管理长度的块。</span></b></span><b><span lang=ZH-CN
style='background:yellow'> </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>如果输入可以在一个块中处理，</span><span
style='background:yellow'>DCA </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>会产生与原始注意力相同的结果。</span><span
lang=ZH-CN style='background:yellow'> </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;background:yellow'>否则，</span><span
style='background:yellow'>DCA </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;background:yellow'>有助于有效捕获块内和块间标记之间的相对位置信息，从而提高长上下文性能。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，我们还使用</span>
YARN (Peng et al., 2023) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>来重新调整注意力权重，以获得更好的长度外推。</span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，我们遵循</span>
Qwen<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，使用</span>
SwiGLU (Dauphin et al., 2017) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行激活，使用旋转位置嵌入</span>
(RoPE, Su et al., 2024) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行位置嵌入，</span>QKV
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>偏差</span>
(Su, 2023) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行注意，</span>RMSNorm
(  Jiang et al., 2023b) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和训练稳定性的预归一化。</span></p>

<h4>2.2.2 QWEN2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>专家混合模型</span></h4>

<p class=MsoNormal style='text-indent:.25in'> Qwen2 MoE <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>模型的架构与</span>
Qwen1.5-MoE-A2.7B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的架构非常相似（</span>Qwen
Team<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2024c<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><b><span
lang=ZH-CN style='color:red'> </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>作为原始</span><span style='color:red'> FFN
</span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>的替代品，</span><span style='color:red'>MoE  FFN </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>由</span><span
style='color:red'> n </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>个单独的</span><span style='color:red'> FFN </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>组成，每个</span><span
style='color:red'> FFN </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>充当专家。</span></b><span lang=ZH-CN> </span><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;color:red'>每个令牌都被定向到特定的专家</span><span
style='color:red'> Ei</span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>，以便根据门控网络</span><span style='color:red'> G </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>分配的概率进行计算：</span></b></p>

<p class=MsoNormal style='margin-left:1.0in;text-indent:.25in'>p = softmax (G
(x)) ,</p>

<p class=MsoNormal style='margin-left:1.0in;text-indent:.25in'>y= <span
style='font-size:9.0pt;font-family:"Times New Roman",serif;position:relative;
top:4.5pt'><img width=55 height=29 src="qwen2_files/image001.png"></span>Ei(x)</p>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>下面，我们介绍</span> Qwen2 MoE <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>的关键设计考虑因素。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal style='text-indent:.25in'><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>专家粒度</span> MoE</b>  <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>模型和密集模型之间的关键结构差异在于</span><span
style='color:red'> MoE </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>层包含多个</span><span style='color:red'> FFN</span><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;color:red'>，每个</span><span
style='color:red'> FFN </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>充当单独的专家。</span><span lang=ZH-CN style='color:red'> </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>因此，从密集架构过渡到</span><span
style='color:red'> MoE </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>架构的一种直接策略是将每个专家的参数设置为等于原始密集模型中单个</span><span style='color:red'> FFN </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>的参数。</span><span
lang=ZH-CN style='color:red'> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>例如，从</span>
Mistral-7B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Jiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）过渡到</span> Mixtral
8x7B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Jiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）需要一次激活八个专家之一。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>不同的是，<b><span
style='color:red'>我们的模型采用细粒度专家（</span></b></span><b><span style='color:red'>Dai
et al., 2024</span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>），创建较小规模的专家，同时激活更多数量的专家。</span><span lang=ZH-CN style='color:red'> </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>给定相同数量的专家参数和激活参数，细粒度专家提供更丰富的专家组合集。</span><span
lang=ZH-CN style='color:red'> </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>通过利用这些细粒度的专家，</span><span
style='color:red'>Qwen2 MoE</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>可以促进更加多样化和动态的专家利用，从而提高整体性能和适应性。</span></b></p>

<p class=MsoNormal style='text-indent:.25in'><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>专家路由</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>专家路由机制的设计对于增强</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的性能至关重要。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>最近，<b>在</b></span><b>
MoE </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>层内集成共享专家和特定于路由的专家已成为一个显着趋势</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Rajbhandari
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2022
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>年；</span>Dai <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>年）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们采用这种方法，因为它有助于共享专家在各种任务中的应用，同时保留其他专家在特定路由场景中选择性使用。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>共享专家和专业专家的引入为开发</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>路由机制提供了一种适应性更强、更高效的方法。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=427 height=213 id="图片 1"
  src="qwen2_files/image002.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:15.0pt'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 1</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2 </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>密集模型和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> MoE </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型的架构。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>对于</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> MoE </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>模型，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>57B-A14B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>表示模型总共有</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 57B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>个参数，每个令牌有</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 14B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>个参数处于活动状态，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Intermediate </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>大小表示每个专家的参数，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>#Activated Experts </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>不包括共享专家。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal style='text-indent:.25in'><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;color:white'>专家初始化</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们利用密集模型的权重，以与升级类似的方式初始化专家（</span>Komatsuzaki
et al., 2023<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比之下，我们的方法强调细粒度专家的多样化，以增强模型的代表性广度。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>给定指定的专家中间大小</span>
hE<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、专家数量</span>
n <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和原始</span>
FFN <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>中间大小</span>
hFFN<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>FFN
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>被复制</span><span
lang=ZH-CN> </span><span style='font-family:"Cambria Math",serif'>⌈</span>n×hE/hFFN<span
style='font-family:"Cambria Math",serif'>⌉</span> <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>次。</span><span lang=ZH-CN> </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这种复制确保了与指定数量的专家的兼容性，同时适应任意任意专家中间大小。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了促进每个</span>
FFN <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>副本内的多样性，参数沿着中间维度进行改组。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这保证了每个细粒度专家即使在不同的</span>
FFN <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>副本中也能展现出独特的特征。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>随后，从</span>
FFN <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>副本中提取这些专家，并丢弃剩余的维度。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于每个细粒度专家，其</span>
50% <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的参数会被随机重新初始化。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此过程在专家初始化中引入了额外的随机性，有可能增强模型在训练期间的探索能力。</span></p>

<h4>2.2.3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>型号配置</span></h4>

<p class=MsoNormal style='text-indent:.25in'><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>下面我们提供</span>Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>系列的关键配置和信息。</span>  <b>Qwen2</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列由</span>5</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个尺寸型号组成，分别是</span>Qwen2-0.5B</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Qwen2-1.5B</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Qwen2-7B</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Qwen2-57B-A14B</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Qwen2-72B</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表</span>
1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>列出了超参数和重要信息，例如预训练令牌的数量。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>特别是，</span>Qwen2-57B-A14B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>是</span>
Qwen2-7B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的升级版。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>值得注意的是，与</span>
Qwen1.5 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型相比，</span>Qwen2
</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型显示每个代币的键值</span>
(KV) </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>大小要低得多。</span><span
lang=ZH-CN> </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此特性可减少内存占用，这在长上下文推理任务中特别有利</span></b></p>

<h2><a name="_Toc173831966">3 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>预训练</span></h2>

<p class=MsoNormal style='text-indent:.25in'><b> </b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>在</span> Qwen2 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的预训练中，我们的工作重点是<b>完善数据集和研究有效处理扩展上下文长度</b>的方法。</span></p>

<h3> <a name="_Toc173831967">3.1 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>预训练数据</span></h3>

<p class=MsoNormal style='text-indent:.25in'> Qwen2 <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>模型的预训练涉及开发一个新的、大规模的、高质量的多语言数据集。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该数据集比之前的</span>
Qwen </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen1.5 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型中使用的语料库有所改进（</span></b>Bai
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>；</span>Qwen Team<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），在<b><span
style='background:yellow'>几个关键领域提高了预训练数据的规模、质量和多样性</span></b>：</span><span
lang=ZH-CN> </span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>质量增强</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>过滤算法已通过额外的启发式和基于模型的方法进行了改进，包括使用</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型来过滤掉低质量数据。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，这些模型用于合成高质量的预训练数据</span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>数据扩展</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>Qwen1.5<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>团队，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）相比，我们收集了<b>更多的高质量代码、数学和多语言数据</b>，增强了模型在各自领域的能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这个新数据集支持大约</span>
30 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>种语言，例如英语、中文、西班牙语、法语、德语、阿拉伯语、俄语、韩语、日语、泰语和越南语。</span><span
lang=ZH-CN> </span></p>

<p class=MsoListParagraph style='margin-left:39.0pt;text-indent:-21.0pt'><span
style='font-family:Wingdings'>l<span style='font:7.0pt "Times New Roman"'>&nbsp;&nbsp;
</span></span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分布改进</span></b><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
background:yellow'>为了确保模型学习类似于人类学习的分布，我们在缩小模型上进行实验，以优化来自不同来源和领域的数据的混合</span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span></b></p>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>基于这些增强，<b><span
style='color:red'>预训练数据从</span></b></span><b><span style='color:red'> Qwen1.5</span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>（</span><span
style='color:red'>Qwen Team</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>，</span><span style='color:red'>2024a</span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>）中的</span><span
style='color:red'> 3 </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>万亿个代币扩展到</span><span style='color:red'> 7 </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>万亿个代币。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进一步放宽质量门槛的尝试导致了</span>
12 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>万亿的代币数据集。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>然而，与</span>
7 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>万亿代币模型相比，在此数据集上训练的模型并未显示出显着的性能改进。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>怀疑增加数据量并不一定有利于模型预训练。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>考虑到训练成本，我们选择使用更高质量的</span>7<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>万亿代币数据集来训练更大的模型，为未来的模型迭代留下进一步的探索。</span></p>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所有</span><span
lang=ZH-CN> </span><b><span style='color:red'>Qwen2 </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>密集模型（不包括</span><span
style='color:red'> Qwen2-0.5B</span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>）都在这个包含超过</span><span style='color:
red'> 7 </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>万亿个代币的大规模数据集上进行了预训练。</span></b><span lang=ZH-CN> </span><b><span
style='color:red'>Qwen2-0.5B </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>使用</span><span style='color:red'> 12 </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>万亿代币数据集进行预训练。</span></b> 
MoE<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型额外获得了</span>4.5<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>万亿代币的预训练，符合升级循环的原则。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与之前的</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型类似，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>预训练过程中集成了高质量的多任务指令数据，以增强情境学习和指令跟踪能力。</span></p>

<h3><a name="_Toc173831968">3.2 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>长上下文训练</span></h3>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>为了增强</span><span style='color:red'> Qwen2 </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>的长上下文能力，我们在预训练的最后阶段将上下文长度从</span><span
style='color:red'> 4,096 </span></b><b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif;color:red'>个标记增加到</span><span style='color:red'>
32,768 </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>个标记</span></b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这种扩展还伴随着大量高质量、长数据的引入。</span><span
lang=ZH-CN> </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结合这些增强功能，我们将</span>
RoPE </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的基本频率从</span>
10,000 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>修改为</span>
1,000,000</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，以优化长上下文场景中的性能</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Xiong <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了充分利用<b><span
style='color:red'>模型的长度外推潜力，我们采用了</span></b></span><b><span style='color:red'>
YARN </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>机制（</span><span style='color:red'>Peng et al., 2023</span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>）和</span><span
style='color:red'> Dual Chunk Attention </span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;color:red'>机制（</span><span
style='color:red'>An et al., 2024</span></b><b><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif;color:red'>）。</span></b><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:red'>这些策略使模型能够处理多达</span><span style='color:red'> 131,072 </span></b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;color:red'>个标记的序列，同时保持高性能</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，初步实验中的困惑度降级最小就证明了这一点。</span></p>

<h2><a name="_Toc173831969">4 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>后训练</span></h2>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在广泛的大规模预训练之后，我们进入了</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的后训练阶段。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这一过程对于提高其在广泛领域的熟练程度至关重要，包括编码、数学、逻辑推理、指令遵循和多语言理解</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，它确保模型的生成与人类价值观和谐一致，使其有益、诚实且无害。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与严重依赖广泛的人类监督的传统方法不同，我们的方法侧重于以最少的人类注释进行可扩展的对齐（</span>Cao
et al., 2024<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们<b>研究了从人类反馈中获取监督微调（</b></span><b>SFT</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和强化学习（</span>RLHF</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）的高质量演示和偏好数据的方法，旨在最大限度地减少人工标记的需求</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，同时最大限度地提高数据的质量和可靠性。</span></p>

<h3><a name="_Toc173831970">4.1 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>训练后数据</span></h3>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>训练后数据主要由两部分组成：演示数据</span>
D = {(xi, yi)} <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和偏好数据</span>
P = {(xi, y+ i , y− i )}<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，其中</span>
xi <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>代表指令</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>yi
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表示满意的响应，</span>y+
i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span> y−
i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>是</span> xi
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的两个响应，其中</span>
y+ i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>是</span>
y− i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的首选。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>集合</span>D<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>用于</span>SFT<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，而</span>P<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>用于</span>RLHF<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>训练数据的构建需要两个步骤：协作数据注释和自动数据合成。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>首先，我们从大规模指令语料库中提取数据本体，从而产生广泛且多样化的高质量指令。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这些指令经过系统增强，变得更加复杂。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>通过人工注释，我们获得目标响应</span>
yi <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>及其正负对应项</span>
(y+ i , y− i )<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>随后，各种自动化采用对齐策略来合成大量跨代码、数学、指令遵循、创建、角色扮演和安全领域的人工注释数据。</span></p>

<h4>4.1.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>协作数据</span><span
lang=ZH-CN> </span></h4>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>标注自动本体</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>提取该过程始于应用开放集细粒度标记器</span>InsTag<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Lu et al.,
2024c<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）从大规模指令数据集中提取底层本体。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>随后的人工精炼保证了提取本体的准确性。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>指令选择</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>每个带有标签注释的指令都会评估标签多样性、语义丰富性、复杂性和意图完整性。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>根据这些标准，我们选择了一组代表性指令（</span>Dong
et al., 2023<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>指令进化</span><span
lang=ZH-CN> </span></b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了丰富指令数据集，采用了自我进化策略（</span>Zhao
et al., 2024<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），促使</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型向现有指令添加约束或要求，从而增加其复杂性并确保数据集中的不同难度级别</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>人工注释</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>使用不同的生成策略和不同尺度的</span>
Qwen <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型获得对指令的多个响应。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>注释者根据自己的偏好对这些响应进行排名，确保最佳响应符合既定标准，从而生成演示数据和偏好数据。</span></p>

<h4>4.1.2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>自动数据合成</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>维持对指令响应的注释质量提出了大规模的重大挑战，特别是那些需要专业知识、经验、细心或耐心的挑战。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了应对这些挑战，我们设计了各种自动对齐策略来大规模合成数据。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>拒绝抽样</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于具有明确最终答案的数学或类似任务，应用拒绝抽样（</span>Yuan
et al., 2023<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）来提高解决方案的质量。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>大型语言模型</span>
(LLM) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的任务是为每条指令生成多个响应，即推理路径。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>得出准确结论并被模型认为合理的路径被保留，作为示范数据。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>偏好数据是通过对比正确和错误路径生成的。</span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>执行反馈</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于编码任务，法学硕士用于生成解决方案和相关的测试用例。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这些解决方案的功效是通过针对测试用例进行编译和执行来评估的，从而创建演示和偏好数据。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该方法也适用于评估指令遵循情况（</span>Dong
et al., 2024<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于每条具有约束（例如长度限制）的指令，法学硕士的任务是生成一个</span>
Python <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>验证函数，以确保响应符合指令要求。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>数据再利用</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于未经专门培训的注释者来说，在文学写作任务中创建熟练的响应是一项挑战。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了解决这个问题，我们汇总了公共领域的高质量文学作品，并聘请法学硕士来制定不同详细程度的说明。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这些说明与原始作品配对，作为演示数据。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>例如，为了编译具有生动且引人入胜的反应的角色扮演数据，我们从维基百科等知识库中获取详细的角色资料，并指导法学硕士生成相应的指令和反应（</span>Lu
et al., 2024b<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这个过程类似于阅读理解任务，可确保保持角色个人资料的完整性。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>宪法反馈</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>宪法人工智能是指指导法学硕士根据预定义的原则集生成响应的过程（</span>Bai
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2022<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了确保遵守安全和价值观等准则，编制了体质数据集。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该数据集描述了应遵循的原则和应避免的原则。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>它被用来指导法学硕士做出符合或偏离这些指南的回答，作为演示和偏好数据的参考。</span></p>

<h3><a name="_Toc173831971">4.2 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>有监督的微调</span></h3>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们已经组装了一个广泛的指令数据集，包含超过</span><b>
500,000 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个示例，涵盖指令遵循、编码、数学、逻辑推理、角色扮演、多语言和安全等技能</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们的模型针对两个时期进行了微调，序列长度为</span>
32,768 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个令牌。</span><span
lang=ZH-CN> </span><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了优化学习，学习率从</span>
7 × 10−6 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>逐渐降低到</span>
7 × 10−7</b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了解决过度拟合问题，我们应用了</span><b>
0.1 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的权重衰减，并将梯度限制为最大值</span>
1.0</b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span></p>

<h3><a name="_Toc173831972">4.3 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>根据人类反馈进行强化学习</span></h3>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们的</span>
RLHF <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>训练制度包括两个连续的阶段：离线和在线训练。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在离线训练阶段，我们使用预编译的偏好数据集</span>
P <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>通过直接偏好优化来最大化</span>
y+ i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
y− i <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>之间的可能性差异（</span>DPO<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>Rafailov <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在在线训练阶段，模型利用奖励模型进行即时反馈，实时迭代地完善其性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们从当前策略模型中采样多个响应，奖励模型选择最喜欢和最不喜欢的响应，形成用于每个情节中的</span>
DPO <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的偏好对。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，我们采用在线合并优化器（</span>Lu
et al., 2024a<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）来减轻对齐税，即与将模型生成与人类偏好对齐相关的性能下降。</span></p>

<h2><a name="_Toc173831973">5 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>评估</span></h2>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了彻底评估由基础模型和指令调整模型组成的</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型，我们实施了全面的评估协议。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>该协议检查一系列能力，包括一般知识理解、语言理解、生成、编码、数学、推理和其他专业领域。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，除非另有说明，否则使用已建立的大型语言模型（</span>LLM<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）基准数据集来评估基础模型，并通过几次提示来引出响应。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于指令调整模型，除了基准评估之外，我们还优先考虑人类偏好评估。</span></p>

<h3><a name="_Toc173831974">5.1 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>基础语言模型</span></h3>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在本节中，我们将说明</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列的基础语言模型的评估。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们在知识和基本能力的基准数据集上评估模型，并应用多语言基准数据集来评估其对语言的支持。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>由于模型尺寸有多种，我们将它们与相似或更大尺寸的最先进</span>
(SOTA) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型进行比较。</span></p>

<h4>5.1.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>核心能力基准和评估协议</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>评估基础语言模型核心能力的常见做法是通过少样本或零样本提示实施基准数据集评估。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>评估主要关注自然语言理解、一般问答、编码、数学、科学知识、推理等方面的模型表现。评估的数据集包括</span>
MMLU (Hendrycks et al., 2021a) (5-shot)<span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>、</span>MMLU-  Pro (Wang et al., 2024) (5-shot)<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GPQA (Rein
et al., 2023) (5-shot)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Theorem
QA (Chen et al., 2023a) (5-shot)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>BBH
(Suzgun et al., 2023) (5-shot)  , 2023) (3-shot), HellaSwag (Zellers et al.,
2019) (10-shot), Winogrande (Sakaguchi et al., 2021) (5-shot), TruthfulQA (Lin
et al., 2022a) (0  -shot<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）、</span>ARC-C<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Clark <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2018<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>25 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）、</span>HumanEval<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Chen <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2021<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>0 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）、</span>MBPP<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Austin <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2021<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>0 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）</span>  <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>EvalPlus<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Liu <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>0 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）、</span>MultiPL-E<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Cassano <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>Python<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>C++<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Java<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>PHP<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>TypeScript<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>C#<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Bash <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span> JavaScript
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上的</span> 0 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次），</span>  GSM8K<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Cobbe <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2021<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个镜头）、</span>MATH<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Hendrycks <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2021b<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>4 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个镜头）、</span>C-Eval<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Huang <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个镜头）和</span> CMMLU<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>  Li <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>多语言数据集可以分为四类：（</span>a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）考试：</span>M3Exam<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>5-shot<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，我们只选择不需要图像的示例）、</span>IndoMMLU<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Koto et
al., 2023<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>3-shot<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）、</span>ruMMLU<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Fenogenova
et al.<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）</span> 
., 2024) (5-shot)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，并翻译了</span>
MMLU (Chen et al., 2023b)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（关于阿拉伯语、西班牙语、法语、葡萄牙语、德语、意大利语、日语和韩语的</span>
5-shot<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）；</span> 
(b) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>理解：</span>BELEBELE<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Bandarkar <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）、</span>XCOPA<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Ponti <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2020<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）、</span>XWinograd<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Muennighoff
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次），</span> 
XStoryCloze<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Lin
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2022b<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>0 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）和</span> PAWS-X<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Yang <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2019<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）；</span>  <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>C<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）数学：</span>MGSM<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Goyal <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2022<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>8-shot CoT<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）；</span>  (d) <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>翻译：</span>Flores-101<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Goyal <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2022<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）（</span>5 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>次）。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a align=center style='text-align:center;text-indent:.25in'><img
  width=433 height=393 id="图片 17" src="qwen2_files/image003.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:15.0pt'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'> 2</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>：</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>70B+ </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>型号的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%'> </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>我们将</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Qwen2-72B </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>与基线进行比较，包括</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Mixtral-8x22B</span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>、</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Llama-3-70B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>、</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Qwen1.5-110B
  </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Qwen1.5-72B</span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%'> </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>对于大多数数据集，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Qwen2-72B
  </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>表现出优于基线的优势。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><b><span style='color:white'>Qwen2-72B</span></b> <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>就</span> Qwen2 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的最大模型而言，我们将</span>
Qwen2-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与竞争性基线开放权重模型进行比较，包括</span>
Mixtral-8x22B (Jiang et al., 2024)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Llama-3-70B
(AI@Meta, 2024)  <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，以及</span>
Qwen1.5-72B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>团队，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span>
Qwen1.5-110B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>团队，</span>2024b<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果如表</span>
2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。</span>Qwen2-72B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span> MMLU
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
MMLU-Pro <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上的一般知识理解方面均优于</span>
Llama-3-70B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，分别实现了</span>
4.7 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
2.8 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的准确度提升。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在科学评估中，</span>Qwen2-72B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出优于</span>
Llama-3-70B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的优越性，在</span><span
lang=ZH-CN> </span>GPQA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和定理</span>
QA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上分别提高了</span>
1.6 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
9.8<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在丰富编码数据后，</span>Qwen272B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>
HumanEval <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
MBPP <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>评估中比</span>
Qwen1.5-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出显着的</span>
18.3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
10.0 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个百分点的优势。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>增强的数学相关数据使</span>
Qwen2-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>
GSM8K <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
MATH <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>基准测试中比</span>
Qwen1.5-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>分别高出</span>
10.0 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
17.0 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个百分点。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>考虑到</span>
BBH<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Winogrande
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
ARC-C<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>Qwen272B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>显示出与</span>
Llama-3-70B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相当的推理能力，这归因于其改进的编码和数学数据。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在评估中文语言理解时，</span>Qwen2-72B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>显着优于</span>
Mixtral-8x22B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Llama-3-70B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，也优于</span>
Qwen1.5-72B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span></p>

<p class=MsoNormal><b>Qwen2-57B-A14B</b> <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>为了评估</span> MoE <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>模型，将</span> Qwen2-57B-A14B <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与类似尺寸的基线进行比较。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这些基线包括其他</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型，例如</span>
Mixtral-8x7B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Jiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span> Jamba<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Lieber <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），以及密集模型，例如</span>
Yi-1.5-34B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Young
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span> Qwen1.5-32B<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen Team<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），两者都有大约</span> 300
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果如表</span>
3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。我们预计激活</span>
140 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数的</span>
Qwen2-57B-A14B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>将与</span>
300 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数密集等效</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的性能相匹配。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们的评估表明，</span>Qwen2-57B-A14B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在自然语言理解任务中的表现与</span>
Yi-1.5-34B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相当。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，它在编码和数学任务中优于基线模型。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span>Qwen2-57B-A14B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出强大的中文理解能力，可与更大的</span>
Qwen2-72B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>型号相媲美。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>本质上，</span>Qwen2-57B-A14B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>是一个高效的模型，虽然每次前向传递仅激活</span>
140 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数，但保持了</span>
300 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数密集模型的性能水平。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=479 height=481 id="图片 18"
  src="qwen2_files/image004.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 3</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>30B+ </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>密集模型和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 40B+ MoE </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>  Qwen2-57B-A14B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>是一个</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>MoE</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>模型，总共有</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>570</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>亿个参数和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>140</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>亿个激活参数，旨在匹配</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>300</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>亿个参数密集模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>此比较包括密集模型基线：</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Yi-1.5-34B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen1.5-32B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>，以及</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> MoE </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>基线：</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Mixtral-8x7B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Jamba</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>结果表明，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2-57B-A14B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>总体表现具有竞争力，在编码和数学任务中具有显着的优势。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><b><span style='color:white'>Qwen2-7B</span></b> 7B <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型得到广泛应用，因为它可以在配备</span>
16GB <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>内存的加速器上以</span>
16 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>位浮点执行。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们的重点是将该模型与其他领先的</span>
7B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型进行比较，包括</span>
Llama-3-8B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，它最近在聊天机器人竞技场中表现出了卓越的性能（</span>Chiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此比较还包括</span>
Mistral-7B-v0.2<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Jiang
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）、</span>Gemma-7B<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Mesnard <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）以及我们的前身</span>
Qwen1.5-7B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen
Team<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=489 height=472 id="图片 19"
  src="qwen2_files/image005.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:15.0pt'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'> 4</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>：</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>7B+ </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>型号的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%'> </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>我们将</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Qwen2-7B </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>与之前发布的最先进的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'> 7B+ </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>模型进行比较，包括</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Mixtral-7B</span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>、</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Gemma-7B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%;font-family:"Microsoft YaHei",sans-serif'>、</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Llama-3-8B
  </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>和我们之前的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>
  Qwen1.5-7B</span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;line-height:
  150%'> </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>在大多数评估数据集中，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt;line-height:150%'>Qwen2-7B
  </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;line-height:150%;font-family:"Microsoft YaHei",sans-serif'>表现出优于基线的显着优势。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:white'>结果如表</span> 4 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。与其他模型相比，</span>Qwen2-7B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在大多数数据集上表现出优异的性能，特别是在编码任务、数学和中文任务中表现出色。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>它在多语言理解和考试方面也表现出了强劲的表现。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这表明</span>
Qwen2-7B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>已针对各种基于语言和逻辑的任务进行了优化，展示了其多功能性和先进功能。</span></p>

<p class=MsoNormal>Qwen2-1.5B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen2-0.5B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了评估我们较小模型（特别是</span>
Qwen2-1.5B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen2-0.5B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）的性能，我们将它们与已建立的基线进行比较：</span>Phi-2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Abdin <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>年）、</span>Gemma- 
2B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Mesnard
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span>
Qwen1.5-1.8B<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Qwen
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>团队，</span>2024a<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果如表</span>
5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。在语言理解方面，</span>Qwen2-1.5B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>优于</span>
Phi-2<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（一种使用教科书数据训练的模型）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于编码任务，</span>Qwen2-0.5B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Gemma-2B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen1.5-1.8B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的性能相匹配，而</span>
Qwen2-1.5B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>超越了这些基线（</span>Phi-2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>除外）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与竞争对手相比，这两种</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型在数学方面都表现出了卓越的性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在一般推理方面，我们发现</span>Phi-2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>总体上优于所有其他推理，这在一定程度上反映了教科书数据对于推理能力的重要性。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在</span>TruthfulQA<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>中，</span>Qwen2-1.5B<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现最好，证明较小的模型不一定会出现幻觉。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在中文理解方面，两个</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型都优于其他所有模型，这一趋势与各自比较中的较大模型一致</span></p>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>总体而言，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列在不同型号尺寸的基准上表现出卓越的性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>值得注意的是，</span>Qwen2-72B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在所有</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型中表现出最高的性能，凸显了模型尺寸缩放的功效。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=494 height=275 id="图片 20"
  src="qwen2_files/image006.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 5</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：较小型号的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-0.5B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-1.5B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与之前的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> SOTA </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>小型型号（包括</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Phi-2</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Gemma-2B </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5-1.8B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>）进行比较。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>  Qwen2-0.5B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>凭借更小的模型尺寸实现了具有竞争力的性能，并且</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2-1.5B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>显着优于</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2-0.5B</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>。</span></span></p>
  </td>
 </tr>
</table>

<h3><a name="_Toc173831975"><span style='color:white'>5.2 </span></a><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>指令调整模型</span></h3>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了批判性地评估指令调整模型，我们实施了一种多方面的方法。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>使用开放数据集和基准对基础技能和人类偏好进行评估。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们详细的内部检查进一步探讨了模型在关键领域的能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>特别关注评估长上下文能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>安全措施包括多语言安全评估和红队演习。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>以下各节详细介绍了评估方法及其结果。</span></p>

<h4>5.2.1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>开放基准评估</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了全面评估指令调整模型的质量，我们编写了自动和人工评估来评估能力和人类偏好。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于基本能力的评估，我们在预训练模型评估中应用了类似的数据集，针对自然语言理解、编码、数学和推理。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们评估语言理解和知识的</span>
MMLU<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>MMLU-Pro<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GPQA <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span> Theorem QA<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，评估编码的</span>
HumanEval<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>MBPP<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>MultiPL-E <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
LiveCodeBench v1 (Jain et al., 2024)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，评估数学的</span>
GSM8K <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
MATH<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，我们通过评估</span>
MT-Bench (Zheng et al., 2023)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Arena-Hard
(Li et al., 2024)<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>AlignBench
(Liu et al., 2023b) <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等基准来评估人类偏好对齐和指令遵循的表现</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）、</span>MixEval<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Ni <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），其结果近似于</span>
Chatbot Arena <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的结果，以及</span>
IFEval<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Zhou
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）</span>4 <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>用于指令跟踪。</span></p>

<p class=MsoNormal><span style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'>Qwen2-72B-Instruct </span><span lang=ZH-CN
style='font-size:10.5pt;font-family:"Microsoft YaHei",sans-serif;color:black;
background:white'>我们将</span><span style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'> Qwen2-72B-Instruct </span><span lang=ZH-CN
style='font-size:10.5pt;font-family:"Microsoft YaHei",sans-serif;color:black;
background:white'>与指令调整模型（包括</span><span style='font-size:10.5pt;font-family:
"Segoe UI",sans-serif;color:black;background:white'> Mixtral-8x22B-Instruct</span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Microsoft YaHei",sans-serif;
color:black;background:white'>、</span><span style='font-size:10.5pt;font-family:
"Segoe UI",sans-serif;color:black;background:white'>Llama-3-70B-Instruct </span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Microsoft YaHei",sans-serif;
color:black;background:white'>以及</span><span style='font-size:10.5pt;
font-family:"Segoe UI",sans-serif;color:black;background:white'>
Qwen1.5-72B-Chat</span><span lang=ZH-CN style='font-size:10.5pt;font-family:
"Microsoft YaHei",sans-serif;color:black;background:white'>）进行比较。</span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'> </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>结果如表</span><span
style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;color:black;
background:white'>6</span><span lang=ZH-CN style='font-size:10.5pt;font-family:
"Microsoft YaHei",sans-serif;color:black;background:white'>所示。可以发现，强大的基础语言模型可以帮助提高指令调整模型的下游性能。</span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'> </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>具体来说，</span><span
style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;color:black;
background:white'>Qwen2-72B-Instruct </span><span lang=ZH-CN style='font-size:
10.5pt;font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>在语言理解、编码和数学等领域（</span><span
style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;color:black;
background:white'>GPQA </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>和</span><span
style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;color:black;
background:white'> MBPP </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>除外）优于其他同行。</span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'> </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>在人类偏好对齐和指令遵循方面，</span><span
style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;color:black;
background:white'>Qwen2-72B </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>比基线具有显着优势。</span><span
lang=ZH-CN style='font-size:10.5pt;font-family:"Segoe UI",sans-serif;
color:black;background:white'> </span><span lang=ZH-CN style='font-size:10.5pt;
font-family:"Microsoft YaHei",sans-serif;color:black;background:white'>我们认为这一成就归功于高质量的预训练模型以及训练后数据和训练技术的改进。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=479 height=322 id="图片 21"
  src="qwen2_files/image007.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 6</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>70B+ </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>指令调整模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Mixtral-8x22B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Llama-3-70B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen1.5-72B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen1.5-110B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>进行比较。“</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>”或“</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>-Chat</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>”被省略</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>桌子。</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>  Qwen2-72B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>展现了核心能力的优势，以及人类偏好对齐方面的卓越表现。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><b><span style='color:white'>Qwen2-57B-A14B-Instruct</span></b>
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于中型模型，我们将</span>
Qwen2-57B-A14B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Mixtral-8x7B-Instruct<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（另一个</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>基线）以及具有超过</span>
300 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数的密集</span>
SOTA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型进行比较，例如</span>
Yi-  1.5-34B-<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>聊天和</span>Qwen1.5-32B-<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>聊天。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果如表</span>7<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。与</span>Qwen1.5-32B-Chat<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比，</span>Qwen2-57B-A14B-Instruct<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在几乎所有基准测试中都达到了优越的性能，并且与</span>30B
SOTA<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型</span>Yi-1.5-34B-Chat<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Qwen2-<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比</span>
57BA14B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在除数学以外的大多数评估中都取得了优势。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在比对评估方面，</span>Qwen2-57B-A14B-Instruct<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的优势尤为明显。</span>  </p>

<p class=MsoNormal><b>Qwen2-7B-Instruct </b><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>在</span> 7B <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>至</span> 9B <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>模型范围内，我们将</span> Qwen2-7B-Instruct <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Llama-3-8B-Instruct<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>Yi-1.5-9B-Chat<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>GLM-4-9B-Chat
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Qwen1.5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行比较</span>
-7B-<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>聊天。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果如表</span>
8 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示。在综</span><span
lang=ZH-CN style='font-family:"MS Gothic"'>​​</span><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>合评估中，</span>Qwen2-7B-Instruct <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与其前身</span>
Qwen1.5-7B-Chat <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比取得了显着进步，特别是在编码和数学相关任务中取得了更高的分数。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与最新的</span>
SOTA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型</span>
Llama-38B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比，</span>Qwen2-7B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出了竞争性的性能，特别是在编码方面实现了优越的性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>然而，在指令遵循方面，</span>Qwen2-7B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>大大落后于竞争对手。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了解决这一限制，我们计划通过提高训练后数据的质量来增强</span>
7B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的指令跟踪能力，确保对复杂命令的更强大的理解和执行。</span> 
</p>

<p class=MsoNormal><b>Qwen2-1.5B-Instruct</b> <span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>和</span> Qwen2-0.5B-Instruct <span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在较小模型的背景下，我们将</span>
Qwen2-0.5B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Qwen1.5-0.5B-Chat <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行比较，并将</span>
Qwen2-1.5B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Qwen1.5- <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行比较。</span> 
1.8B-<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>聊天。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>值得注意的是，为较大模型设计的某些数据集的复杂性超出了这些较小模型的能力；</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>因此，我们的分析集中于选定的子集。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>如表</span>
9 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型在核心功能和指令执行任务方面均表现出优于其前代产品的显着优势。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这一成就主要归功于预训练数据的扩展。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>因此，我们的结果证实，即使在参数模型不足十亿的领域，数据扩展仍然是增强模型性能的有效策略。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=491 height=385 id="图片 22"
  src="qwen2_files/image008.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 7</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>30B+ </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>密集和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 40B+ MoE </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>指令调整模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-57B-A14B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与类似尺寸的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> MoE </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Mixtral-8x7B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Yi-1.5-34B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen1.5-32B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>等</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> 30B </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>密集模型进行比较。“</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>”或“</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>- </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表中省略了“聊天”。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 
  Qwen2-57B-A14B-Instruct </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>与最近的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> SOTA 30B </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>密集模型具有竞争力，并且显着优于</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> MoE </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>基线。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><span style='color:white'>5.2.2 </span><span lang=ZH-CN
style='font-family:"Microsoft YaHei",sans-serif'>内部自动评估</span></p>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>尽管有许多用于评估的开放基准数据集，但我们认为这还远远不足以完全理解法学硕士的能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们制作了一系列内部数据集来评估模型的不同能力，例如知识理解、文本生成、编码等。评估是中文和英文的。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结果分别汇总在表</span>10<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和表</span>11<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>中。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>中文评估</span><span
lang=ZH-CN> </span></b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于中文评估，我们重点比较</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型与</span>
Qwen1.5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于小型模型，即使参数较少，</span>Qwen2-1.5B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在几乎所有评估中通常都优于</span>
Qwen1.5-1.8B-Chat<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对比</span>7B<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>机型而言，</span>Qwen2<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的优势更为显着。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>值得注意的是，</span>Qwen272B
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的性能优于</span>
Qwen1.5-110B-Chat<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，尽管后者的参数要多得多。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相对于</span>
Qwen1.5-32B-Chat<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>MoE
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型在大多数领域都显示出优越的性能（不包括知识理解）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这种差异可能归因于预训练令牌的短缺。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在不久的将来，我们将继续对</span>
MoE <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型进行预训练，以发现其扩展行为。</span><span
lang=ZH-CN> </span></p>

<p class=MsoNormal><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>英语评估</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于英语，我们将</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
Qwen1.5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
Llama-3 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>进行比较。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>同样，</span>Qwen2
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的小型型号明显优于</span>
Qwen1.5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>同类产品。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>然而，与</span>
Llama-3-70B <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比，</span>Qwen2-72B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>略有落后，尤其是在理解和编码方面。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们假设预训练的英语标记数量和训练后数据的数量和多样性都会导致英语的性能差距。</span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=495 height=341 id="图片 23"
  src="qwen2_files/image009.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 8</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>7B+ </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>指令调整模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-7B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与最新具有</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 7-90 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>亿个参数的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> SOTA </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型进行比较，包括</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Llama-3-8B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Yi-1.5-9B-Chat</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>GLM-4-9B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen1.5-7B  -Chat</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>。表中省略了“</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>”或“</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>-Chat</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>”。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>  Qwen2-7BInstruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>展示了与</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Llama-3-8B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>相比的竞争性能。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=483 height=117 id="图片 24"
  src="qwen2_files/image010.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 9</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：较小的指令调整模型的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-0.5B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-1.5B-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen1.5-0.5B-Chat</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-1.8B-Chat</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>进行比较。表中省略了“</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>-Instruct</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>”或“</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>-Chat</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>”。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与相似大小的基线相比，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>的性能显着超过了</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>。</span></span></p>
  </td>
 </tr>
</table>

<h4><span style='color:white'>5.2.3 </span><span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>长上下文能力</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>采用三种方法来评估长上下文能力：<b>大海捞针</b>（</span>NIAH<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>Kamradt<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）、</span><b>NeedleBench</b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>OpenCompass
Contributors<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，</span>2023<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）和</span><span
lang=ZH-CN> </span><b>LV-Eval</b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Yuan
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>大海捞针这个实验评估了<b>模型在大量文本中查明事实的能力。</b></span><b><span
lang=ZH-CN> </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>长度为</span>
8K</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>16K</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>...</b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>、</span>128K </b><b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>标记的文本经过精心设计，事实被策略性地定位在不同的深度。</span></b><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>每个深度区间，例如从</span>
0% <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>到</span>
10%<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，包含两个实例。</span><b><span
lang=ZH-CN> </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于超过</span>
32K </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的上下文，本次评估中应用了</span>
YARN</b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Peng
et al., 2023<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>如图</span>
1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示，</span>Qwen2-72B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在从整个</span>
128K <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上下文中检索信息方面表现出卓越的准确性。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>加上其固有的优势，假设有足够的资源可用，该模型就成为处理大量文本的最佳选择。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，同系列中的模型在不同的上下文长度下都展示了卓越的性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>准确地说<b>，</b></span><b>Qwen27B-Instruct
</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在处理高达</span>
128K </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>令牌的上下文时实现了高精度。</span><span
lang=ZH-CN> </span></b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>同时，</span>Qwen2-57B-A14B-Instruct
</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>可以熟练地管理高达</span>
64K </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>令牌的上下文，而</span>
Qwen2 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列中的两个较小的型号可以支持</span>
32K </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>令牌的上下文。</span></b></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=474 height=290 id="图片 25"
  src="qwen2_files/image011.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 10</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型在我们内部中文自动评估基准上的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>超过同等尺寸</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>同类产品的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen2 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>型号的分数以粗体显示。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 
  Qwen2-57B-A14B-Instruct </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>与</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5-32B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>对比</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=506 height=424 id="图片 26"
  src="qwen2_files/image012.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 11</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>模型在我们内部英语自动评估基准上的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>超过同等尺寸</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Llama-3 </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>同类产品的</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen2 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>型号以粗体显示。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 
  Qwen2-57B-A14B-Instruct </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>与</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Qwen1.5-32B-Chat </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>进行比较。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<p class=MsoNormal>&nbsp;</p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=487 height=480 id="图片 27"
  src="qwen2_files/image013.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>图</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 1</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：在</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Haystack </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>测试中，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>指令调整模型在</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Needle </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>上的性能。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>所有支持上下文长度超过</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 32k token </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>的模型都集成了</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> YARN </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>机制。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=491 height=136 id="图片 28"
  src="qwen2_files/image014.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:0in'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 12</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-7B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>在</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> NeedleBench </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> LV-Eval </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>上的性能。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>  +YARN+DCA </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>不会改变</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 32k </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>令牌内的模型行为。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal>&nbsp;</p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=482 height=186 id="图片 29"
  src="qwen2_files/image015.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:15.0pt'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 13</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>和专有法学硕士在多语言人类评估中的表现。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> GPT-3.5-Turbo-1106</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>GPT-4-Turbo-0409</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>GPT4o-0513</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>、</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'>Claude-3-Opus-0229 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>进行比较。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>分数范围为</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 1 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>到</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> 5</span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>。总体而言，</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>的性能明显优于</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> GPT-3.5-Turbo</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>，但与过去</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 6 </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>个月发布的专有模型相比，仍有一些进步。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><b><span style='color:white'>NeedleBench</span></b>  NeedleBench
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>通过在段落中包含多个事实（两到五个）来应对</span>
NIAH <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的挑战，从而需要同时识别和多跳推理（</span>multi-hop
reasoning<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表</span>
12 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>显示，</span><b>YARN
</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
DCA </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的集成（</span>An
et al., 2024</b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>）显着提高了</span>
Qwen2 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型的长上下文能力</span></b><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span> 
Qwen2-7B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>超越了</span>
ChatGLM4-9B-1M<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>（</span>Zeng
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>等人，</span>2024<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>），后者声称上下文长度为</span>
1M<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span>Qwen2-72B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>表现出强大的性能，与</span>
ChatGLM4-9B-1M <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>相比，准确度仅降低了</span>
6 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个点，</span>ChatGLM4-9B-1M
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的下降更为明显，达到了</span>
11 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>个点，特别是考虑到其初始准确度较低。</span> 
</p>

<p class=MsoNormal><b>LV-Eval</b>  LV-Eval <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>包含</span> 11 <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>个不同的</span> QA <span lang=ZH-CN style='font-family:
"Microsoft YaHei",sans-serif'>数据集，需要同时理解多个证据。</span><span lang=ZH-CN> </span><span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了纠正其原始指标过于严格并导致假阴性率较高的缺点，我们采用关键词召回率作为报告分数。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>如表</span>
12 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示，集成</span>
YARN <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>和</span>
DCA <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>极大地增强了</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型在</span>
LV-Eval <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>上的长上下文能力。</span> 
Qwen2-7B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>与</span>
ChatGLM4-9B-1M <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>达到了同等水平，尽管在扩展环境下下降更为明显。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，</span>Qwen2-72B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在所有长度上都表现出强大的性能，证实了其处理长上下文任务的能力。</span></p>

<h4>5.2.4 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>多语言评估</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于多语言评估，我们实施了全面的人工评估来评估多语言能力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们设计了不同的测试用例来评估大型语言模型的不同功能，并且我们有多种语言的测试用例。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于标注者，我们为每种语言邀请一名该语言专业的专业标注者进行评估。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>对于每个测试用例，注释者都会对模型的响应进行评分，分数从</span>
1 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>到</span> 5<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。我们报告模型的结果以及不同语言评估的基线。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>从表</span>13<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>可以发现，平均而言，</span>Qwen2-72B-Instruct<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>显着优于</span>GPT-3.5Turbo<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，与</span>GPT-4-Turbo<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具有竞争力，略落后于</span>Claude-3-Opus<span
lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>这表明我们的多语言预训练和指令调优数据有助于</span>
Qwen2-72B-Instruct <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的多语言能力，并且与大多数最先进的专有法学硕士具有竞争力。</span></p>

<h4>5.2.5 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>安全与责任</span></h4>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具有公开权重的法学硕士可以有效地加速研究及其应用的发展。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>此外，我们认为建立安全和负责任的法学硕士至关重要，这样才能显着减轻人工智能技术滥用的影响。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们实施多语言安全评估，以不同语言测试法学硕士。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>具体来说，我们评估模型在非法行为、欺诈、色情内容和隐私。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>我们收集了容易越狱的提示，并用它们来测试模型是否可以通过拒绝提供安全的响应。</span><span
lang=ZH-CN> </span></p>

<table class=MsoTableGrid border=1 cellspacing=0 cellpadding=0
 style='border-collapse:collapse;border:none'>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  padding:0in 5.4pt 0in 5.4pt'>
  <p class=a style='text-indent:.25in'><img width=476 height=122 id="图片 30"
  src="qwen2_files/image016.jpg"></p>
  </td>
 </tr>
 <tr>
  <td width=553 valign=top style='width:414.8pt;border:solid windowtext 1.0pt;
  border-top:none;padding:0in 5.4pt 0in 5.4pt'>
  <p class=MsoNormal style='text-indent:15.0pt'><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>表</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> 14</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>：安全评估模型的表现。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>我们将</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>与</span></span><span class=MsoSubtleEmphasis><span
  style='font-size:7.5pt'> GPT-4 </span></span><span class=MsoSubtleEmphasis><span
  lang=ZH-CN style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>和</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'> Mixtral-8x22B-Instruct
  </span></span><span class=MsoSubtleEmphasis><span lang=ZH-CN
  style='font-size:7.5pt;font-family:"Microsoft YaHei",sans-serif'>进行比较。</span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt'> </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>越低越好。</span></span><span
  class=MsoSubtleEmphasis><span style='font-size:7.5pt'>  Qwen2-72B-Instruct </span></span><span
  class=MsoSubtleEmphasis><span lang=ZH-CN style='font-size:7.5pt;font-family:
  "Microsoft YaHei",sans-serif'>比竞争对手拒绝了更多带有风险的提示。</span></span></p>
  </td>
 </tr>
</table>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif;
color:white'>结果如表</span> 14 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>所示，其中显示了模型产生的有害反应的比例，越低越好。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>可以看出，</span>Qwen2-72B-Instruct
<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的性能优于专有模型</span>
GPT-4<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，并且显着优于开放权重模型</span>
Mixtral-8x22B-Instruct<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>然而，我们相信我们的模型还有很大的改进空间，成为一个更安全、更负责任的模型，特别是在色情内容方面，这是一个传统上很难区分的类别，即使对人类来说也是如此。</span></p>

<h2><a name="_Toc173831976">6 </a><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>结论</span></h2>

<p class=MsoNormal><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>本技术报告介绍了</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>系列，这是一套多功能的<b>基础和指令调整语言模型</b>，<b>参数范围从</b></span><b>
0.5 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>到</span>
720 </b><b><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>亿个参数，包括密集模型和专家混合架构模型。</span><span
lang=ZH-CN> </span></b> Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的性能优于之前的开放权重模型，尤其是其前身</span>
Qwen1.5<span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>，并且在语言理解、生成、多语言能力、编码、数学和推理等广泛的基准测试中显示出与专有模型相比的竞争性能。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>在此更新中，我们<span
style='background:yellow'>特别关注长上下文、多语言、编码、数学能力以及安全和责任。</span></span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>为了致力于促进社区内的创新和可访问性，我们公开了</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>模型权重，使研究人员和开发人员能够在各种应用和研究项目中充分利用</span>
Qwen2 <span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>的潜力。</span><span
lang=ZH-CN> </span><span lang=ZH-CN style='font-family:"Microsoft YaHei",sans-serif'>通过这些努力，我们旨在为人工智能技术的进步及其对社会的积极影响做出贡献。</span></p>

</div>

</body>

</html>


